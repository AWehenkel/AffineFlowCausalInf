Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Shiffrin,
abstract = {Human society has found the means to collect and store vast amounts of information about every subject imaginable, and is archiving this information in at- tempts to use it for scientific, utilitarian (e.g., health), and business purposes. These large databases are colloqui- ally termed BigData.Howbig BigData are is of course a matter of perspective, and can range, for example, from the “tiny” amount of data sociologists and climate sci- entists dealt with many years ago to everything being posted on theWorld WideWeb; BigData can arise from relatively well-controlled experiments or from uncon- trolled sets of natural observations.},
author = {Shiffrin, Richard M.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Shiffrin - Unknown - Drawing causal inference from Big Data.pdf:pdf},
journal = {Proc. Natl. Acad. Sci.},
number = {27},
pages = {7308--7309},
title = {{Drawing causal inference from Big Data}},
volume = {113},
year = {2016}
}

@article{VanEssen2012,
author = {{Van Essen}, D. C. and others},
eprint = {NIHMS150003},
isbn = {1095-9572},
journal = {NeuroImage},
keywords = {Behavior,Connectivity,Diffusion imaging,FMRI,MEG/EEG,Twins},
number = {4},
pages = {2222--2231},
title = {{The Human Connectome Project: A data acquisition perspective}},
volume = {62},
year = {2012}
}

@article{Poldrack2015,
abstract = {Psychiatric disorders are characterized by major fluctuations in psychological function over the course of weeks and months, but the dynamic characteristics of brain function over this timescale in healthy individuals are unknown. Here, as a proof of concept to address this question, we present the MyConnectome project. An intensive phenome-wide assessment of a single human was performed over a period of 18 months, including functional and structural brain connectivity using magnetic resonance imaging, psychological function and physical health, gene expression and metabolomics. A reproducible analysis workflow is provided, along with open access to the data and an online browser for results. We demonstrate dynamic changes in brain connectivity over the timescales of days to months, and relations between brain connectivity, gene expression and metabolites. This resource can serve as a testbed to study the joint dynamics of human brain and metabolic function over time, an approach that is critical for the development of precision medicine strategies for brain disorders.},
author = {Poldrack, Russell A and others},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Poldrack et al. - 2015 - ARTICLE Long-term neural and physiological phenotyping of a single human.pdf:pdf},
isbn = {2041-1723 (Electronic)$\backslash$r2041-1723 (Linking)},
journal = {Nat. Commun.},
title = {{Long-term neural and physiological phenotyping of a single human}},
volume = {6},
year = {2015}
}

@article{Hyvarinen2017a,
abstract = {We develop a nonlinear generalization of independent component analysis (ICA) or blind source separation, based on temporal dependencies (e.g. autocorrelations). We introduce a nonlinear generative model where the independent sources are assumed to be temporally dependent, non-Gaussian, and stationary, and we observe arbitrarily nonlinear mixtures of them. We develop a method for estimating the model (i.e. separating the sources) based on logistic regression in a neural network which learns to discriminate between a short temporal window of the data vs. a temporal window of temporally permuted data. We prove that the method estimates the sources for general smooth mixing nonlinearities, assuming the sources have sufficiently strong temporal dependencies, and these dependencies are in a certain way different from dependencies found in Gaussian processes. For Gaussian (and similar) sources, the method estimates the nonlinear part of the mixing. We thus provide the first rigorous and general proof of identifiability of nonlinear ICA for temporally dependent sources, together with a practical method for its estimation.},
author = {Hyv{\"{a}}rinen, Aapo and Morioka, Hiroshi},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hyv{\"{a}}rinen, Morioka - 2017 - Nonlinear ICA of Temporally Dependent Stationary Sources.pdf:pdf},
journal = {AISTATS},
pages = {460--469},
title = {{Nonlinear ICA of Temporally Dependent Stationary Sources}},
volume = {54},
year = {2017}
}

@article{Gal2016,
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Gal, Yarin and Ghahramani, Zoubin},
doi = {10.1109/TKDE.2015.2507132},
eprint = {1611.03530},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gal, Ghahramani - Unknown - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:pdf},
isbn = {1506.02142},
issn = {10414347},
journal = {ICML},
pmid = {88045},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning Yarin}},
url = {http://proceedings.mlr.press/v48/gal16.pdf http://arxiv.org/abs/1506.02142},
year = {2016}
}
@article{Dempster1972,
abstract = {The covariance structure of a multivariate normal population can be simplified by setting elements of the inverse of the covariance matrix to zero. Reasons for adopting such a model and a rule for estimating its parameters are given in section 2. It is also proposed to select the zeros in the inverse from sample data. A numerical illustration of the proposed technique is given in section 3. Appendix A sketches the general theory of exponential families which underlies the special results of section 2, and Appendix B describes two approaches to computation of the proposed estimator.},
author = {Dempster, A P},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Dempster - 1972 - COVARIANCE SELECTION.pdf:pdf},
journal = {Biometrics},
number = {1},
pages = {157},
title = {{Covariance Selection}},
volume = {28},
year = {1972}
}
@article{Pompili2014,
abstract = {Approximate matrix factorization techniques with both nonnegativity and orthogonality constraints, referred to as orthogonal nonnegative matrix factorization (ONMF), have been recently introduced and shown to work remarkably well for clustering tasks such as document classification. In this paper, we introduce two new methods to solve ONMF. First, we show mathematical equivalence between ONMF and a weighted variant of spherical k-means, from which we derive our first method, a simple EM-like algorithm. This also allows us to determine when ONMF should be preferred to k-means and spherical k-means. Our second method is based on an augmented Lagrangian approach. Standard ONMF algorithms typically enforce nonnegativity for their iterates while trying to achieve orthogonality at the limit (e.g., using a proper penalization term or a suitably chosen search direction). Our method works the opposite way: orthogonality is strictly imposed at each step while nonnegativity is asymptotically obtained, using a quadratic penalty. Finally, we show that the two proposed approaches compare favorably with standard ONMF algorithms on synthetic, text and image data sets. {\textcopyright} 2014 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {1201.0901},
author = {Pompili, Filippo and Gillis, Nicolas and Absil, P. A. and Glineur, Fran{\c{c}}ois},
doi = {10.1016/j.neucom.2014.02.018},
eprint = {1201.0901},
file = {:Users/ricardo/Downloads/Two algorithms for orthogonal nonnegative matrix factorization with application to clustering.pdf:pdf},
isbn = {9782874190810},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Clustering,Document classification,Hyperspectral images,Nonnegative matrix factorization,Orthogonality},
month = {oct},
pages = {15--25},
publisher = {Elsevier},
title = {{Two algorithms for orthogonal nonnegative matrix factorization with application to clustering}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231214004068},
volume = {141},
year = {2014}
}
@book{Williams2006,
author = {Williams, Christopher K I and Rasmussen, Carl Edward},
editor = {Press, MIT},
title = {{Gaussian processes for machine learning}},
year = {2006}
}
@article{Johansson2016,
abstract = {Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, "Would this patient have lower blood sugar had she received a different medication?". We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1605.03661},
author = {Johansson, Fredrik D and Shalit, Uri and Sontag, David},
eprint = {1605.03661},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Johansson, Shalit, Sontag - Unknown - Learning Representations for Counterfactual Inference.pdf:pdf},
isbn = {9781510829008},
journal = {ICML},
title = {{Learning Representations for Counterfactual Inference}},
url = {http://proceedings.mlr.press/v48/johansson16.pdf http://arxiv.org/abs/1605.03661},
year = {2016}
}
@article{Bedrat2016,
abstract = {Critical evidence for the biological relevance of G-quadruplexes (G4) has recently been obtained in seminal studies performed in a variety of organisms. Four-stranded G-quadruplex DNA structures are promising drug targets as these non-canonical structures appear to be involved in a number of key biological processes. Given the growing interest for G4, accurate tools to predict G-quadruplex propensity of a given DNA or RNA sequence are needed. Several algorithms such as Quadparser predict quadruplex forming propensity. However, a number of studies have established that sequences that are not detected by these tools do form G4 structures (false negatives) and that other sequences predicted to form G4 structures do not (false positives). Here we report development and testing of a radically different algorithm, G4Hunter that takes into account G-richness and G-skewness of a given sequence and gives a quadruplex propensity score as output. To validate this model, we tested it on a large dataset of 392 published sequences and experimentally evaluated quadruplex forming potential of 209 sequences using a combination of biophysical methods to assess quadruplex formation in vitro. We experimentally validated the G4Hunter algorithm on a short complete genome, that of the human mitochondria (16.6 kb), because of its relatively high GC content and GC skewness as well as the biological relevance of these quadruplexes near instability hotspots. We then applied the algorithm to genomes of a number of species, including humans, allowing us to conclude that the number of sequences capable of forming stable quadruplexes (at least in vitro) in the human genome is significantly higher, by a factor of 2-10, than previously thought.},
author = {Bedrat, Amina and Lacroix, Laurent and Mergny, Jean Louis},
doi = {10.1093/nar/gkw006},
file = {:Users/ricardo/Downloads/Re-evaluation of G-quadruplex propensity with G4Hunter.pdf:pdf},
isbn = {1362-4962 (Electronic)$\backslash$r0305-1048 (Linking)},
issn = {13624962},
journal = {Nucleic Acids Res.},
number = {4},
pages = {1746--1759},
pmid = {26792894},
title = {{Re-evaluation of G-quadruplex propensity with G4Hunter}},
volume = {44},
year = {2016}
}
@article{Zela2018,
abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters , and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.05118v1},
author = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
eprint = {arXiv:1807.05118v1},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zela et al. - 2018 - AutoML Workshop.pdf:pdf},
journal = {ICML AutoML Work.},
keywords = {Bayesian Opti-mization,Hyperparameter Optimization,Neural Architecture Search,Object Recognition},
title = {{Towards automated deep learning: efficient joint neural architecture and hyperparameter search}},
url = {https://arxiv.org/pdf/1807.06906.pdf http://ray.readthedocs.io/en/latest/tune.html.},
year = {2018}
}
@article{Wang2017,
abstract = {There is increasing interest in learning how human brain networks vary as a function of a continuous trait, but flexible and efficient procedures to accomplish this goal are limited. We develop a Bayesian semiparametric model, which combines low-rank factorizations and flexible Gaussian process priors to learn changes in the conditional expectation of a network-valued random variable across the values of a continuous predictor, while including subject-specific random effects. The formulation leads to a general framework for inference on changes in brain network structures across human traits, facilitating borrowing of information and coherently characterizing uncertainty. We provide an efficient Gibbs sampler for posterior computation along with simple procedures for inference, prediction and goodness-of-fit assessments. The model is applied to learn how human brain networks vary across individuals with different intelligence scores. Results provide interesting insights on the association between intelligence and brain connectivity, while demonstrating good predictive performance.},
archivePrefix = {arXiv},
arxivId = {1606.00921},
author = {Wang, Lu and Durante, Daniele and Jung, Rex E and Dunson, David B},
doi = {10.1093/bioinformatics/btx050},
eprint = {1606.00921},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - Bayesian network–response regression.pdf:pdf},
issn = {14602059},
journal = {Bioinformatics},
number = {12},
pages = {1859--1866},
pmid = {28165112},
title = {{Bayesian network-response regression}},
url = {https://watermark.silverchair.com/btx050.pdf?token=AQECAHi208BE49Ooan9kkhW{\_}Ercy7Dm3ZL{\_}9Cf3qfKAc485ysgAAAeYwggHiBgkqhkiG9w0BBwagggHTMIIBzwIBADCCAcgGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMme8IE9lxjGLeLpMzAgEQgIIBmXI0RUBmNc6HoCgVUP1fnyKtIjYe0VtJ-U21uUMyLF6yv1KW},
volume = {33},
year = {2017}
}
@article{Yan2006,
abstract = {Although color is commonly experienced as an indispensable quality in describing the world around us, state-of-the art local feature-based representations are mostly based on shape description, and ignore color information. The description of color is hampered by the large amount of variations which causes the measured color values to vary significantly. In this paper we aim to extend the description of local features with color information. To accomplish a wide applicability of the color descriptor, it should be robust to: 1. photometric changes commonly encountered in the real world, 2. varying image quality, from high quality images to snap-shot photo quality and compressed internet images. Based on these requirements we derive a set of color descriptors. The set of proposed descriptors are compared by extensive testing on multiple applications areas, namely, matching, retrieval and classification, and on a wide variety of image qualities. The results show that color descriptors remain reliable under photometric and geometrical changes, and with decreasing image quality. For all experiments a combination of color and shape outperforms a pure shape-based approach.},
author = {Yan, Shuicheng and Tang, Xiaoou},
file = {:Users/ricardo/Downloads/Trace{\_}Quotient{\_}Problems{\_}Revisited.pdf:pdf},
isbn = {978-3-540-33838-3},
issn = {0302-9743},
journal = {Proc. Eur. Conf. Comput. Vis.},
title = {{Trace Quotient Problems Revisted}},
volume = {3954},
year = {2006}
}
@article{DAspremont2007,
abstract = {Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse ...},
archivePrefix = {arXiv},
arxivId = {arXiv:0707.0705v3},
author = {D'Aspremont, Alexandre and Bach, Francis R and Ghaoui, Laurent El},
doi = {10.1145/1273496.1273519},
eprint = {arXiv:0707.0705v3},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/D 'aspremont, Bach, Ghaoui - Unknown - Full Regularization Path for Sparse Principal Component Analysis.pdf:pdf},
isbn = {9781595937933},
journal = {Proc. 24th Int. Conf. Mach. Learn.},
number = {1},
pages = {177--184},
title = {{Full regularization path for sparse principal component analysis}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2007{\_}dAspremontBG07.pdf http://portal.acm.org/citation.cfm?doid=1273496.1273519},
year = {2007}
}
@article{Comon1994,
abstract = {The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept oflCA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution. Zusammenfassung Die Analyse unabhfingiger Komponenten (ICA) eines Vektors beruht auf der Suche nach einer linearen Transforma-tion, die die statistische Abh{\~{}}ingigkeit zwischen den Komponenten minimiert. Zur Definition geeigneter Such-Kriterien wird die Entwicklung gemeinsamer Information als Funktion von Kumulanten steigender Ordnung genutzt. Es wird ein effizienter Algorithmus vorgeschlagen, der die Berechnung der ICA ffir Datenmatrizen innerhalb einer polynomischen Zeit erlaubt. Das Konzept der ICA kann eigentlich als Erweiterung der 'Principal Component Analysis' (PCA) betrachtet werden, die nur die Unabh{\~{}}ingigkeit bis zur zweiten Ordnung erzwingen kann und deshalb Richtungen definiert, die orthogonal sind. Potentielle Anwendungen der ICA beinhalten Daten-Analyse und Kompression, Bayes-Detektion, Quellenlokalisierung und blinde Identifikation und Entfaltung. R{\~{}}sum{\~{}} L'Analyse en Composantes Ind6pendantes (ICA) d'un vecteur al6atoire consiste en la recherche d'une transformation lin6aire qui minimise la d6pendance statistique entre ses composantes. Afin de d6finir des crit6res d'optimisation appropribs, on utilise un d6veloppement en s6rie de l'information mutuelle en fonction de cumulants d'ordre croissant. On propose ensuite un algorithme pratique permettant le calcul de I'ICA d'une matrice de donn6es en un temps polynomial. Le concept d'ICA peut 6tre vu en r{\~{}}alitb comme une extension de l'Analyse en Composantes Principales (PCA) qui, elle, ne peut imposer l'ind6pendance qu'au second ordre et d6finit par cons6quent des directions orthogonales. Les applications potentielles de I'ICA incluent l'analyse et la compression de donn6es, la d{\&}ection bayesienne, la localisation de sources, et l'identification et la d6convolution aveugles.},
author = {Comon, Pierre},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Comon - 1994 - SIGNAL PROCESSING Independent component analysis, A new concept.pdf:pdf},
journal = {Signal Processing},
keywords = {Array processing,Blind identification,Data analysis,Entropy,High-order statistics,Independent components,Information,Mixture,Noise reduction,Principal components,Random variables,Source separation,Statistical independence},
pages = {287--314},
title = {{Independent component analysis - a new concept? Signal Processing}},
volume = {36},
year = {1994}
}
@article{Sigg2008,
abstract = {We study the problem of finding the dom- inant eigenvector of the sample covariance matrix, under additional constraints on the vector: a cardinality constraint limits the number of non-zero elements, and non- negativity forces the elements to have equal sign. This problem is},
author = {Sigg, Christian D and Buhmann, Joachim M},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Sigg, Buhmann - Unknown - Expectation-Maximization for Sparse and Non-Negative PCA.pdf:pdf},
journal = {ICML},
pages = {960--967},
title = {{Expectation-maximization for sparse and non-negative PCA}},
year = {2008}
}
@article{Park2007,
abstract = {We introduce a path following algorithm for L1-regularized generalized linear mod- els.The L1-regularization procedure is useful especially because it, in effect, selects variables according to the amount of penalization on the L1-norm of the coefficients, in a manner that is less greedy than forward selection–backward deletion. The generalized linear model path algorithm efficiently computes solutions along the entire regularization path by using the pre- dictor–corrector method of convex optimization. Selecting the step length of the regularization parameter is critical in controlling the overall accuracy of the paths; we suggest intuitive and flexible strategies for choosing appropriate values. We demonstrate the implementation with several simulated and real data sets.},
author = {Park, Mee Young and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2007.00607.x},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Park, Hastie - 2007 - L sub1sub -regularization path algorithm for generalized linear models.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
keywords = {Generalized linear model,Lasso,Path algorithm,Predictor-corrector method,Regularization,Variable selection},
month = {sep},
number = {4},
pages = {659--677},
publisher = {Blackwell Publishing Ltd},
title = {{L1-regularization path algorithm for generalized linear models}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2007.00607.x},
volume = {69},
year = {2007}
}
@article{Hoyer2008,
abstract = {An important task in data analysis is the discovery of causal relationships between observed variables. For continuous-valued data, linear acyclic causal models are commonly used to model the data-generating process, and the inference of such models is a wellstudied problem. However, existing methods have significant limitations. Methods based on conditional independencies (Spirtes et al. 1993; Pearl 2000) cannot distinguish between independence-equivalent models, whereas approaches purely based on Independent Component Analysis (Shimizu et al. 2006) are inapplicable to data which is partially Gaussian. In this paper, we generalize and combine the two approaches, to yield a method able to learn the model structure in many cases for which the previous methods provide answers that are either incorrect or are not as informative as possible. We give exact graphical conditions for when two distinct models represent the same family of distributions, and empirically demonstrate the power of our method through thorough simulations.},
author = {Hoyer, Patrik O and Hyv{\"{a}}rinen, Aapo and Scheines, Richard and Spirtes, Peter and Ramsey, Joseph and Lacerda, Gustavo and Shimizu, Shohei},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hoyer et al. - Unknown - Causal discovery of linear acyclic models with arbitrary distributions.pdf:pdf},
isbn = {0-9749039-4-9},
journal = {UAI},
keywords = {acyclic models with arbitrary,distributions,sal discovery of linear},
pages = {282--289},
pmid = {9223372036854775808},
title = {{Causal discovery of linear acyclic models with arbitrary distributions}},
url = {https://arxiv.org/pdf/1206.3260.pdf http://arxiv.org/abs/1206.3260},
year = {2008}
}
@article{Ding2004,
abstract = {Principal component analysis (PCA) is a widely used statistical technique for unsupervised dimension reduction. K-means clustering is a commonly used data clustering for performing unsupervised learning tasks. Here we prove that principal components are the continuous solutions to the discrete cluster membership indicators for K-means clustering. New lower bounds for K-means objective function are derived, which is the total variance minus the eigenvalues of the data covariance matrix. These results indicate that unsupervised dimension reduction is closely related to unsupervised learning. Several implications are discussed. On dimension reduction, the result provides new insights to the observed effectiveness of PCA-based data reductions, beyond the conventional noisereduction explanation that PCA, via singular value decomposition, provides the best low-dimensional linear approximation of the data. On learning, the result suggests effective techniques for K-means data clustering. DNA gene expression and Internet newsgroups are analyzed to illustrate our results. Experiments indicate that the new bounds are within 0.5-1.5 of the optimal values.},
author = {Ding, Chris and He, Xiaofeng},
doi = {10.1145/1015330.1015408},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ding, He - Unknown - K-means Clustering via Principal Component Analysis.pdf:pdf},
isbn = {1581138285},
issn = {1581138385},
journal = {ICML},
pages = {29},
title = {{K-means clustering via principal component analysis}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2004{\_}DingH04a.pdf http://portal.acm.org/citation.cfm?doid=1015330.1015408},
volume = {Cl},
year = {2004}
}
@misc{VandenHeuvel2010,
abstract = {Our brain is a network. It consists of spatially distributed, but functionally linked regions that continuously share information with each other. Interestingly, recent advances in the acquisition and analysis of functional neuroimaging data have catalyzed the exploration of functional connectivity in the human brain. Functional connectivity is defined as the temporal dependency of neuronal activation patterns of anatomically separated brain regions and in the past years an increasing body of neuroimaging studies has started to explore functional connectivity by measuring the level of co-activation of resting-state fMRI time-series between brain regions. These studies have revealed interesting new findings about the functional connections of specific brain regions and local networks, as well as important new insights in the overall organization of functional communication in the brain network. Here we present an overview of these new methods and discuss how they have led to new insights in core aspects of the human brain, providing an overview of these novel imaging techniques and their implication to neuroscience. We discuss the use of spontaneous resting-state fMRI in determining functional connectivity, discuss suggested origins of these signals, how functional connections tend to be related to structural connections in the brain network and how functional brain communication may form a key role in cognitive performance. Furthermore, we will discuss the upcoming field of examining functional connectivity patterns using graph theory, focusing on the overall organization of the functional brain network. Specifically, we will discuss the value of these new functional connectivity tools in examining believed connectivity diseases, like Alzheimer's disease, dementia, schizophrenia and multiple sclerosis. {\textcopyright} 2010 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {0811.3721},
author = {van den Heuvel, Martijn P. and {Hulshoff Pol}, Hilleke E},
booktitle = {Eur. Neuropsychopharmacol.},
doi = {10.1016/j.euroneuro.2010.03.008},
eprint = {0811.3721},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Van Den Heuvel, Hulshoff Pol - 2010 - Exploring the brain network A review on resting-state fMRI functional connectivity.pdf:pdf},
isbn = {1873-7862 (Electronic)$\backslash$r0924-977X (Linking)},
issn = {0924977X},
keywords = {Anatomical connectivity,Complex systems,Complexity,DTI,Diffusion tensor imaging,FMRI,Functional brain networks,Functional connectivity,Graph analysis,Network,Network analysis,Resting-state connectivity,Resting-state fMRI,Review,White matter},
number = {8},
pages = {519--534},
pmid = {20471808},
title = {{Exploring the brain network: A review on resting-state fMRI functional connectivity}},
volume = {20},
year = {2010}
}
@article{Durante2016,
abstract = {Network data are increasingly collected along with other variables of interest. Our motivation is drawn from neurophysiology studies measuring brain connectivity networks for a sample of individuals along with their membership to a low or high creative reasoning group. It is of paramount importance to develop statistical methods for testing of global and local changes in the structural inter-connections among brain regions across groups. We develop a general Bayesian procedure for inference and testing of group differences in the network structure, which relies on a nonparametric representation for the conditional probability mass function associated with a network-valued random variable. By leveraging a mixture of low-rank factorizations, we allow simple global and local hypothesis testing adjusting for multiplicity. An efficient Gibbs sampler is defined for pos-terior computation. We provide theoretical results on the flexibility of the model and assess testing performance in simulations. The approach is applied to provide novel insights on the relationships between human brain networks and creativity.},
archivePrefix = {arXiv},
arxivId = {1411.6506},
author = {Durante, Daniele and Dunson, David B},
doi = {10.1214/16-BA1030},
eprint = {1411.6506},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Durante, Dunson - 2016 - Bayesian Inference and Testing of Group Differences in Brain Networks.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Anal.},
keywords = {brain network,mixture model,multiple testing,nonparametric Bayes},
pages = {1--30},
title = {{Bayesian Inference and Testing of Group Differences in Brain Networks}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.ba/1479179031 http://projecteuclid.org/euclid.ba/1479179031},
year = {2016}
}
@article{Steeg2014a,
abstract = {We consider a set of probabilistic functions of some input variables as a representation of the inputs. We present bounds on how informative a representation is about input data. We extend these bounds to hierarchical representations so that we can quantify the contribution of each layer towards capturing the information in the original data. The special form of these bounds leads to a simple, bottom-up optimization procedure to construct hierarchical representations that are also maximally informative about the data. This optimization has linear computational complexity and constant sample complexity in the number of variables. These results establish a new approach to unsupervised learning of deep representations that is both principled and practical. We demonstrate the usefulness of the approach on both synthetic and real-world data.},
archivePrefix = {arXiv},
arxivId = {1410.7404},
author = {Steeg, Greg Ver and Galstyan, Aram},
eprint = {1410.7404},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Steeg, Galstyan - Unknown - Maximally Informative Hierarchical Representations of High-Dimensional Data.pdf:pdf},
isbn = {1410.7404},
issn = {15337928},
journal = {NIPS},
title = {{Maximally Informative Hierarchical Representations of High-Dimensional Data}},
url = {https://arxiv.org/pdf/1410.7404.pdf http://arxiv.org/abs/1410.7404},
year = {2014}
}
@article{Bradley2014,
author = {Bradley, Chris and Abrams, Jared and Geisler, Wilson S},
doi = {10.1167/14.12.22.doi},
file = {:Users/ricardo/Downloads/i1534-7362-14-12-22.pdf:pdf},
isbn = {1534-7362},
journal = {J. Vis.},
number = {22},
pages = {1--28},
title = {{Retina ‐ V1 Model of Detectability across the Visual Field The RV1 Model}},
volume = {14},
year = {2014}
}
@inproceedings{Rahim2017,
abstract = {Segmentation of anatomy on abdominal CT enables patient-specific image guidance in clinical endoscopic procedures and in endoscopy training. Because robust interpatient registration of abdom-inal images is necessary for existing multi-atlas-and statistical-shape-model-based segmentations, but remains challenging, there is a need for automated multi-organ segmentation that does not rely on regis-tration. We present a deep-learning-based algorithm for segmenting the liver, pancreas, stomach, and esophagus using dilated convolution units with dense skip connections and a new spatial prior. The algorithm was evaluated with an 8-fold cross-validation and compared to a joint-label-fusion-based segmentation based on Dice scores and boundary distances. The proposed algorithm yielded more accurate segmentations than the joint-label-fusion-based algorithm for the pancreas (median Dice scores 66 vs 37), stomach (83 vs 72) and esophagus (73 vs 54) and marginally less accurate segmentation for the liver (92 vs 93). We conclude that dilated convolutional networks with dense skip connections can segment the liver, pancreas, stomach and esophagus from abdominal CT with-out image registration and have the potential to support image-guided navigation in gastrointestinal endoscopy procedures.},
author = {Rahim, Mehdi and Thirion, Bertrand and Varoquaux, Ga{\"{e}}l},
booktitle = {MICCAI},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Rahim, Thirion, Varoquaux - 2017 - Medical Image Computing and Computer Assisted Intervention − MICCAI 2017.pdf:pdf},
title = {{Population-shrinkage of covariance to estimate better brain functional connectivity}},
year = {2017}
}
@article{Sporns2016,
abstract = {The development of new technologies for mapping structural and functional brain connectivity has led to the creation of comprehensive network maps of neuronal circuits and systems. The architecture of these brain networks can be examined and analyzed with a large variety of graph theory tools. Methods for detecting modules, or network communities, are of particular interest because they uncover major building blocks or subnetworks that are particularly densely connected, often corresponding to specialized functional components. A large number of methods for community detection have become available and are now widely applied in network neuroscience. This article first surveys a number of these methods, with an emphasis on their advantages and shortcomings; then it summarizes major findings on the existence of modules in both structural and functional brain networks and briefly considers their potential functional roles in brain evolution, wiring minimization, and the emergence of functional specialization and complex dynamics. Expected final online publication date for the Annual Review of Psychology Volume 67 is January 03, 2016. Please see http://www.annualreviews.org/catalog/pubdates.aspx for revised estimates.},
author = {Sporns, Olaf and Betzel, Richard F},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Sporns, Betzel - 2016 - Modular Brain Networks.pdf:pdf},
journal = {Annu. Rev. Psychol.},
keywords = {clustering,connectome,functional connectivity,graph theory,hubs,resting state},
number = {1},
pages = {613--640},
title = {{Modular Brain Networks}},
volume = {67},
year = {2016}
}
@techreport{Ollivier2017,
abstract = {We present a canonical way to turn any smooth parametric family of probability distributions on an arbitrary search space {\'{i}} µ{\'{i}}± into a continuous-time black-box optimization method on {\'{i}} µ{\'{i}}±, the information-geometric optimization (IGO) method. Invariance as a major design principle keeps the number of arbitrary choices to a minimum. The resulting IGO flow is the flow of an ordinary differential equation conducting the natural gradient ascent of an adaptive, time-dependent transformation of the objective function. It makes no particular assumptions on the objective function to be optimized. The IGO method produces explicit IGO algorithms through time discretization. It naturally recovers versions of known algorithms and offers a systematic way to derive new ones. In continuous search spaces, IGO algorithms take a form related to natural evolution strategies (NES). The cross-entropy method is recovered in a particular case with a large time step, and can be extended into a smoothed, parametrization-independent maximum likelihood update (IGO-ML). When applied to the family of Gaussian distributions on R {\'{i}} µ{\'{i}}± , the IGO framework recovers a version of the well-known CMA-ES algorithm and of xNES. For the family of Bernoulli distributions on {\{}0, 1{\}} {\'{i}} µ{\'{i}}± , we recover the seminal PBIL algorithm and cGA. For the distributions of restricted Boltzmann machines, we naturally obtain a novel algorithm for discrete optimization on {\{}0, 1{\}} {\'{i}} µ{\'{i}}±. All these algorithms are natural instances of, and unified under, the single information-geometric optimization framework. The IGO method achieves, thanks to its intrinsic formulation, maximal invariance properties: invariance under reparametrization of the search space {\'{i}} µ{\'{i}}±, under a change of parameters of the probability distribution, and under increasing transformation of the function to be optimized. The latter is achieved through an adaptive, quantile-based formulation of the objective. Theoretical considerations strongly suggest that IGO algorithms are essentially characterized by a minimal change of the distribution over time. Therefore they have minimal loss in diversity through the course of optimization, provided the initial diversity is high. First experiments using restricted Boltzmann machines confirm this insight. As a simple conse-c ○2017 Yann Ollivier, Ludovic Arnold, Anne Auger and Nikolaus Hansen. License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v18/14-467.html. Ollivier, Arnold, Auger and Hansen quence, IGO seems to provide, from information theory, an elegant way to simultaneously explore several valleys of a fitness landscape in a single run.},
author = {Ollivier, Yann and Arnold, Ludovic and Auger, Anne and Hansen, Nikolaus},
booktitle = {J. Mach. Learn. Res.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ollivier et al. - 2017 - Information-Geometric Optimization Algorithms A Unifying Picture via Invariance Principles.pdf:pdf},
keywords = {black-box optimization,evolution strategy,information-geometric optimization,invariance,natural gradient,randomized optimization,stochastic optimization},
pages = {1--65},
title = {{Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles}},
url = {http://jmlr.org/papers/v18/14-467.html.},
volume = {18},
year = {2017}
}
@article{Keown2013,
abstract = {Although growing evidence indicates atypical long-distance connectivity in autism spectrum disorder (ASD), much less is known about local connectivity, despite conjectures that local overconnectivity may be causally involved in the disorder. Using functional connectivity MRI and graph theory, we found that local functional connectivity was atypically increased in adolescents with ASD in temporo-occipital regions bilaterally. Posterior overconnectivity was found to be associated with higher ASD symptom severity, whereas an ASD subsample with low severity showed frontal underconnectivity. The findings suggest links between symptomatology and local connectivity, which vary within the autism spectrum},
author = {Keown, Christopher Lee and Shih, Patricia and Nair, Aarti and Peterson, Nick and Mulvey, Mark Edward and M{\"{u}}ller, Ralph Axel},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Keown et al. - 2013 - Local Functional Overconnectivity in Posterior Brain Regions Is Associated with Symptom Severity in Autism Spectru.pdf:pdf},
journal = {Cell Rep.},
number = {3},
pages = {567--572},
title = {{Local functional overconnectivity in posterior brain regions is associated with symptom severity in autism spectrum disorders}},
volume = {5},
year = {2013}
}
@article{Shalit2017,
abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a "balanced" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1606.03976},
author = {Shalit, Uri and Johansson, Fredrik D and Sontag, David},
eprint = {1606.03976},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Shalit, Johansson, Sontag - Unknown - Estimating individual treatment effect generalization bounds and algorithms.pdf:pdf},
journal = {ICML2},
title = {{Estimating individual treatment effect: generalization bounds and algorithms}},
url = {https://arxiv.org/pdf/1606.03976.pdf http://arxiv.org/abs/1606.03976},
year = {2017}
}
@article{Melis2017,
abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
archivePrefix = {arXiv},
arxivId = {1707.05589},
author = {Melis, G{\'{a}}bor and Dyer, Chris and Blunsom, Phil},
eprint = {1707.05589},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Melis, Dyer, Deepmind - Unknown - On the State of the Art of Evaluation in Neural Language Models.pdf:pdf},
title = {{On the State of the Art of Evaluation in Neural Language Models}},
url = {https://arxiv.org/pdf/1707.05589.pdf http://arxiv.org/abs/1707.05589},
year = {2017}
}
@techreport{Kusner,
abstract = {Most approaches in algorithmic fairness constrain machine learning methods so the resulting predictions satisfy one of several intuitive notions of fairness. While this may help private companies comply with non-discrimination laws or avoid negative publicity, we believe it is often too little, too late. By the time the training data is collected, individuals in disadvantaged groups have already suffered from discrimination and lost opportunities due to factors out of their control. In the present work we focus instead on interventions such as a new public policy, and in particular, how to maximize their positive effects while improving the fairness of the overall system. We use causal methods to model the effects of interventions, allowing for potential interference-each individual's outcome may depend on who else receives the intervention. We demonstrate this with an example of allocating a budget of teaching resources using a dataset of schools in New York City.},
author = {Kusner, Matt J and Russell, Chris and Loftus, Joshua R and Silva, Ricardo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kusner et al. - Unknown - Causal Interventions for Fairness.pdf:pdf},
title = {{Causal Interventions for Fairness}}
}
@article{Hernandez-Lobato2016,
abstract = {We provide theoretical and empirical evidence for a type of asymmetry between causes and effects that is present when these are related via linear models contaminated with additive non-Gaussian noise. Assuming that the causes and the effects have the same distribution, we show that the distribution of the residuals of a linear fit in the anti-causal direction is closer to a Gaussian than the distribution of the residuals in the causal direction. This Gaussianization effect is characterized by reduction of the magnitude of the high-order cumulants and by an increment of the differential entropy of the residuals. The problem of non-linear causal inference is addressed by performing an embedding in an expanded feature space, in which the relation between causes and effects can be assumed to be linear. The effectiveness of a method to discriminate between causes and effects based on this type of asymmetry is illustrated in a variety of experiments using different measures of Gaussianity. The proposed method is shown to be competitive with state-of-the-art techniques for causal inference.},
author = {Hern{\'{a}}ndez-Lobato, Daniel and Morales-Mombiela, Pablo and Lopez-Paz, David and Su{\'{a}}rez, Alberto},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hern{\'{a}}ndez-Lobato et al. - 2016 - Non-linear Causal Inference using Gaussianity Measures(2).pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {Gaussianity of the residuals,causal inference,cause-effect pairs},
pages = {1--39},
title = {{Non-linear Causal Inference using Gaussianity Measures}},
url = {http://www.jmlr.org/papers/volume17/14-375/14-375.pdf},
volume = {17},
year = {2016}
}
@article{Altenburger2018,
abstract = {The observation that individuals tend to be friends with people who are similar to themselves, commonly known as homophily, is a prominent feature of social networks. While homophily describes a bias in attribute preferences for similar others, it gives limited attention to variability. Here, we observe that attribute preferences can exhibit variation beyond what can be explained by homophily. We call this excess variation monophily to describe the presence of individuals with extreme preferences for a particular attribute possibly unrelated to their own attribute. We observe that monophily can induce a similarity among friends-of-friends without requiring any similarity among friends. To simulate homophily and monophily in synthetic networks, we propose an overdispersed extension of the classical stochastic block model. We use this model to demonstrate how homophily-based methods for predicting attributes on social networks based on friends (that is, 'the company you keep') are fundamentally different from monophily-based methods based on friends-of-friends (that is, 'the company you're kept in'). We place particular focus on predicting gender, where homophily can be weak or non-existent in practice. These findings offer an alternative perspective on network structure and prediction, complicating the already difficult task of protecting privacy on social networks.},
author = {Altenburger, K. M. and Ugander, J.},
doi = {10.1038/s41562-018-0321-8},
file = {:Users/ricardo/Downloads/Altenburger{\_}et{\_}al-2018-Nature{\_}Human{\_}Behaviour.pdf:pdf},
issn = {2397-3374},
journal = {Nat. Hum. Behav.},
publisher = {Springer US},
title = {{Monophily in social networks introduces similarity among friends-of-friends}},
url = {http://dx.doi.org/10.1038/s41562-018-0321-8},
year = {2018}
}
@article{Leonardi2013,
abstract = {Functional connectivity (FC) as measured by correlation between fMRI BOLD time courses of distinct brain regions has revealed meaningful organization of spontaneous fluctuations in the resting brain. However, an increasing amount of evidence points to non-stationarity of FC; i.e., FC dynamically changes over time reflecting additional and rich information about brain organization, but representing new challenges for analysis and interpretation. Here, we propose a data-driven approach based on principal component analysis (PCA) to reveal hidden patterns of coherent FC dynamics across multiple subjects. We demonstrate the feasibility and relevance of this new approach by examining the differences in dynamic FC between 13 healthy control subjects and 15 minimally disabled relapse-remitting multiple sclerosis patients. We estimated whole-brain dynamic FC of regionally-averaged BOLD activity using sliding time windows. We then used PCA to identify FC patterns, termed "eigenconnectivities", that reflect meaningful patterns in FC fluctuations. We then assessed the contributions of these patterns to the dynamic FC at any given time point and identified a network of connections centered on the default-mode network with altered contribution in patients. Our results complement traditional stationary analyses, and reveal novel insights into brain connectivity dynamics and their modulation in a neurodegenerative disease. {\textcopyright} 2013 Elsevier Inc.},
author = {Leonardi, Nora and Richiardi, Jonas and Gschwind, Markus and Simioni, Samanta and Annoni, Jean Marie and Schluep, Myriam and Vuilleumier, Patrik and {Van De Ville}, Dimitri},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Leonardi et al. - 2013 - Principal components of functional connectivity A new approach to study dynamic brain connectivity during re(2).pdf:pdf},
journal = {Neuroimage},
keywords = {Dynamics,FMRI,Functional connectivity,Multiple sclerosis,Resting state},
pages = {937--950},
title = {{Principal components of functional connectivity: A new approach to study dynamic brain connectivity during rest}},
volume = {83},
year = {2013}
}
@article{Smith2017,
abstract = {This paper tackles two related questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work is inspired by Zhang et al. (2017), who showed deep networks can easily memorize randomly labeled training data, despite generalizing well when shown real labels of the same inputs. We show here that the same phenomenon occurs in small linear models. These observations are explained by evaluating the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization. We also explore the "generalization gap" between small and large batch training, identifying an optimum batch size which maximizes the test set accuracy. Interpreting stochastic gradient descent as a stochastic differential equation, we identify a "noise scale" {\$}g = \backslashepsilon (\backslashfrac{\{}N{\}}{\{}B{\}} - 1) \backslashapprox \backslashepsilon N/B{\$}, where {\$}\backslashepsilon{\$} is the learning rate, {\$}N{\$} training set size and {\$}B{\$} batch size. Consequently the optimum batch size is proportional to the learning rate and the training set size, {\$}B{\_}{\{}opt{\}} \backslashpropto \backslashepsilon N{\$}. We verify these predictions empirically.},
archivePrefix = {arXiv},
arxivId = {1710.06451},
author = {Smith, Samuel L. and Le, Quoc V.},
eprint = {1710.06451},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Smith, Le, Brain - Unknown - A BAYESIAN PERSPECTIVE ON GENERALIZATION AND STOCHASTIC GRADIENT DESCENT.pdf:pdf},
title = {{A Bayesian Perspective on Generalization and Stochastic Gradient Descent}},
url = {http://arxiv.org/abs/1710.06451},
year = {2017}
}
@article{Molchanov,
abstract = {We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation - a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10x theoretical (5x practical) reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the large-scale ImageNet dataset to emphasize the flexibility of our approach.},
archivePrefix = {arXiv},
arxivId = {1611.06440},
author = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
doi = {10.1051/0004-6361/201527329},
eprint = {1611.06440},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Molchanov et al. - 2016 - Pruning Convolutional Neural Networks for Resource Efficient Inference.pdf:pdf},
isbn = {9781611970685},
issn = {0004-6361},
journal = {ICLR},
title = {{Pruning Convolutional Neural Networks for Resource Efficient Inference}},
url = {https://arxiv.org/pdf/1611.06440.pdf http://arxiv.org/abs/1611.06440},
year = {2016}
}
@article{Bareinboim,
abstract = {The exponential growth of electronically accessible information has led some to conjecture that data alone can replace sub- stantive knowledge in practical decision making and scientific explorations. In this paper, we argue that traditional scientific methodologies that have been successful in the natural and bio- medical sciences would still be necessary for big data applications, albeit tasked with new challenges: to go beyond predictions and, using information from multiple sources, provide users with rea- soned recommendations for actions and policies. The feasibility of meeting these challenges is demonstrated here using specific data fusion tasks, following a brief introduction to the structural causal model (SCM) framework (1–3).},
archivePrefix = {arXiv},
arxivId = {1605.03373},
author = {Bareinboim, Elias and Pearl, Judea},
doi = {10.1073/pnas.1510507113},
eprint = {1605.03373},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bareinboim, Pearl - Unknown - Causal inference and the data-fusion problem.pdf:pdf},
isbn = {10916490 (Electronic)},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci.},
number = {27},
pages = {7345--7352},
pmid = {27382148},
title = {{Causal inference and the data-fusion problem}},
url = {www.pnas.org/cgi/doi/10.1073/pnas.1510507113 http://www.pnas.org/lookup/doi/10.1073/pnas.1510507113},
volume = {113},
year = {2016}
}
@article{Rubin2017,
abstract = {A central goal of cognitive neuroscience is to decode human brain activity—that is, to infer mental processes from observed patterns of whole-brain activation. Previous decoding efforts have focused on classifying brain activity into a small set of discrete cognitive states. To attain maximal utility, a decoding framework must be open-ended, systematic, and con-text-sensitive—that is, capable of interpreting numerous brain states, presented in arbitrary combinations, in light of prior information. Here we take steps towards this objective by intro-ducing a probabilistic decoding framework based on a novel topic model—Generalized Cor-respondence Latent Dirichlet Allocation—that learns latent topics from a database of over 11,000 published fMRI studies. The model produces highly interpretable, spatially-circum-scribed topics that enable flexible decoding of whole-brain images. Importantly, the Bayes-ian nature of the model allows one to " seed " decoder priors with arbitrary images and text— enabling researchers, for the first time, to generate quantitative, context-sensitive interpreta-tions of whole-brain patterns of brain activity.},
author = {Rubin, Timothy N. and Koyejo, Oluwasanmi and Gorgolewski, Krzysztof J. and Jones, Michael N. and Poldrack, Russell A. and Yarkoni, Tal},
doi = {10.1371/journal.pcbi.1005649},
file = {:Users/ricardo/Downloads/journal.pcbi.1005649.pdf:pdf},
isbn = {1111111111},
issn = {15537358},
journal = {PLoS Comput. Biol.},
number = {10},
pages = {1--24},
title = {{Decoding brain activity using a large-scale probabilistic functional-anatomical atlas of human cognition}},
volume = {13},
year = {2017}
}
@article{Cai2008,
abstract = {Matrix factorization techniques have been frequently applied in information retrieval, computer vision, and pattern recognition. Among them, Nonnegative Matrix Factorization (NMF) has received considerable attention due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts based in the human brain. On the other hand, from the geometric perspective, the data is usually sampled from a low-dimensional manifold embedded in a high-dimensional ambient space. One then hopes to find a compact representation,which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure. In this paper, we propose a novel algorithm, called Graph Regularized Nonnegative Matrix Factorization (GNMF), for this purpose. In GNMF, an affinity graph is constructed to encode the geometrical information and we seek a matrix factorization, which respects the graph structure. Our empirical study shows encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms on real-world problems.},
archivePrefix = {arXiv},
arxivId = {3},
author = {Cai, Deng and He, Xiaofei and Han, Jiawei},
doi = {10.1109/TPAMI.2010.231},
eprint = {3},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Cai et al. - Unknown - Graph Regularized Non-negative Matrix Factorization for Data Representation.pdf:pdf},
isbn = {0162-8828},
issn = {1939-3539},
journal = {IEEE Trans. Pattern Anal. Mach. Learn.},
keywords = {Clustering,Graph Laplacian,Index Terms—Non-negative Matrix Factorization,Manifold Regularization},
number = {8},
pages = {1--17},
pmid = {21173440},
title = {{Graph Regularized Non-negative Matrix Factorization for Data}},
url = {http://www.cad.zju.edu.cn/home/dengcai/Publication/Journal/TPAMI-GNMF.pdf},
volume = {33},
year = {2008}
}
@article{Lyu2009,
abstract = {Score matching is a recently developed pa- rameter learning method that is particu- larly effective to complicated high dimen- sional density models with intractable par- tition functions. In this paper, we study two issues that have not been completely re- solved for score matching. First, we provide a formal link between maximum likelihood and score matching. Our analysis shows that score matching finds model parameters that are more robust with noisy training data. Second, we develop a generalization of score matching. Based on this generalization, we further demonstrate an extension of score matching to models of discrete data.},
archivePrefix = {arXiv},
arxivId = {1205.2629},
author = {Lyu, Siwei},
eprint = {1205.2629},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lyu - 2009 - Interpretation and Generalization of Score Matching.pdf:pdf},
isbn = {978-0-9749039-5-8},
journal = {Conf. Uncertain. Artif. Intell.},
number = {June},
pages = {18--21},
title = {{Interpretation and generalization of score matching}},
url = {http://www.cs.mcgill.ca/ http://dl.acm.org/citation.cfm?id=1795156},
year = {2009}
}
@article{Selbst2018,
author = {Selbst, Andrew D. and Barocas, Solon},
file = {:Users/ricardo/Downloads/SSRN-id3126971.pdf:pdf},
keywords = {algorithmic accountability,big data,discirmination,explanations,law and technology,machine learning,privacy},
number = {January},
title = {{The Intuitive Appeal of Explainable Machines}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=3126971},
year = {2018}
}
@article{Ravi2017,
abstract = {Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a classifier has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity classifiers requires many iterative steps over many examples to perform well. Here, we propose an LSTM- based meta-learner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametriza- tion of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick con- vergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.},
author = {Ravi, Sachin and Larochelle, Hugo},
file = {:Users/ricardo/Downloads/ravi{\_}larochelle{\_}2018.pdf:pdf},
journal = {ICLR},
pages = {1--11},
title = {{Optimization As a Model for Few-Shot Learning}},
url = {https://openreview.net/pdf?id=rJY0-Kcll},
year = {2017}
}
@article{Malla,
abstract = {Deep convolutional networks provide state-of-the-art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and nonlinearities. A mathematical framework is introduced to analyse their properties. Computations of invariants involve multiscale contractions with wavelets, the linearization of hierarchical symmetries and sparse separations. Applications are discussed.},
archivePrefix = {arXiv},
arxivId = {1601.04920},
author = {Mallat, St{\'{e}}phane},
doi = {10.1098/rsta.2015.0203},
eprint = {1601.04920},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Malla - Unknown - Understanding Deep Convolutional Networks.pdf:pdf},
isbn = {1581136625},
issn = {1364-503X},
journal = {Philos. Trans. A. Math. Phys. Eng. Sci.},
number = {2065},
pages = {17},
pmid = {26953183},
title = {{Understanding deep convolutional networks.}},
url = {https://arxiv.org/pdf/1601.04920.pdf},
volume = {374},
year = {2016}
}
@article{He2011a,
abstract = {Nonnegative matrix factorization (NMF) is an unsupervised learning method useful in various applications including image processing and semantic analysis of documents. This paper focuses on symmetric NMF (SNMF), which is a special case of NMF decomposition. Three parallel multiplicative update algorithms using level 3 basic linear algebra subprograms directly are developed for this problem. First, by minimizing the Euclidean distance, a multiplicative update algorithm is proposed, and its convergence under mild conditions is proved. Based on it, we further propose another two fast parallel methods: $\alpha$-SNMF and $\beta$ -SNMF algorithms. All of them are easy to implement. These algorithms are applied to probabilistic clustering. We demonstrate their effectiveness for facial image clustering, document categorization, and pattern clustering in gene expression.},
author = {He, Zhaoshui and Xie, Shengli and Zdunek, Rafal and Zhou, Guoxu and Cichocki, Andrzej},
doi = {10.1109/TNN.2011.2172457},
file = {:Users/ricardo/Downloads/06061964.pdf:pdf},
isbn = {1941-0093 (Electronic)$\backslash$r1045-9227 (Linking)},
issn = {10459227},
journal = {IEEE Trans. Neural Networks},
keywords = {Basic linear algebra subprograms,completely positive,coordinate update,multiplicative update,nonnegative matrix factorization,parallel update,probabilistic clustering,symmetric nonnegative matrix factorization},
number = {12 PART 1},
pages = {2117--2131},
pmid = {22042156},
title = {{Symmetric nonnegative matrix factorization: Algorithms and applications to probabilistic clustering}},
volume = {22},
year = {2011}
}
@article{Heinze-Deml,
abstract = {An important problem in many domains is to predict how a system will respond to interventions. This task is inherently linked to estimating the system's underlying causal structure. To this end, 'invariant causal prediction' (ICP) (Peters et al., 2016) has been proposed which learns a causal model exploiting the invariance of causal relations using data from different environments. When considering linear models, the implementation of ICP is relatively straight-forward. However, the nonlinear case is more challenging due to the difficulty of performing nonparametric tests for conditional independence. In this work, we present and evaluate an array of methods for nonlinear and nonparametric versions of ICP for learning the causal parents of given target variables. We find that an approach which first fits a nonlinear model with data pooled over all environments and then tests for differences between the residual distributions across environments is quite robust across a large variety of simulation settings. We call this procedure "Invariant residual distribution test". In general, we observe that the performance of all approaches is critically dependent on the true (unknown) causal structure and it becomes challenging to achieve high power if the parental set includes more than two variables. As a real-world example, we consider fertility rate modelling which is central to world population projections. We explore predicting the effect of hypothetical interventions using the accepted models from nonlinear ICP. The results reaffirm the previously observed central causal role of child mortality rates.},
archivePrefix = {arXiv},
arxivId = {1706.08576},
author = {Heinze-Deml, Christina and Peters, Jonas and Meinshausen, Nicolai},
eprint = {1706.08576},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Heinze-Deml, Peters, Meinshausen - Unknown - Invariant Causal Prediction for Nonlinear Models(2).pdf:pdf},
title = {{Invariant Causal Prediction for Nonlinear Models}},
url = {https://arxiv.org/pdf/1706.08576.pdf http://arxiv.org/abs/1706.08576},
year = {2017}
}
@misc{Dicarlo2012,
abstract = {Mounting evidence suggests that 'core object recognition,' the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex. However, the algorithm that produces this solution remains poorly understood. Here we review evidence ranging fromindividual neurons and neuronal populations to behavior and computational models. We propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical subnetworks with a common functional goal. {\textcopyright} 2012 Elsevier Inc.},
author = {DiCarlo, James J. and Zoccolan, Davide and Rust, Nicole C},
booktitle = {Neuron},
doi = {10.1016/j.neuron.2012.01.010},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Dicarlo, Zoccolan, Rust - 2012 - How Does the Brain Solve Visual Object Recognition(2).pdf:pdf},
isbn = {1097-4199 (Electronic)$\backslash$r0896-6273 (Linking)},
issn = {08966273},
number = {3},
pages = {415--434},
pmid = {22325196},
title = {{How does the brain solve visual object recognition?}},
url = {https://doc-10-2c-apps-viewer.googleusercontent.com/viewer/secure/pdf/5ncqahjrr4a8j8123pjd90kknekhjgh1/u9ri5egqfpi93hpa2tj10e39okhtu345/1518007575000/gmail/17108864164372769262/ACFrOgDhc7e{\_}v4OBQ9q4PmsOSz87rMw4i9{\_}y4v84r0LqwY4yJhCQi2gelhXqwAyeVqdgVF0gg0EI7a},
volume = {73},
year = {2012}
}
@article{Goodfellow2013,
abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
author = {Goodfellow, Ian J and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - 2013 - Maxout Networks.pdf:pdf},
journal = {Proc. 30th Int. Conf. Mach. Learn.},
title = {{Maxout Networks}},
year = {2013}
}
@article{Mcwilliams2014,
abstract = {In several application domains, high-dimensional observations are collected and then analysed in search for naturally occurring data clusters which might provide further insights about the nature of the problem. In this paper we describe a new approach for partitioning such high-dimensional data. Our assumption is that, within each cluster, the data can be approximated well by a linear subspace estimated by means of a principal component analysis (PCA). The proposed algorithm, Predictive Subspace Clustering (PSC) partitions the data into clusters while simultaneously estimating cluster-wise PCA parameters. The algorithm minimises an objective function that depends upon a new measure of influence for PCA models. A penalised version of the algorithm is also described for carrying our simultaneous subspace clustering and variable selection. The convergence of PSC is discussed in detail, and extensive simulation results and comparisons to competing methods are presented. The comparative performance of PSC has been assessed on six real gene expression data sets for which PSC often provides state-of-art results.},
archivePrefix = {arXiv},
arxivId = {arXiv:1203.1065v1},
author = {McWilliams, Brian and Montana, Giovanni},
doi = {10.1007/s10618-013-0317-y},
eprint = {arXiv:1203.1065v1},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Mcwilliams et al. - 2014 - Subspace clustering of high-dimensional data a predictive approach.pdf:pdf},
issn = {13845810},
journal = {Data Min. Knowl. Discov.},
keywords = {Microarrays,Model selection,PCA,PRESS statistics,Subspace clustering,Variable selection},
number = {3},
pages = {736--772},
title = {{Subspace clustering of high-dimensional data: A predictive approach}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs10618-013-0317-y.pdf},
volume = {28},
year = {2014}
}
@article{Bolton2018,
author = {Bolton, Thomas A W and Member, Student and Is, Fikret and Eliez, Stephan and Schaer, Marie and Ville, Dimitri Van De and Member, Senior},
doi = {10.1109/TMI.2018.2863944},
file = {:Users/ricardo/Downloads/08454483.pdf:pdf},
issn = {0278-0062},
number = {c},
pages = {1--13},
title = {{Robust recovery of temporal overlap between network activity using transient-informed spatio-temporal regression}},
volume = {0062},
year = {2018}
}
@article{Morcos,
abstract = {Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyper-parameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on indi-vidual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.},
archivePrefix = {arXiv},
arxivId = {1803.06959},
author = {Morcos, Ari S and Barrett, David G T and Rabinowitz, Neil C and Botvinick, Matthew and London, Deepmind},
eprint = {1803.06959},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Morcos et al. - Unknown - ON THE IMPORTANCE OF SINGLE DIRECTIONS FOR GENERALIZATION.pdf:pdf},
title = {{on the Importance of Single Directions for Generalization}},
url = {https://arxiv.org/pdf/1803.06959.pdf}
}
@article{Tipping1999,
author = {Tipping, M. E. and Bishop, C. M.},
file = {:Users/ricardo/Downloads/ppca.pdf:pdf},
journal = {J. R. Stat. Soc. Ser. B (Statistical Methodol.},
keywords = {analysis of variance,choice of variables,crossvalidation,doublecross,modelmix,multiple regression,prediction,prescription,univariate estimation},
number = {2},
pages = {150--210},
title = {{Probablistic Principle Component Analysis}},
volume = {11},
year = {1999}
}
@article{Lopez-Paz2013,
abstract = {We introduce the Randomized Dependence Coefficient (RDC), a measure of non-linear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-R$\backslash$'enyi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper.},
archivePrefix = {arXiv},
arxivId = {1304.7717},
author = {Lopez-Paz, David and Hennig, Philipp and Sch{\"{o}}lkopf, Bernhard},
doi = {10.1214/aos/1176345528},
eprint = {1304.7717},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lopez-Paz, Hennig, Sch{\"{o}}lkopf - Unknown - The Randomized Dependence Coefficient.pdf:pdf},
isbn = {00905364},
issn = {10495258},
journal = {NIPS},
title = {{The Randomized Dependence Coefficient}},
url = {https://arxiv.org/pdf/1304.7717.pdf http://arxiv.org/abs/1304.7717},
year = {2013}
}
@article{Ledoit2004,
abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For large-dimensional covariance matrices, the usual estimator - the sample covariance matrix - is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample. {\textcopyright} 2003 Elsevier Inc. All rights reserved.},
author = {Ledoit, Olivier and Wolf, Michael},
doi = {10.1016/S0047-259X(03)00096-4},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ledoit, Wolf - 2004 - A well-conditioned estimator for large-dimensional covariance matrices.pdf:pdf},
isbn = {0047-259X},
issn = {0047259X},
journal = {J. Multivar. Anal.},
keywords = {Condition number,Covariance matrix estimation,Empirical Bayes,General asymptotics,Shrinkage},
month = {feb},
number = {2},
pages = {365--411},
publisher = {Academic Press},
title = {{A well-conditioned estimator for large-dimensional covariance matrices}},
url = {http://www.sciencedirect.com/science/article/pii/S0047259X03000964},
volume = {88},
year = {2004}
}
@article{Kusmierczyk,
abstract = {A wide variety of online platforms use digital badges to encourage users to take certain types of desirable actions. However, despite their growing popularity, their causal effect on users' behavior is not well understood. This is partly due to the lack of counterfactual data and the myriad of complex factors that influence users' behavior over time. As a consequence, their design and deployment lacks general principles. In this paper, we focus on first-time badges, which are awarded after a user takes a particular type of action for the first time, and study their causal effect by harnessing the delayed introduction of several badges in a popular Q{\&}A website. In doing so, we introduce a novel causal inference framework for badges whose main technical innovations are a robust survival-based hypothesis testing procedure, which controls for the utility heterogeneity across users, and a bootstrap difference-in-differences method, which controls for the random fluctuations in users' behavior over time. We find that first-time badges steer users' behavior if the utility a user obtains from taking the corresponding action is sufficiently low, otherwise, the badge does not have a significant effect. Moreover, for badges that successfully steered user behavior, we perform a counterfactual analysis and show that they significantly improved the functioning of the site at a community level.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.08160v1},
author = {Kusmierczyk, Tomasz and Gomez-rodriguez, Manuel},
eprint = {arXiv:1707.08160v1},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kusmierczyk, Gomez-Rodriguez - Unknown - Harnessing Natural Experiments to Quantify the Causal Effect of Badges.pdf:pdf},
pages = {1--13},
title = {{Harnessing Natural Experiments to Quantify the Causal Effect of Badges}},
url = {https://arxiv.org/pdf/1707.08160.pdf}
}
@techreport{Peters,
author = {Peters, Jonas},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(9).pdf:pdf},
title = {{Lecture Notes on Causality}},
year = {2015}
}
@article{Huggins,
abstract = {The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it.},
archivePrefix = {arXiv},
arxivId = {1605.06423},
author = {Huggins, Jonathan H and Campbell, Trevor and Broderick, Tamara},
eprint = {1605.06423},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Huggins, Campbell, Broderick - Unknown - Coresets for Scalable Bayesian Logistic Regression.pdf:pdf},
issn = {10495258},
title = {{Coresets for Scalable Bayesian Logistic Regression}},
url = {http://papers.nips.cc/paper/6486-coresets-for-scalable-bayesian-logistic-regression.pdf http://arxiv.org/abs/1605.06423},
year = {2016}
}
@article{Thompson2015,
abstract = {When studying brain connectivity using fMRI, signal intensity time-series are typically correlated with each other in time to compute estimates of the degree of interaction between different brain regions and/or networks. In the static connectivity case, the problem of defining which connections that should be considered significant in the analysis can be addressed in a rather straightforward manner by a statistical thresholding that is based on the magnitude of the correlation coefficients. More recently, interest has come to focus on the dynamical aspects of brain connectivity and the problem of deciding which brain connections that are to be considered relevant in the context of dynamical changes in connectivity provides further options. Since we, in the dynamical case, are interested in changes in connectivity over time, the variance of the correlation time-series becomes a relevant parameter. In this study, we discuss the relationship between the mean and variance of brain connectivity time-series and show that by studying the relation between them, two conceptually different strategies to analyze dynamic functional brain connectivity become available. Using both simulated data as well as resting-state fMRI data from a cohort of 46 subjects, we show that the mean of fMRI connectivity time-series scales negatively with its variance. This finding leads to the suggestion that magnitude- versus variance-based thresholding strategies will induce different results in studies of dynamic functional brain connectivity. Our assertion is exemplified by showing that the magnitude-based strategy is more sensitive to within-RSN connectivity compared to between-RSN connectivity whereas the opposite holds true for a variance-based analysis strategy. The implications of our findings for dynamical functional brain connectivity studies are discussed.},
author = {Thompson, William Hedley and Fransson, Peter},
doi = {10.3389/fnhum.2015.00398},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Windischberger et al. - 2015 - The mean–variance relationship reveals two possible strategies for dynamic brain connectivity analysis.pdf:pdf},
issn = {1662-5161},
journal = {Front. Hum. Neurosci.},
keywords = {Signal variance,brain connectivity,dynamics,fMRI,mean,resting-state},
number = {398},
pages = {1--7},
pmid = {26236216},
title = {{The mean–variance relationship reveals two possible strategies for dynamic brain connectivity analysis in fMRI}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4500903/pdf/fnhum-09-00398.pdf http://journal.frontiersin.org/article/10.3389/fnhum.2015.00398/abstract{\%}5Cnhttp://journal.frontiersin.org/Article/10.3389/fnhum.2015.00398/abstract},
volume = {9},
year = {2015}
}
@article{Varoquaux2017a,
abstract = {Decoding, i.e. prediction from brain images or signals, calls for empirical evaluation of its predictive power. Such evaluation is achieved via cross-validation, a method also used to tune decoders' hyper-parameters. This paper is a review on cross-validation procedures for decoding in neuroimaging. It includes a didactic overview of the relevant theoretical considerations. Practical aspects are highlighted with an extensive empirical study of the common decoders in within- and across-subject predictions, on multiple datasets –anatomical and functional MRI and MEG– and simulations. Theory and experiments outline that the popular “leave-one-out” strategy leads to unstable and biased estimates, and a repeated random splits method should be preferred. Experiments outline the large error bars of cross-validation in neuroimaging settings: typical confidence intervals of 10{\%}. Nested cross-validation can tune decoders' parameters while avoiding circularity bias. However we find that it can be favorable to use sane defaults, in particular for non-sparse decoders.},
author = {Varoquaux, Ga{\"{e}}l and Raamana, Pradeep Reddy and Engemann, Denis A and Hoyos-Idrobo, Andr{\'{e}}s and Schwartz, Yannick and Thirion, Bertrand},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Varoquaux et al. - 2017 - Assessing and tuning brain decoders Cross-validation, caveats, and guidelines.pdf:pdf},
journal = {Neuroimage},
keywords = {Bagging,Cross-validation,Decoding,FMRI,MVPA,Model selection,Sparse},
pages = {166--179},
title = {{Assessing and tuning brain decoders: Cross-validation, caveats, and guidelines}},
volume = {145},
year = {2017}
}
@article{Monti2017a,
abstract = {{\textcopyright} 2016 Wiley Periodicals, Inc.Two novel and exciting avenues of neuroscientific research involve the study of task-driven dynamic reconfigurations of functional connectivity networks and the study of functional connectivity in real-time. While the former is a well-established field within neuroscience and has received considerable attention in recent years, the latter remains in its infancy. To date, the vast majority of real-time fMRI studies have focused on a single brain region at a time. This is due in part to the many challenges faced when estimating dynamic functional connectivity networks in real-time. In this work, we propose a novel methodology with which to accurately track changes in time-varying functional connectivity networks in real-time. The proposed method is shown to perform competitively when compared to state-of-the-art offline algorithms using both synthetic as well as real-time fMRI data. The proposed method is applied to motor task data from the Human Connectome Project as well as to data obtained from a visuospatial attention task. We demonstrate that the algorithm is able to accurately estimate task-related changes in network structure in real-time. Hum Brain Mapp 38:202–220, 2017. {\textcopyright} 2016 Wiley Periodicals, Inc.},
author = {Monti, Ricardo Pio and Lorenz, Romy and Braga, Rodrigo M. and Anagnostopoulos, Christoforos and Leech, Robert and Montana, Giovanni},
file = {:Users/ricardo/Downloads/Monti{\_}et{\_}al-2017-Human{\_}Brain{\_}Mapping (1).pdf:pdf},
journal = {Hum. Brain Mapp.},
keywords = {dynamic networks,functional connectivity,neurofeedback,real-time,streaming penalized optimization},
number = {1},
pages = {202--220},
title = {{Real-time estimation of dynamic functional connectivity networks}},
volume = {38},
year = {2017}
}
@article{Buesing2014,
abstract = {High-dimensional, simultaneous recordings of neural spiking activity are often explored, analyzed and visualized with the help of latent variable or factor models. Such models are however ill-equipped to extract structure beyond shared, distributed aspects of firing activity across multiple cells. Here, we extend unstructured factor models by proposing a model that discovers subpopulations or groups of cells from the pool of recorded neurons. The model combines aspects of mixture of factor analyzer models for capturing clustering structure, and aspects of latent dynamical system models for capturing temporal dependencies. In the resulting model, we infer the subpopulations and the latent factors from data using variational inference and model parameters are estimated by Expectation Maximization (EM). We also address the crucial problem of initializing parameters for EM by extending a sparse subspace clustering algorithm to integer-valued spike count observations. We illustrate the merits of the proposed model by applying it to calcium-imaging data from spinal cord neurons, and we show that it uncovers meaningful clustering structure in the data.},
author = {Buesing, Lars and Machado, Timothy and Cunningham, John P and Paninski, Liam},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Buesing et al. - Unknown - Clustered factor analysis of multineuronal spike data.pdf:pdf},
issn = {10495258},
journal = {NIPS},
pages = {3500--3508},
title = {{Clustered factor analysis of multineuronal spike data}},
volume = {27},
year = {2014}
}
@article{Wen2013,
abstract = {Minimization with orthogonality constraints (e.g., {\$}{\$}X{\^{}}$\backslash$top X = I{\$}{\$} ) and/or spherical constraints (e.g., {\$}{\$}$\backslash$Vert x$\backslash$Vert {\_}2 = 1{\$}{\$} ) has wide applications in polynomial optimization, combinatorial optimization, eigenvalue problems, sparse PCA, p-harmonic flows, 1-bit compressive sensing, matrix rank minimization, etc. These problems are difficult because the constraints are not only non-convex but numerically expensive to preserve during iterations. To deal with these difficulties, we apply the Cayley transform—a Crank-Nicolson-like update scheme—to preserve the constraints and based on it, develop curvilinear search algorithms with lower flops compared to those based on projections and geodesics. The efficiency of the proposed algorithms is demonstrated on a variety of test problems. In particular, for the maxcut problem, it exactly solves a decomposition formulation for the SDP relaxation. For polynomial optimization, nearest correlation matrix estimation and extreme eigenvalue problems, the proposed algorithms run very fast and return solutions no worse than those from their state-of-the-art algorithms. For the quadratic assignment problem, a gap 0.842 {\%} to the best known solution on the largest problem “tai256c” in QAPLIB can be reached in 5 min on a typical laptop. [ABSTRACT FROM AUTHOR]},
author = {Wen, Zaiwen and Yin, Wotao},
doi = {10.1007/s10107-012-0584-1},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Wen et al. - 2013 - A feasible method for optimization with orthogonality constraints(2).pdf:pdf},
isbn = {0025-5610},
issn = {00255610},
journal = {Math. Program.},
keywords = {Cayley transformation,Curvilinear search,Eigenvalue and eigenvector,Invariant subspace,Maxcut SDP,Nearest correlation matrix,Orthogonality constraint,Polynomial optimization,Quadratic assignment problem,Spherical constraint,Stiefel manifold},
number = {1-2},
pages = {397--434},
title = {{A feasible method for optimization with orthogonality constraints}},
url = {https://search.proquest.com/docview/1449927009?pq-origsite=gscholar},
volume = {142},
year = {2013}
}
@article{Hyvarinen2016,
abstract = {In many multivariate time series, the correlation structure is$\backslash$nnonstationary, that is, it changes over time. The correlation structure$\backslash$nmay also change as a function of other cofactors, for example, the$\backslash$nidentity of the subject in biomedical data. A fundamental approach for$\backslash$nthe analysis of such data is to estimate the correlation structure$\backslash$n(connectivities) separately in short timewindows or for different$\backslash$nsubjects and use existing machine learning methods, such as principal$\backslash$ncomponent analysis (PCA), to summarize or visualize the changes in$\backslash$nconnectivity. However, the visualization of such a straightforward PCA$\backslash$nis problematic because the ensuing connectivity patterns are much more$\backslash$ncomplex objects than, say, spatial patterns. Here, we develop a new$\backslash$nframework for analyzing variability in connectivities using the PCA$\backslash$napproach as the starting point. First, we show how to analyze and$\backslash$nvisualize the principal components of connectivity matrices by a$\backslash$ntailor-made rank-two matrix approximation in which we use the outer$\backslash$nproduct of two orthogonal vectors. This leads to a new kind of$\backslash$ntransformation of eigenvectors that is particularly suited for this$\backslash$npurpose and often enables interpretation of the principal component as$\backslash$nconnectivity between two groups of variables. Second, we show how to$\backslash$nincorporate the orthogonality and the rank-two constraint in the$\backslash$nestimation of PCA itself to improve the results. We further provide an$\backslash$ninterpretation of these methods in terms of estimation of a$\backslash$nprobabilistic generative model related to blind separation of dependent$\backslash$nsources. Experiments on brain imaging data give very promising results.},
annote = {They address the issue of visualising and interpreting multiple connectivity (e.g., covariance) matrices

Simply running PCA on the vectorized matricies is not interesting and it reports the set of nodes which are jointly varying the most (i.e., the linear sum with the highest variance). Instead we would like groups of nodes whose connectivity changes the most. 

They propose to achieve this goal via a rank-2 orthogonal approximation for which there is a closed form solution.

They also relate the solution to the approximate log-likelihood in an ICA model where the mixing matrix is orthogonal and the partial dependence structure of two variables changes over time.},
author = {Hyv{\"{a}}rinen, Aapo and Hirayama, Jun Ichiro and Kiviniemi, Vesa and Kawanabe, Motoaki},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hyv{\"{a}}rinen et al. - Unknown - Orthogonal Connectivity Factorization Interpretable Decomposition of Variability in Correlation Matrices.pdf:pdf},
journal = {Neural Comput.},
number = {3},
pages = {445--484},
title = {{Orthogonal Connectivity Factorization: Interpretable Decomposition of Variability in Correlation Matrices}},
volume = {28},
year = {2016}
}
@article{Spirtes2010,
abstract = {The goal of many sciences is to understand the mechanisms by which variables came to take on the values they have (that is, to find a generative model), and to predict what the values of those variables would be if the naturally occurring mechanisms were subject to outside manipulations. The past 30 years has seen a number of conceptual developments that are partial solutions to the problem of causal inference from observational sample data or a mixture of observational sample and experimental data, particularly in the area of graphical causal modeling. However, in many do-mains, problems such as the large numbers of variables, small samples sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of these developments. The articles in the Special Topic on Causality address these and other problems in applying graphi-cal causal modeling algorithms. This introduction to the Special Topic on Causality provides a brief introduction to graphical causal modeling, places the articles in a broader context, and describes the differences between causal inference and ordinary machine learning classification and prediction problems.},
author = {Spirtes, Peter},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Spirtes - 2010 - Introduction to Causal Inference(2).pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {Bayesian networks,causal inference,causation},
pages = {1643--1662},
title = {{Introduction to Causal Inference}},
url = {http://www.jmlr.org/papers/volume11/spirtes10a/spirtes10a.pdf},
volume = {11},
year = {2010}
}
@article{Varin2011,
abstract = {A survey of recent developments in the theory and application of composite likelihood is provided, building on the review paper of Varin (2008). A range of application areas, including geostatistics, spatial extremes, and space-time models, as well as clustered and longitudinal data and time series are considered. The important area of applications to statistical genetics is omitted, in light of Larribe and Fearnhead (2011). Emphasis is given to the development of the theory, and the current state of knowledge on efficiency and robustness of composite likelihood inference.},
author = {Varin, Cristiano and Reid, Nancy and Firth, David},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Varin et al. - 2011 - AN OVERVIEW OF COMPOSITE LIKELIHOOD METHODS.pdf:pdf},
isbn = {1017-0405},
issn = {10170405},
journal = {Stat. Sin.},
keywords = {and phrases,copulas,generalized estimating equations,geostatistics,godambe information,hood,longitudinal data,multivariate binary data,pseudo-likeli-,quasi-likelihood,robustness,spatial extremes,time series},
pages = {5----42},
title = {{An overview of composite likelihood methods}},
url = {https://www.wu.ac.at/fileadmin/wu/d/i/statmath/Research{\_}Seminar/SS{\_}2016/varin{\_}paper.pdf http://www3.stat.sinica.edu.tw/statistica/j21n1/J21N11/J21N11.html},
volume = {21},
year = {2011}
}
@article{Faleiros2016,
abstract = {LDA (Latent Dirichlet Allocation) and NMF (Non-negative Matrix Factorization) are two popular techniques to extract topics in a textual document corpus. This paper shows that NMF with Kullback-Leibler divergence approximate the LDA model under a uniform Dirichlet prior, therefore the comparative analysis can be useful to elucidate the implementation of variational inference algorithm for LDA.},
author = {Faleiros, Thiago De Paulo and Lopes, Alneu De Andrade},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/De et al. - Unknown - On the equivalence between algorithms for Non-negative Matrix Factorization and Latent Dirichlet Allocation.pdf:pdf},
isbn = {9782875870278},
journal = {ESANN},
number = {April},
pages = {27--29},
title = {{On the equivalence between algorithms for Non-negative Matrix Factorization and Latent Dirichlet Allocation}},
url = {https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2016-162.pdf},
year = {2016}
}
@article{Paul,
abstract = {We present a method based on the orthogonal symmetric non-negative matrix tri-factorization of the normalized Laplacian matrix for community detection in complex networks. While the exact factorization of a given order may not exist and is NP hard to compute, we obtain an approximate factorization by solving an optimization problem. We establish the connection of the factors obtained through the factorization to a non-negative basis of an invariant subspace of the estimated matrix, drawing parallel with the spectral clustering. Using such factorization for clustering in networks is motivated by analyzing a block-diagonal Laplacian matrix with the blocks representing the connected components of a graph. The method is shown to be consistent for community detection in graphs generated from the stochastic block model and the degree corrected stochastic block model. Simulation results and real data analysis show the effectiveness of these methods under a wide variety of situations, including sparse and highly heterogeneous graphs where the usual spectral clustering is known to fail. Our method also performs better than the state of the art in popular benchmark network datasets, e.g., the political web blogs and the karate club data.},
archivePrefix = {arXiv},
arxivId = {1605.05349},
author = {Paul, Subhadeep and Chen, Yuguo},
eprint = {1605.05349},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Paul, Chen - Unknown - ORTHOGONAL SYMMETRIC NON-NEGATIVE MATRIX FACTORIZATION UNDER THE STOCHASTIC BLOCK MODEL.pdf:pdf},
title = {{Orthogonal symmetric non-negative matrix factorization under the stochastic block model}},
url = {https://arxiv.org/pdf/1605.05349.pdf http://arxiv.org/abs/1605.05349},
year = {2016}
}
@article{Hron2017,
abstract = {Gaussian multiplicative noise is commonly used as a stochastic regularisation technique in training of deterministic neural networks [12]. A recent paper [6] reinterpreted the technique as a specific algorithm for approximate inference in Bayesian neural networks; several extensions ensued [8, 10, 11]. We show that the log-uniform prior used in all the above publications does not generally induce a proper posterior, and thus Bayesian inference in such models is ill-posed. In-dependent of the log-uniform prior, the correlated weight noise approximation proffered in [6] has further issues leading to either infinite objective or high risk of overfitting. The above implies that the reported sparsity of obtained solutions cannot be explained by Bayesian or the related minimum description length argu-ments. We thus study the objective from a non-Bayesian perspective, provide its previously unknown analytical form which allows exact gradient evaluation, and show that the reparametrisation proposed in [10] introduces minima not present in the original [6]. Implications and future research directions are discussed.},
archivePrefix = {arXiv},
arxivId = {1711.02989},
author = {Hron, Jiri and Matthews, Alexander G. de G. and Ghahramani, Zoubin},
eprint = {1711.02989},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hron et al. - Unknown - Variational Gaussian Dropout is not Bayesian.pdf:pdf},
title = {{Variational Gaussian Dropout is not Bayesian}},
url = {https://arxiv.org/pdf/1711.02989.pdf https://arxiv.org/abs/1711.02989},
year = {2017}
}
@article{Ramdas2017,
abstract = {In the online multiple testing problem, p-values corresponding to different null hypotheses are observed one by one, and the decision of whether or not to reject the current hypothesis must be made immediately, after which the next p-value is observed. Alpha-investing algorithms to control the false discovery rate (FDR), formulated by Foster and Stine, have been generalized and applied to many settings, including quality-preserving databases in science and multiple A/B or multi-armed bandit tests for internet commerce. This paper improves the class of generalized alpha-investing algorithms (GAI) in four ways: (a) we show how to uniformly improve the power of the entire class of monotone GAI procedures by awarding more alpha-wealth for each rejection, giving a win-win resolution to a recent dilemma raised by Javanmard and Montanari, (b) we demonstrate how to incorporate prior weights to indicate domain knowledge of which hypotheses are likely to be non-null, (c) we allow for differing penalties for false discoveries to indicate that some hypotheses may be more important than others, (d) we define a new quantity called the decaying memory false discovery rate (mem-FDR) that may be more meaningful for truly temporal applications, and which alleviates problems that we describe and refer to as "piggybacking" and "alpha-death". Our GAI++ algorithms incorporate all four generalizations simultaneously, and reduce to more powerful variants of earlier algorithms when the weights and decay are all set to unity. Finally, we also describe a simple method to derive new online FDR rules based on an estimated false discovery proportion.},
archivePrefix = {arXiv},
arxivId = {1710.00499},
author = {Ramdas, Aaditya and Yang, Fanny and Wainwright, Martin J and Jordan, Michael I},
doi = {10.03.17},
eprint = {1710.00499},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ramdas et al. - Unknown - Online control of the false discovery rate with decaying memory.pdf:pdf},
journal = {NIPS},
title = {{Online control of the false discovery rate with decaying memory}},
url = {https://arxiv.org/pdf/1710.00499.pdf http://arxiv.org/abs/1710.00499},
year = {2017}
}
@article{Gandy2013,
abstract = {This article presents an algorithm that generates a conservative confidence interval of a specified length and coverage probability for the power of a Monte Carlo test (such as a bootstrap or permutation test). It is the first method that achieves this aim for almost any Monte Carlo test. Previous research has focused on obtaining as accurate a result as possible for a fixed computational effort, without providing a guaranteed precision in the above sense. The algorithm we propose does not have a fixed effort and runs until a confidence interval with a user-specified length and coverage probability can be constructed. We show that the expected effort required by the algorithm is finite in most cases of practical interest, including situations where the distribution of the {\$}p{\$}-value is absolutely continuous or discrete with finite support. The algorithm is implemented in the R-package simctest, available on CRAN.},
archivePrefix = {arXiv},
arxivId = {arXiv:1110.1248v2},
author = {Gandy, Axel and Rubin-Delanchy, Patrick},
doi = {10.1214/12-AOS1076},
eprint = {arXiv:1110.1248v2},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gandy, Rubin-Delanchy - 2013 - AN ALGORITHM TO COMPUTE THE POWER OF MONTE CARLO TESTS WITH GUARANTEED PRECISION 1.pdf:pdf},
issn = {00905364},
journal = {Ann. Stat.},
keywords = {Algorithm,Monte Carlo testing,Power,Significance test},
number = {1},
pages = {125--142},
title = {{An algorithm to compute the power of Monte Carlo tests with guaranteed precision}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aos/1362493042},
volume = {41},
year = {2013}
}
@article{Monti2017,
abstract = {In neuroimaging data analysis, Gaussian graphical models are often used to model statistical dependencies across spatially remote brain regions known as functional connectivity. Typically, data is collected across a cohort of subjects and the scientific objectives consist of estimating population and subject-specific connectivity networks. A third objective that is often over-looked involves quantifying inter-subject variability, and thus identifying re-gions or subnetworks that demonstrate heterogeneity across subjects. Such information is crucial to thoroughly understand the human connectome. We propose Mixed Neighborhood Selection to simultaneously address the three aforementioned objectives. By recasting covariance selection as a neighbor-hood selection problem, we are able to efficiently learn the topology of each node. We introduce an additional mixed effect component to neighborhood selection to simultaneously estimate a graphical model for the population of subjects as well as for each individual subject. The proposed method is vali-dated empirically through a series of simulations and applied to resting state data for healthy subjects taken from the ABIDE consortium.},
author = {Monti, Ricardo Pio and Anagnostopoulos, Christoforos and Montana, Giovanni},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Monti, Anagnostopoulos, Montana - 2017 - LEARNING POPULATION AND SUBJECT-SPECIFIC BRAIN CONNECTIVITY NETWORKS VIA MIXED NEIGHBORHOOD SEL.pdf:pdf},
journal = {Ann. Appl. Stat.},
keywords = {Functional connectivity,graphical models,inter-subject variability,neuroimaging},
number = {4},
pages = {2142--2164},
title = {{Learning population and subject-specific brain connectivity networks via Mixed Neighborhood Selection}},
volume = {11},
year = {2017}
}
@article{Barratt,
abstract = {Deep generative models are powerful tools that have produced impressive results in recent years. These advances have been for the most part empirically driven, making it essential that we use high quality evaluation metrics. In this paper, we provide new insights into the Inception Score, a recently proposed and widely used evaluation metric for generative models, and demonstrate that it fails to provide useful guidance when comparing models. We discuss both suboptimalities of the metric itself and issues with its application. Finally, we call for researchers to be more systematic and careful when evaluating and comparing generative models, as the advancement of the field depends upon it.},
archivePrefix = {arXiv},
arxivId = {1801.01973},
author = {Barratt, Shane and Sharma, Rishi},
eprint = {1801.01973},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Barratt, Sharma - Unknown - A Note on the Inception Score.pdf:pdf},
title = {{A Note on the Inception Score}},
url = {https://arxiv.org/pdf/1801.01973.pdf http://arxiv.org/abs/1801.01973},
year = {2018}
}
@article{Yarkoni2011,
abstract = {The rapid growth of the literature on neuroimaging in humans has led to major advances in our understanding of human brain function but has also made it increasingly difficult to aggregate and synthesize neuroimaging findings. Here we describe and validate an automated brain-mapping framework that uses text-mining, meta-analysis and machine-learning techniques to generate a large database of mappings between neural and cognitive states. We show that our approach can be used to automatically conduct large-scale, high-quality neuroimaging meta-analyses, address long-standing inferential problems in the neuroimaging literature and support accurate 'decoding' of broad cognitive states from brain activity in both entire studies and individual human subjects. Collectively, our results have validated a powerful and generative framework for synthesizing human neuroimaging data on an unprecedented scale.},
author = {Yarkoni, Tal and Poldrack, Russell A. and Nichols, Thomas E. and {Van Essen}, David C. and Wager, Tor D.},
file = {:Users/ricardo/Downloads/Yarkoni{\_}NatureMethods{\_}2011.pdf:pdf},
journal = {Nat. Methods},
number = {8},
pages = {665--670},
title = {{Large-scale automated synthesis of human functional neuroimaging data}},
volume = {8},
year = {2011}
}
@article{Agarwal2010,
abstract = {This paper describes a probabilistic framework for studying associations between$\backslash$r$\backslash$nmultiple genotypes, biomarkers, and phenotypic traits in the presence of noise and$\backslash$r$\backslash$nunobserved confounders for large genetic studies. The framework builds on sparse$\backslash$r$\backslash$nlinear methods developed for regression and modified here for inferring causal$\backslash$r$\backslash$nstructures of richer networks with latent variables. The method is motivated by the$\backslash$r$\backslash$nuse of genotypes as “instruments” to infer causal associations between phenotypic$\backslash$r$\backslash$nbiomarkers and outcomes, without making the common restrictive assumptions of$\backslash$r$\backslash$ninstrumental variable methods. The method may be used for an effective screening$\backslash$r$\backslash$nof potentially interesting genotype-phenotype and biomarker-phenotype associations$\backslash$r$\backslash$nin genome-wide studies, which may have important implications for validating$\backslash$r$\backslash$nbiomarkers as possible proxy endpoints for early-stage clinical trials. Where$\backslash$r$\backslash$nthe biomarkers are gene transcripts, the method can be used for fine mapping of$\backslash$r$\backslash$nquantitative trait loci (QTLs) detected in genetic linkage studies. The method is$\backslash$r$\backslash$napplied for examining effects of gene transcript levels in the liver on plasma HDL$\backslash$r$\backslash$ncholesterol levels for a sample of sequenced mice from a heterogeneous stock,$\backslash$r$\backslash$nwith ∼ 105 genetic instruments and ∼ 47 × 103 gene transcripts.},
author = {Agarwal, Alekh and Berkeley, U C and Bartlett, Peter L and Wainwright, Martin J},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Agakov et al. - Unknown - Sparse Instrumental Variables (SPIV) for Genome-Wide Studies.pdf:pdf},
isbn = {9781617823800},
journal = {NIPS},
pages = {1--6},
title = {{Information-theoretic lower bounds on the oracle complexity of sparse convex optimization}},
url = {http://papers.nips.cc/paper/3976-sparse-instrumental-variables-spiv-for-genome-wide-studies.pdf},
year = {2010}
}
@article{Bickel2008,
abstract = {Accessed: 01-12-2017 11:23 UTC JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. This paper considers regularizing a covariance matrix of p variables estimated from n observations, by hard thresholding. We show that the thresholded estimate is consistent in the operator norm as long as the true covariance matrix is sparse in a suitable sense, the variables are Gaussian or sub-Gaussian, and (logp)/n -* 0, and obtain explicit rates. The results are uniform over families of covariance matrices which satisfy a fairly natural no tion of sparsity. We discuss an intuitive resampling scheme for threshold se lection and prove a general cross-validation result that justifies this approach. We also compare thresholding to other covariance estimators in simulations and on an example from climate data.},
author = {Bickel, Peter J and Levina, Elizaveta},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bickel et al. - 2008 - Covariance Regularization by Thresholding(3).pdf:pdf},
journal = {Ann. Stat.},
number = {6},
pages = {2577--2604},
title = {{Covariance Regularization by Thresholding}},
volume = {36},
year = {2008}
}
@article{Grunwald2017,
abstract = {We empirically show that Bayesian inference can be inconsistent under misspecification in simple linear regression problems, both in a model averaging/selection and in a Bayesian ridge regression setting. We use the standard linear model, which assumes homoskedasticity, whereas the data are heteroskedastic, and observe that the posterior puts its mass on ever more high-dimensional models as the sample size increases. To remedy the problem, we equip the likelihood in Bayes' theorem with an exponent called the learning rate, and we propose the Safe Bayesian method to learn the learning rate from the data. SafeBayes tends to select small learning rates as soon the standard posterior is not `cumulatively concentrated', and its results on our data are quite encouraging.},
archivePrefix = {arXiv},
arxivId = {1412.3730},
author = {Gr{\"{u}}nwald, Peter and van Ommen, Thijs},
doi = {10.1214/17-BA1085},
eprint = {1412.3730},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gr{\"{u}}nwald, Van Ommen - 2017 - Inconsistency of Bayesian Inference for Misspecified Linear Models, and a Proposal for Repairing It(2).pdf:pdf},
issn = {19316690},
journal = {Bayesian Anal.},
number = {4},
pages = {1069--1103},
title = {{Inconsistency of Bayesian inference for misspecified linear models, and a proposal for repairing it}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.ba/1510974325},
volume = {12},
year = {2017}
}
@article{Hyvarinen2006,
abstract = {One often wants to estimate statisticalmodels where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlomethods, or approximations of the normalization constant. Here,we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.},
author = {Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hyv{\"{a}}rinen - 2005 - Estimation of Non-Normalized Statistical Models by Score Matching.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {contrastive divergence,markov chain,monte carlo,non-normalized densities,pseudo-likelihood,statistical estimation},
pages = {695--708},
title = {{Estimation of non-normalized statistical models by score matching}},
volume = {6},
year = {2005}
}
@article{Cribben2012,
abstract = {Most statistical analyses of fMRI data assume that the nature, timing and duration of the psychological processes being studied are known. However, often it is hard to specify this information a priori. In this work we introduce a data-driven technique for partitioning the experimental time course into distinct temporal intervals with different multivariate functional connectivity patterns between a set of regions of interest (ROIs). The technique, called Dynamic Connectivity Regression (DCR), detects temporal change points in functional connectivity and estimates a graph, or set of relationships between ROIs, for data in the temporal partition that falls between pairs of change points. Hence, DCR allows for estimation of both the time of change in connectivity and the connectivity graph for each partition, without requiring prior knowledge of the nature of the experimental design. Permutation and bootstrapping methods are used to perform inference on the change points. The method is applied to various simulated data sets as well as to an fMRI data set from a study (N= 26) of a state anxiety induction using a socially evaluative threat challenge. The results illustrate the method's ability to observe how the networks between different brain regions changed with subjects' emotional state. {\textcopyright} 2012 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Cribben, Ivor and Haraldsdottir, Ragnheidur and Atlas, Lauren Y. and Wager, Tor D. and Lindquist, Martin A.},
doi = {10.1016/j.neuroimage.2012.03.070},
eprint = {NIHMS150003},
file = {:Users/ricardo/Downloads/Dynamic connectivity regression{\_} Determining state-related changes in brain connectivity.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$n1053-8119 (Linking)},
issn = {10538119},
journal = {Neuroimage},
keywords = {Change point analysis,FMRI,Functional connectivity,Graphical lasso,Regression trees},
month = {jul},
number = {4},
pages = {907--920},
pmid = {22484408},
publisher = {Academic Press},
title = {{Dynamic connectivity regression: Determining state-related changes in brain connectivity}},
url = {http://www.sciencedirect.com/science/article/pii/S1053811912003515},
volume = {61},
year = {2012}
}
@article{McClure2016,
abstract = {We propose representational distance learning (RDL), a technique that allows transferring knowledge from a model of arbitrary type to a deep neural network (DNN). This method seeks to maximize the similarity between the representational dissimilarity, or distance, matrices (RDMs) of a model with desired knowledge, the teacher, and a DNN currently being trained, the student. This knowledge transfer is performed using auxiliary error functions. This allows DNNs to simultaneously learn from a teacher model and learn to perform some task within the framework of backpropagation. We test the use of RDL for knowledge distillation, also known as model compression, from a large teacher DNN to a small student DNN using the MNIST and CIFAR-10 datasets. Also, we test the use of RDL for knowledge transfer between tasks using the CIFAR-10 and CIFAR-100 datasets. For each test, RDL significantly improves performance when compared to traditional backpropagation alone and performs similarly to, or better than, recently proposed methods for model compression and knowledge transfer.},
archivePrefix = {arXiv},
arxivId = {1511.03979},
author = {McClure, Patrick and Kriegeskorte, Nikolaus},
doi = {10.3389/fncom.2016.00131},
eprint = {1511.03979},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/McClure, Kriegeskorte - 2016 - Representational Distance Learning for Deep Neural Networks.pdf:pdf},
isbn = {9783901608353},
issn = {1662-5188},
journal = {Front. Comput. Neurosci.},
keywords = {computational neuroscience,distance matrices,neural networks,transfer learning,visual perception},
pages = {131},
pmid = {28082889},
publisher = {Frontiers Media SA},
title = {{Representational Distance Learning for Deep Neural Networks}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28082889 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5187453 http://journal.frontiersin.org/article/10.3389/fncom.2016.00131/full},
volume = {10},
year = {2016}
}
@inproceedings{Paatero1997,
abstract = {Positive matrix factorization (PMF) is a recently published factor analytic technique where the left and right factor matrices (corresponding to scores and loadings) are constrained to non-negative values. The PMF model is a weighted least squares fit, weights based on the known standard deviations of the elements of the data matrix. The following aspects of PMF are discussed in this work: (1) Robust factorization (based on the Huber influence function) is achieved by iterative reweighting of individual data values. This appears especially useful if individual data values may be in error. (2) Desired rotations may be obtained automatically with the help of suitably chosen regularization terms. (3) The algorithms for PMF are discussed. A synthetic spectroscopic example is shown, demonstrating both the robust processing and the automatic rotations.},
author = {Paatero, Pentti},
booktitle = {Chemom. Intell. Lab. Syst.},
doi = {10.1016/S0169-7439(96)00044-5},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Paatero - 1997 - Least squares formulation of robust non-negative factor analysis.pdf:pdf},
isbn = {0169-7439},
issn = {01697439},
keywords = {Huber influence function,Iterative reweighting,Positive matrix factorization},
number = {1},
pages = {23--35},
title = {{Least squares formulation of robust non-negative factor analysis}},
url = {https://ac.els-cdn.com/S0169743996000445/1-s2.0-S0169743996000445-main.pdf?{\_}tid=965d8430-d688-11e7-b87b-00000aab0f02{\&}acdnat=1512126958{\_}c8c941332bc4c070df3e9994b21a095f},
volume = {37},
year = {1997}
}
@article{Tibshirani2013,
abstract = {The lasso is a popular tool for sparse linear regression, especially for problems in which the number of variables p exceeds the number of observations n. But when p{\textgreater}n, the lasso criterion is not strictly convex, and hence it may not have a unique minimum. An important question is: when is the lasso solution well-defined (unique)? We review results from the literature, which show that if the predictor variables are drawn from a continuous probability distribution, then there is a unique lasso solution with probability one, regardless of the sizes of n and p. We also show that this result extends easily to {\$}\backslashell{\_}1{\$} penalized minimization problems over a wide range of loss functions. A second important question is: how can we deal with the case of non-uniqueness in lasso solutions? In light of the aforementioned result, this case really only arises when some of the predictor variables are discrete, or when some post-processing has been performed on continuous predictor measurements. Though we certainly cannot claim to provide a complete answer to such a broad question, we do present progress towards understanding some aspects of non-uniqueness. First, we extend the LARS algorithm for computing the lasso solution path to cover the non-unique case, so that this path algorithm works for any predictor matrix. Next, we derive a simple method for computing the component-wise uncertainty in lasso solutions of any given problem instance, based on linear programming. Finally, we review results from the literature on some of the unifying properties of lasso solutions, and also point out particular forms of solutions that have distinctive properties.},
archivePrefix = {arXiv},
arxivId = {1206.0313},
author = {Tibshirani, Ryan J},
doi = {10.1214/13-EJS815},
eprint = {1206.0313},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Tibshirani - 2013 - The lasso problem and uniqueness.pdf:pdf},
isbn = {1935-7524},
issn = {19357524},
journal = {Electron. J. Stat.},
keywords = {High-dimensional,LARS,Lasso,Uniqueness},
number = {1},
pages = {1456--1490},
title = {{The lasso problem and uniqueness}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.ejs/1369148600},
volume = {7},
year = {2013}
}
@article{Gutmann2013a,
abstract = {Parametric statistical models of continuous or discrete valued data are often not properly normalized, that is, they do not integrate or sum to unity. The normalization is essential for maximum likelihood estimation. While in principle,models can always be normalized by dividing themby their integral or sum (their partition function), this can in practice be extremely difficult. We have been developing methods for the estimation of unnormalized models which do not approximate the partition function using numerical integration. We review these methods, score matching and noise-contrastive estimation, point out extensions and connections both between them and methods by other authors, and discuss their pros and cons.},
author = {Gutmann, Michael and Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gutmann, Hyv{\"{a}}rinen - Unknown - ESTIMATION OF UNNORMALIZED STATISTICAL MODELS WITHOUT NUMERICAL INTEGRATION(2).pdf:pdf},
journal = {Int. Work. Information-Theoretic Methods Sci. Eng.},
number = {2},
title = {{Estimation of unnormalized statistical models without numerical integration}},
url = {https://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann13WITMSE.pdf http://www.me.inf.kyushu-u.ac.jp/witmse2013/proceeding{\_}files/Th1.pdf},
year = {2013}
}
@article{Lorenz2017,
abstract = {bioRxiv - the preprint server for biology, operated by Cold Spring Harbor Laboratory, a research and educational institution},
author = {Lorenz, Romy and Violante, Ines R and Monti, Ricardo Pio and Montana, Giovanni and Hampshire, Adam and Leech, Robert},
doi = {10.1101/128678},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lorenz et al. - Unknown - Dissociating frontoparietal brain networks with neuroadaptive Bayesian optimization.pdf:pdf},
journal = {bioRxiv},
keywords = {adaptive{\_}design,bayesian{\_}model,rtfmri},
pages = {128678+},
title = {{Dissociating frontoparietal brain networks with neuroadaptive Bayesian optimization}},
url = {https://www.nature.com/articles/s41467-018-03657-3.pdf http://dx.doi.org/10.1101/128678},
year = {2017}
}
@article{Theisa,
abstract = {The increasingly popular independent component analysis (ICA) may only be applied to data following the generative ICA model in order to guarantee algorithm-independent and theoretically valid results. Subspace ICA models generalize the assumption of component independence to independence between groups of components. They are attractive candidates for dimensionality reduction methods, however are currently limited by the assumption of equal group sizes or less general semi-parametric models. By introducing the concept of irreducible independent subspaces or components, we present a generalization to a parameter-free mixture model. Moreover, we relieve the condition of at-most-one-Gaussian by including previous results on non-Gaussian component analysis. After introducing this general model, we discuss joint block diagonalization with unknown block sizes, on which we base a simple extension of JADE to algorithmically perform the subspace analysis. Simulations confirm the feasibility of the algorithm.},
author = {Theis, Fabian J},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Theis - Unknown - Towards a general independent subspace analysis.pdf:pdf},
isbn = {9780262195683},
issn = {10495258},
journal = {NIPS},
pages = {1361--1368},
title = {{Towards a general independent subspace analysis}},
url = {http://papers.nips.cc/paper/3004-towards-a-general-independent-subspace-analysis.pdf http://www.biologie.uni-regensburg.de/Biophysik/Theis/publications/theis06ISA{\_}NIPS06.pdf},
year = {2007}
}
@article{Buhlmann2014,
author = {B{\"{u}}hlmann, Peter and Peters, Jonas and Ernest, Jan},
doi = {10.1214/14-AOS1260},
file = {:Users/ricardo/Downloads/CAM{\_} Causal additive models, high-dimensional order search and penalized regression.pdf:pdf},
journal = {Ann. Stat.},
number = {6},
pages = {2526--2556},
title = {{CAM : Causal additive models, high-dimensional order search and penalized regression}},
volume = {42},
year = {2014}
}
@article{Shimizu2014,
abstract = {In many empirical sciences, the causal mechanisms underlying various phenomena need to be studied. Structural equation modeling is a general framework used for multivariate analysis, and provides a powerful method for studying causal mechanisms. However, in many cases, classical structural equation modeling is not capable of estimating the causal directions of variables. This is because it explicitly or implicitly assumes Gaussianity of data and typically utilizes only the covariance structure of data. In many applications, however, non-Gaussian data are often obtained, which means that more information may be contained in the data distribution than the covariance matrix is capable of containing. Thus, many new methods have recently been proposed for utilizing the non-Gaussian structure of data and estimating the causal directions of variables. In this paper, we provide an overview of such recent developments in causal inference, and focus in particular on the non-Gaussian methods known as LiNGAM. (PsycINFO Database Record (c) 2014 APA, all rights reserved) (journal abstract)},
author = {Shimizu, Shohei},
doi = {10.2333/bhmk.41.65},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Shimizu - 2014 - LiNGAM Non-Gaussian methods for estimating causal structures.pdf:pdf},
isbn = {0385-7417},
issn = {0385-7417},
journal = {Behaviormetrika},
keywords = {causal inference,causal structure learning,estimation of causal directions,non-gaussianity,struc-,tural equation models},
number = {1},
pages = {65--98},
title = {{LiNGAM: Non-Gaussian methods for estimating causal structures}},
url = {https://www.jstage.jst.go.jp/article/bhmk/41/1/41{\_}65/{\_}pdf/-char/en},
volume = {41},
year = {2014}
}
@book{PeyreCNRS,
abstract = {book draft},
author = {{Peyr{\'{e}} Gabriel} and {Cuturi Marco}},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Peyr{\'{e}} Gabriel, Cuturi Marco - 2017 - Computational Optimal Transport(2).pdf:pdf},
title = {{Computational Optimal Transport}},
url = {https://optimaltransport.github.io/pdf/ComputationalOT.pdf https://optimaltransport.github.io/book/},
year = {2017}
}
@inproceedings{Zhangc,
abstract = {It is commonplace to encounter nonstationary or heterogeneous data, of which the underlying generating process changes over time or across data sets (the data sets may have different experimental conditions or data collection conditions). Such a distribution shift feature presents both challenges and opportunities for causal discovery. In this paper we develop a principled framework for causal discovery from such data, called Constraint-based causal Discovery from Nonstationary/heterogeneous Data (CD-NOD), which addresses two important questions. First, we propose an enhanced constraint-based procedure to detect variables whose local mechanisms change and recover the skeleton of the causal structure over observed variables. Second, we present a way to determine causal orientations by making use of independence changes in the data distribution implied by the underlying causal model, benefiting from information carried by changing distributions. Experimental results on various synthetic and real-world data sets are presented to demonstrate the efficacy of our methods.},
author = {Zhang, Kun and Huang, Biwei and Zhang, Jiji and Glymour, Clark and Sch{\"{o}}lkopf, Bernhard},
booktitle = {IJCAI Proc. Conf.},
doi = {10.24963/ijcai.2017/187},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Causal Discovery from NonstationaryHeterogeneous Data Skeleton Estimation and Orientation Determination.pdf:pdf},
title = {{Causal Discovery from Nonstationary/Heterogeneous Data: Skeleton Estimation and Orientation Determination}},
url = {https://ei.is.tuebingen.mpg.de/uploads{\_}file/attachment/attachment/382/IJCAI17{\_}ID2685.pdf https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5617646/pdf/nihms904880.pdf},
year = {2017}
}
@article{DiMartino2014,
abstract = {Autism spectrum disorders (ASDs) represent a formidable challenge for psychiatry and neuroscience because of their high prevalence, lifelong nature, complexity and substantial heterogeneity. Facing these obstacles requires large-scale multidisciplinary efforts. Although the field of genetics has pioneered data sharing for these reasons, neuroimaging had not kept pace. In response, we introduce the Autism Brain Imaging Data Exchange (ABIDE)-a grassroots consortium aggregating and openly sharing 1112 existing resting-state functional magnetic resonance imaging (R-fMRI) data sets with corresponding structural MRI and phenotypic information from 539 individuals with ASDs and 573 age-matched typical controls (TCs; 7-64 years) (http://fcon{\_}1000.projects.nitrc.org/indi/abide/). Here, we present this resource and demonstrate its suitability for advancing knowledge of ASD neurobiology based on analyses of 360 male subjects with ASDs and 403 male age-matched TCs. We focused on whole-brain intrinsic functional connectivity and also survey a range of voxel-wise measures of intrinsic functional brain architecture. Whole-brain analyses reconciled seemingly disparate themes of both hypo- and hyperconnectivity in the ASD literature; both were detected, although hypoconnectivity dominated, particularly for corticocortical and interhemispheric functional connectivity. Exploratory analyses using an array of regional metrics of intrinsic brain function converged on common loci of dysfunction in ASDs (mid- and posterior insula and posterior cingulate cortex), and highlighted less commonly explored regions such as the thalamus. The survey of the ABIDE R-fMRI data sets provides unprecedented demonstrations of both replication and novel discovery. By pooling multiple international data sets, ABIDE is expected to accelerate the pace of discovery setting the stage for the next generation of ASD studies.},
author = {{Di Martino}, Adriana and Others},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Di Martino et al. - Unknown - The Autism Brain Imaging Data Exchange Towards Large-Scale Evaluation of the Intrinsic Brain Architecture.pdf:pdf},
isbn = {1476-5578 (Electronic)$\backslash$r1359-4184 (Linking)},
issn = {14765578},
journal = {Mol. Psychiatry},
keywords = {data sharing,default network,interhemispheric connectivity,intrinsic functional connectivity,resting-state fMRI,thalamus},
number = {6},
pages = {659--667},
title = {{The autism brain imaging data exchange: Towards a large-scale evaluation of the intrinsic brain architecture in autism}},
volume = {19},
year = {2014}
}
@article{Finlayson,
abstract = {The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we argue that the field of medicine may be uniquely susceptible to adversarial attacks, both in terms of mon-etary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud, we extend adversarial attacks to three popular medical imaging tasks, and we provide concrete examples of how and why such attacks could be realistically carried out. For each of our representative medical deep learning classifiers, both white and black box attacks were both effective and human-imperceptible. We urge caution in employing deep learning systems in clinical settings, and encourage research into domain-specific defense strategies.},
archivePrefix = {arXiv},
arxivId = {1804.05296},
author = {Finlayson, Samuel G and Kohane, Isaac S and Beam, Andrew L},
eprint = {1804.05296},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Finlayson, Kohane, Beam - Unknown - Adversarial Attacks Against Medical Deep Learning Systems.pdf:pdf},
title = {{Adversarial Attacks Against Medical Deep Learning Systems}},
url = {https://arxiv.org/pdf/1804.05296.pdf}
}

@article{Zhangb,
abstract = {By taking into account the nonlinear effect of the cause, the inner noise effect, and the measurement distortion effect in the observed variables, the post-nonlinear (PNL) causal model has demonstrated its excellent perfor-mance in distinguishing the cause from ef-fect. However, its identifiability has not been properly addressed, and how to apply it in the case of more than two variables is also a problem. In this paper, we conduct a system-atic investigation on its identifiability in the two-variable case. We show that this model is identifiable in most cases; by enumerating all possible situations in which the model is not identifiable, we provide sufficient conditions for its identifiability. Simulations are given to support the theoretical results. Moreover, in the case of more than two variables, we show that the whole causal structure can be found by applying the PNL causal model to each structure in the Markov equivalent class and testing if the disturbance is independent of the direct causes for each variable. In this way the exhaustive search over all possible causal structures is avoided.},
author = {Zhang, Kun and Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Hyv{\"{a}}rinen - Unknown - On the Identifiability of the Post-Nonlinear Causal Model.pdf:pdf},
title = {{On the Identifiability of the Post-Nonlinear Causal Model}},
journal = {Proc. Twenty-Fifth Conf. Uncertain. Artif. Intell.},
year = 2009
}

@article{Allman2009,
abstract = {While hidden class models of various types arise in many statistical applications, it is often difficult to establish the identifiability of their parameters. Focusing on models in which there is some structure of independence of some of the observed variables conditioned on hidden ones, we demonstrate a general approach for establishing identifiability utilizing algebraic arguments. A theorem of J. Kruskal for a simple latent-class model with finite state space lies at the core of our results, though we apply it to a diverse set of models. These include mixtures of both finite and nonparametric product distributions, hidden Markov models and random graph mixture models, and lead to a number of new results and improvements to old ones. In the parametric setting, this approach indicates that for such models, the classical definition of identifiability is typically too strong. Instead generic identifiability holds, which implies that the set of nonidentifiable parameters has measure zero, so that parameter inference is still meaningful. In particular, this sheds light on the properties of finite mixtures of Bernoulli products, which have been used for decades despite being known to have nonidentifiable parameters. In the nonparametric setting, we again obtain identifiability only when certain restrictions are placed on the distributions that are mixed, but we explicitly describe the conditions.},
archivePrefix = {arXiv},
arxivId = {0809.5032},
author = {Allman, Elizabeth S and Matias, Catherine and Rhodes, John A},
doi = {10.1214/09-AOS689},
eprint = {0809.5032},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Allman et al. - 2009 - IDENTIFIABILITY OF PARAMETERS IN LATENT STRUCTURE MODELS WITH MANY OBSERVED VARIABLES IDENTIFIABILITY OF PARAMETE.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Ann. Stat.},
keywords = {Algebraic statistics,Conditional independence,Contingency table,Finite mixture,Identiflability,Latent structure,Multivariate Bernoulli mixture,Nonparametric mixture},
number = {6 A},
pages = {3099--3132},
title = {{Identifiability of parameters in latent structure models with many observed variables}},
url = {http://www.jstor.org/stable/25662188 http://www.jstor.org/stable/25662188?seq=1{\&}cid=pdf-reference{\#}references{\_}tab{\_}contents http://about.jstor.org/terms},
volume = {37},
year = {2009}
}
@article{Allen2017,
abstract = {The authors Morris and Baladandayuthapani should be congratulated on this comprehensive and compelling review of statistical contributions in bioinformatics. In the last section, the authors discuss a burgeoning new area of research they term 'integromics', which involves integrating and jointly analyzing multiple types of 'omics' data. While we prefer the term 'data integration', we agree that this is an exciting new area of statistical and bioinformatics research. In this piece, we aim to complement Morris and Baladandayuthapani by further discussing data integration from a methodological and practical standpoint. We will specifically outline some of the major challenges of data integration, some recent successes, and highlight open areas for future research. 1 Statistical data integration and multi-view data The term data integration has come to refer to many things in many different fields. The type encountered in bioinformatics is typically integration of what we call 'multi-view' or 'multi-modal' data. Suppose that there are multiple types of omics data profiled on the same set of subjects or samples. The multiple omics platforms offer multiple views of the data or multiple data modes. If data is organized as a typical data matrix where the rows correspond to observations and columns as features, then multi-view data yields a series of coupled data matrices, each with different features (different columns) but measured on the same observations (shared rows). In this sense, the task of integrating multi-view data can be thought of as the opposite of meta-analysis: in data integration, common observations are analyzed across different sets of features as opposed to meta-analysis where common features are analyzed across different sets of observations.},
author = {Allen, Genevera I.},
doi = {10.1177/1471082X17707429},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Allen - 2017 - EProjectsNewTitleprojectsNew{\_}Freshissue{\_}IISMJ{\_}698255698255.dvi.pdf:pdf},
issn = {14770342},
journal = {Stat. Modelling},
number = {4-5},
pages = {332--337},
title = {{Statistical data integration: Challenges and opportunities}},
url = {http://www.stat.rice.edu/{~}gallen/gallen{\_}data{\_}integration{\_}2017.pdf},
volume = {17},
year = {2017}
}
@techreport{Bach2017,
abstract = {We consider neural networks with a single hidden layer and non-decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non-convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non-convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomial-time algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities.},
author = {Bach, Francis},
booktitle = {J. Mach. Learn. Res.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bach - 2017 - Breaking the Curse of Dimensionality with Convex Neural Networks.pdf:pdf},
keywords = {Neural networks,convex optimization,convex relaxation,non-parametric estimation},
pages = {1--53},
title = {{Breaking the Curse of Dimensionality with Convex Neural Networks}},
url = {http://jmlr.org/papers/v18/14-546.html.},
volume = {18},
year = {2017}
}
@book{Bertsekas2014,
author = {Bertsekas, Dimitri P.},
publisher = {Academic Press},
title = {{Constrained optimization and Lagrange multiplier methods}},
year = {2014}
}
@inproceedings{Smidl,
abstract = {An adaptive filter is derived in a Bayesian framework from the assumption that the difference in the parameter distri-bution from one time to another is bounded in terms of the Kullback-Leibler divergence. We show an explicit link to the general concepts of exponential forgetting, and outline the de-tails for a linear Gaussian model with unknown parameter and covariance. We extend the problem to an unknown forget-ting factor, where we provide a particular prior that allows for abrupt changes in forgetting, which is useful in change detec-tion problems. The Rao-Blackwellized particle filter is used for the implementation, and its performance is assessed in a simulation of system with abrupt changes of parameters.},
author = {{\v{S}}m{\'{i}}dl, V{\'{a}}clav and Gustafsson, Fredrik},
booktitle = {2012 IEEE Stat. Signal Process. Work. SSP 2012},
doi = {10.1109/SSP.2012.6319658},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/{\v{S}}m{\'{i}}dl, Gustafsson - Unknown - BAYESIAN ESTIMATION OF FORGETTING FACTOR IN ADAPTIVE FILTERING AND CHANGE DETECTION.pdf:pdf},
isbn = {9781467301831},
issn = {2168-0450},
keywords = {Adaptive filtering,Rao-Blackwellized particle filtering,exponential forgetting,maximum entropy},
pages = {197--200},
title = {{Bayesian estimation of forgetting factor in adaptive filtering and change detection}},
url = {https://pdfs.semanticscholar.org/cb05/26d1f66033bbacfda441137a585e7133de43.pdf},
year = {2012}
}
@article{Russell2017,
abstract = {Machine learning is now being used to make crucial decisions about people's lives. For nearly all of these decisions there is a risk that individuals of a certain race, gender, sexual orientation, or any other subpopulation are unfairly discriminated against. Our recent method has demonstrated how to use techniques from coun-terfactual inference to make predictions fair across different subpopulations. This method requires that one provides the causal model that generated the data at hand. In general, validating all causal implications of the model is not possible without further assumptions. Hence, it is desirable to integrate competing causal models to provide counterfactually fair decisions, regardless of which causal "world" is the correct one. In this paper, we show how it is possible to make predictions that are approximately fair with respect to multiple possible causal models at once, thus mitigating the problem of exact causal specification. We frame the goal of learning a fair classifier as an optimization problem with fairness constraints entailed by competing causal explanations. We show how this optimization problem can be efficiently solved using gradient-based methods. We demonstrate the flexibility of our model on two real-world fair classification problems. We show that our model can seamlessly balance fairness in multiple worlds with prediction accuracy.},
author = {Russell, Chris and Kusner, Matt J and Loftus, Joshua R and Silva, Ricardo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Russell et al. - Unknown - When Worlds Collide Integrating Different Counterfactual Assumptions in Fairness(2).pdf:pdf},
journal = {Neural Inf. Process. Syst.},
title = {{When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness}},
url = {http://papers.nips.cc/paper/7220-when-worlds-collide-integrating-different-counterfactual-assumptions-in-fairness.pdf},
year = {2017}
}
@article{Yang2017,
abstract = {We propose a an alternative framework to existing setups for controlling false alarms when multiple A/B tests are run over time. This setup arises in many practical applications, e.g. when pharmaceutical companies test new treatment options against control pills for different diseases, or when internet companies test their default webpages versus various alternatives over time. Our framework proposes to replace a sequence of A/B tests by a sequence of best-arm MAB instances, where each instance corresponds to an adaptive test of a single hypothesis which can be continuously monitored by the data scientist and stopped at any time. To control for multiple testing, we demonstrate how to interleave the MAB tests with an online false discovery rate (FDR) algorithm so that we can obtain the best of both worlds: low sample complexity and any time online FDR control. Our main contributions are: (i) to propose reasonable definitions of a null hypothesis for MAB instances; (ii) to demonstrate how one can derive an always-valid sequential p-value that allows continuous monitoring of each MAB test; and (iii) to show that using the rejection thresholds of online-FDR algorithms as confidence levels for the MAB algorithms results in both sample-optimality, high power and low FDR at any point in time. We run extensive simulations to verify our claims, and also report results on real data collected from the New Yorker Cartoon Caption contest.},
archivePrefix = {arXiv},
arxivId = {1706.05378},
author = {Yang, Fanny and Ramdas, Aaditya and Jamieson, Kevin and Wainwright, Martin J.},
eprint = {1706.05378},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2017 - A framework for Multi-A(rmed)B(andit) testing with online FDR control.pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst.},
title = {{A framework for Multi-A(rmed)/B(andit) testing with online FDR control}},
url = {http://papers.nips.cc/paper/7177-a-framework-for-multi-armedbandit-testing-with-online-fdr-control.pdf http://arxiv.org/abs/1706.05378},
year = {2017}
}
@article{Saremi2018,
abstract = {Density estimation is a fundamental problem in statistical learning. This problem is especially challenging for complex high-dimensional data due to the curse of dimensionality. A promising solution to this problem is given here in an inference-free hierarchical framework that is built on score matching. We revisit the Bayesian interpretation of the score function and the Parzen score matching, and construct a multilayer perceptron with a scalable objective for learning the energy (i.e. the unnormalized log-density), which is then optimized with stochastic gradient descent. In addition, the resulting deep energy estimator network (DEEN) is designed as products of experts. We present the utility of DEEN in learning the energy, the score function, and in single-step denoising experiments for synthetic and high-dimensional data. We also diagnose stability problems in the direct estimation of the score function that had been observed for denoising autoencoders.},
archivePrefix = {arXiv},
arxivId = {1805.08306},
author = {Saremi, Saeed and Mehrjou, Arash and Sch{\"{o}}lkopf, Bernhard and Hyv{\"{a}}rinen, Aapo},
eprint = {1805.08306},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Saremi et al. - Unknown - Deep Energy Estimator Networks(2).pdf:pdf},
title = {{Deep Energy Estimator Networks}},
url = {https://arxiv.org/pdf/1805.08306.pdf http://arxiv.org/abs/1805.08306},
year = {2018}
}
@article{Cun1990,
abstract = {We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cun, Yann Le and Denker, John S and Solla, Sara a},
doi = {10.1.1.32.7223},
eprint = {arXiv:1011.1669v3},
file = {:Users/ricardo/Downloads/lecun-90b.pdf:pdf},
isbn = {1558601007},
issn = {1098-6596},
journal = {Adv. Neural Inf. Process. Syst.},
number = {1},
pages = {598--605},
pmid = {25246403},
title = {{Optimal Brain Damage}},
volume = {2},
year = {1990}
}
@article{Bolukbasi2016,
abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We define metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to "debias" the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
archivePrefix = {arXiv},
arxivId = {1607.06520},
author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
eprint = {1607.06520},
file = {:Users/ricardo/Downloads/1607.06520.pdf:pdf},
issn = {10495258},
pages = {1--25},
title = {{Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}},
url = {http://arxiv.org/abs/1607.06520},
year = {2016}
}
@article{Balduzzi2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1702.08591v1},
author = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
eprint = {arXiv:1702.08591v1},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(3).pdf:pdf},
issn = {1938-7228},
journal = {ICML},
title = {{The Shattered Gradients Problem: If resnets are the answer, then what is the question?}},
year = {2017}
}
@article{Shah2018,
abstract = {It is a common saying that testing for conditional independence, i.e., testing whether X is independent of Y, given Z, is a hard statistical problem if Z is a continuous random variable. In this paper, we prove that conditional independence is indeed a particularly difficult hypothesis to test for. Statistical tests are required to have a size that is smaller than a predefined significance level, and different tests usually have power against a different class of alternatives. We prove that a valid test for conditional independence does not have power against any alternative. Given the non-existence of a uniformly valid conditional independence test, we argue that tests must be designed so their suitability for a particular problem setting may be judged easily. To address this need, we propose in the case where X and Y are univariate to nonlinearly regress X on Z, and Y on Z and then compute a test statistic based on the sample covariance between the residuals, which we call the generalised covariance measure (GCM). We prove that validity of this form of test relies almost entirely on the weak requirement that the regression procedures are able to estimate the conditional means X given Z, and Y given Z, at a slow rate. We extend the methodology to handle settings where X and Y may be multivariate or even high-dimensional. While our general procedure can be tailored to the setting at hand by combining it with any regression technique, we develop the theoretical guarantees for kernel ridge regression. A simulation study shows that the test based on GCM is competitive with state of the art conditional independence tests. Code will be available as an R package.},
archivePrefix = {arXiv},
arxivId = {1804.07203},
author = {Shah, Rajen D and Peters, Jonas},
eprint = {1804.07203},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Shah, Peters - 2018 - The Hardness of Conditional Independence Testing and the Generalised Covariance Measure.pdf:pdf},
title = {{The Hardness of Conditional Independence Testing and the Generalised Covariance Measure}},
url = {https://arxiv.org/pdf/1804.07203.pdf http://arxiv.org/abs/1804.07203},
year = {2018}
}
@article{Ding2008,
abstract = {Non-negative Matrix Factorization (NMF) and Probabilistic Latent Semantic Indexing (PLSI) have been successfully applied to document clustering recently. In this paper, we show that PLSI and NMF (with the I-divergence objective function) optimize the same objective function, although PLSI and NMF are different algorithms as verified by experiments. This provides a theoretical basis for a new hybrid method that runs PLSI and NMF alternatively, each jumping out of the local minima of the other method successively, thus achieving a better final solution. Extensive experiments on five real-life datasets show relations between NMF and PLSI, and indicate that the hybrid method leads to significant improvements over NMF-only or PLSI-only methods. We also show that at first-order approximation, NMF is identical to the ??2-statistic. ?? 2008.},
author = {Ding, Chris and Li, Tao and Peng, Wei},
doi = {10.1016/j.csda.2008.01.011},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ding, Li, Peng - 2008 - On the equivalence between Non-negative Matrix Factorization and Probabilistic Latent Semantic Indexing.pdf:pdf},
isbn = {01679473},
issn = {01679473},
journal = {Comput. Stat. Data Anal.},
number = {8},
pages = {3913--3927},
title = {{On the equivalence between Non-negative Matrix Factorization and Probabilistic Latent Semantic Indexing}},
url = {www.elsevier.com/locate/csda},
volume = {52},
year = {2008}
}
@article{Smith2017a,
abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate {\$}\backslashepsilon{\$} and scaling the batch size {\$}B \backslashpropto \backslashepsilon{\$}. Finally, one can increase the momentum coefficient {\$}m{\$} and scale {\$}B \backslashpropto 1/(1-m){\$}, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train Inception-ResNet-V2 on ImageNet to {\$}77\backslash{\%}{\$} validation accuracy in under 2500 parameter updates, efficiently utilizing training batches of 65536 images.},
archivePrefix = {arXiv},
arxivId = {1711.00489},
author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Le, Quoc V.},
eprint = {1711.00489},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Smith et al. - Unknown - DON'T DECAY THE LEARNING RATE, INCREASE THE BATCH SIZE.pdf:pdf},
title = {{Don't Decay the Learning Rate, Increase the Batch Size}},
url = {http://arxiv.org/abs/1711.00489},
year = {2017}
}
@article{Peters2015,
abstract = {What is the difference of a prediction that is made with a causal model and a non-causal model? Suppose we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (for example various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.},
archivePrefix = {arXiv},
arxivId = {1501.01332},
author = {Peters, Jonas and B{\"{u}}hlmann, Peter and Meinshausen, Nicolai},
eprint = {1501.01332},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Peters, B{\"{u}}hlmann, Meinshausen - 2015 - Causal inference using invariant prediction identification and confidence intervals.pdf:pdf},
title = {{Causal inference using invariant prediction: identification and confidence intervals}},
url = {https://arxiv.org/pdf/1501.01332.pdf http://arxiv.org/abs/1501.01332},
year = {2015}
}
@article{Balan2017,
abstract = {In this paper we discuss the stability properties of convolutional neural networks. Convolutional neural networks are widely used in machine learning. In classification they are mainly used as feature extractors. Ideally, we expect similar features when the inputs are from the same class. That is, we hope to see a small change in the feature vector with respect to a deformation on the input signal. This can be established mathematically, and the key step is to derive the Lipschitz properties. Further, we establish that the stability results can be extended for more general networks. We give a formula for computing the Lipschitz bound, and compare it with other methods to show it is closer to the optimal value.},
archivePrefix = {arXiv},
arxivId = {1701.05217},
author = {Balan, Radu and Singh, Maneesh and Zou, Dongmian},
eprint = {1701.05217},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Balan, Singh, Zou - 2017 - Lipschitz Properties for Deep Convolutional Networks.pdf:pdf},
journal = {arXiv},
title = {{Lipschitz Properties for Deep Convolutional Networks}},
url = {https://arxiv.org/pdf/1701.05217.pdf http://arxiv.org/abs/1701.05217},
year = {2017}
}
@techreport{Hernandez-Lobato2016a,
abstract = {We provide theoretical and empirical evidence for a type of asymmetry between causes and effects that is present when these are related via linear models contaminated with additive non-Gaussian noise. Assuming that the causes and the effects have the same distribution, we show that the distribution of the residuals of a linear fit in the anti-causal direction is closer to a Gaussian than the distribution of the residuals in the causal direction. This Gaussianization effect is characterized by reduction of the magnitude of the high-order cumulants and by an increment of the differential entropy of the residuals. The problem of non-linear causal inference is addressed by performing an embedding in an expanded feature space, in which the relation between causes and effects can be assumed to be linear. The effectiveness of a method to discriminate between causes and effects based on this type of asymmetry is illustrated in a variety of experiments using different measures of Gaussianity. The proposed method is shown to be competitive with state-of-the-art techniques for causal inference.},
author = {Hern{\'{a}}ndez-Lobato, Daniel and Morales-Mombiela, Pablo and Lopez-Paz, David and Su{\'{a}}rez, Alberto},
booktitle = {J. Mach. Learn. Res.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hern{\'{a}}ndez-Lobato et al. - 2016 - Non-linear Causal Inference using Gaussianity Measures(2).pdf:pdf},
keywords = {Gaussianity of the residuals,causal inference,cause-effect pairs},
pages = {1--39},
title = {{Non-linear Causal Inference using Gaussianity Measures}},
url = {http://www.jmlr.org/papers/volume17/14-375/14-375.pdf},
volume = {17},
year = {2016}
}
@article{Zass2005,
abstract = {We derive the clustering problem from first principles showing that the goal of achieving a probabilistic, or "hard", multi class clustering result is equivalent to the algebraic problem of a completely positive factorization under a doubly stochastic constraint. We show that spectral clustering, normalized cuts, kernel K-means and the various normalizations of the associated affinity matrix are particular instances and approximations of this general principle. We propose an efficient algorithm for achieving a completely positive factorization and extend the basic clustering scheme to situations where partial label information is available.},
author = {Zass, Ron and Shashua, Amnon},
doi = {10.1109/ICCV.2005.27},
file = {:Users/ricardo/Downloads/download.pdf:pdf},
isbn = {076952334X},
issn = {1550-5499},
journal = {Proc. IEEE Int. Conf. Comput. Vis.},
number = {November 2005},
pages = {294--301},
title = {{A unifying approach to hard and probabilistic clustering}},
volume = {I},
year = {2005}
}
@techreport{Loftus,
abstract = {In this work, we argue for the importance of causal reasoning in creating fair algorithms for decision making. We give a review of existing approaches to fairness, describe work in causality necessary for the understanding of causal approaches, argue why causality is necessary for any approach that wishes to be fair, and give a detailed analysis of the many recent approaches to causality-based fairness.},
archivePrefix = {arXiv},
arxivId = {arXiv:1805.05859v1},
author = {Loftus, Joshua R and Russell, Chris and Kusner, Matt J and Silva, Ricardo},
eprint = {arXiv:1805.05859v1},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Loftus et al. - Unknown - Causal Reasoning for Algorithmic Fairness.pdf:pdf},
title = {{Causal Reasoning for Algorithmic Fairness}},
url = {https://arxiv.org/pdf/1805.05859.pdf}
}
@article{Yu2016a,
abstract = {Probabilistic graphical models have been widely used to model complex systems and aid scientific discoveries. As a result, there is a large body of literature focused on consistent model selection. However, scientists are often interested in understanding uncertainty associated with the estimated parameters, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for edge parameters for pairwise graphical models based on Hyv$\backslash$"arinen scoring rule. Hyv$\backslash$"arinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form. We prove that the estimator is {\$}\backslashsqrt{\{}n{\}}{\$}-consistent and asymptotically Normal. This result allows us to construct confidence intervals for edge parameters, as well as, hypothesis tests. We establish our results under conditions that are typically assumed in the literature for consistent estimation. However, we do not require that the estimator consistently recovers the graph structure. In particular, we prove that the asymptotic distribution of the estimator is robust to model selection mistakes and uniformly valid for a large number of data-generating processes. We illustrate validity of our estimator through extensive simulation studies.},
author = {Yu, Ming and Kolar, Mladen and Gupta, Varun},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Yu, Kolar, Gupta - 2016 - Statistical Inference for Pairwise Graphical Models Using Score Matching.pdf:pdf},
journal = {NIPS},
number = {Nips},
pages = {2829--2837},
title = {{Statistical Inference for Pairwise Graphical Models Using Score Matching}},
url = {http://papers.nips.cc/paper/6530-statistical-inference-for-pairwise-graphical-models-using-score-matching},
year = {2016}
}
@article{Hyvarinen2013,
abstract = {We present new measures of the causal direction, or direction of effect, between two non-Gaussian random variables. They are based on the likelihood ratio under the linear non-Gaussian acyclic model (LiNGAM). We also develop simple first-order approximations of the likelihood ratio and analyze them based on related cumulant-based measures, which can be shown to find the correct causal directions. We show how to apply these measures to estimate LiNGAM for more than two variables, and even in the case of more variables than observations. We further extend the method to cyclic and nonlinear models. The proposed framework is statistically at least as good as existing ones in the cases of few data points or noisy data, and it is computationally and conceptually very simple. Results on simulated fMRI data indicate that the method may be useful in neuroimaging where the number of time points is typically quite small.},
author = {Hyv{\"{a}}rinen, Aapo and Smith, Stephen M},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hyv{\"{a}}rinen AAPOHYVARINEN, Smith - 2013 - Pairwise Likelihood Ratios for Estimation of Non-Gaussian Structural Equation Models.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {Bayesian network,causality,independent component analysis,non-Gaussianity,structural equation model},
pages = {111--152},
title = {{Pairwise Likelihood Ratios for Estimation of Non-Gaussian Structural Equation Models}},
volume = {14},
year = {2013}
}
@article{Kietzmann2017,
abstract = {The goal of computational neuroscience is to find mechanistic explanations of how the nervous system processes information to support cognitive function and behaviour. At the heart of the field are its models, i.e. mathematical and computational descriptions of the system being studied. These models typically map sensory stimuli to neural responses and/or neural to behavioural responses and range for simple to complex. Recently, deep neural networks (DNNs), using either feedforward and recurrent architectures, have come to dominate several domains of artificial intelligence (AI). As the term " neural network " suggests, these models are inspired by biological brains. However, current DNN models abstract from many details of biological neural networks. Their abstractions contribute to their computational efficiency, enabling to perform complex feats of intelligence, ranging from perceptual tasks (e.g. visual object and auditory speech recognition) to cognitive tasks (e.g. machine translation), and on to motor control tasks (e.g. playing computer games or controlling a robot arm). In addition to their ability to model complex intelligent behaviours, DNNs have been shown to predict neural responses to novel sensory stimuli that cannot be predicted with any other currently available type of model. DNNs can have millions of parameters (connection strengths), which are required to capture the domain knowledge needed for task performance. These parameters are often set by task training using stochastic gradient descent. The computational properties of the units are the result of four directly manipulable elements: input statistics, network structure, functional objective, and learning algorithm. The advances with neural nets in engineering provide the technological basis for building task-performing models of varying degrees of biological realism that promise substantial insights for computational neuroscience.},
archivePrefix = {arXiv},
arxivId = {133504},
author = {Kietzmann, Tim C and Mcclure, Patrick and Kriegeskorte, Nikolaus},
doi = {10.1101/133504},
eprint = {133504},
file = {:Users/ricardo/Downloads/DNNs in neuroscience.pdf:pdf},
keywords = {biological detail,black box,convolutional neural networks,deep learning,deep neural networks,input statistics,levels of abstraction,modelling the brain,objective functions,recurrence},
title = {{Deep Neural Networks in Computational Neuroscience}},
year = {2017}
}
@article{Janzing2015,
abstract = {

It has become increasingly popular to obtain machine learning
labels through commercial crowdsourcing services. The
crowdsourcing workers or annotators are paid for each label they
provide, but the task requester usually has only a limited
amount of the budget. Since the data instances have different
levels of labeling difficulty and the workers have different
reliability for the labeling task, it is desirable to wisely
allocate the budget among all the instances and workers such
that the overall labeling quality is maximized. In this paper,
we formulate the budget allocation problem as a Bayesian Markov
decision process (MDP), which simultaneously conducts learning
and decision making. The optimal allocation policy can be
obtained by using the dynamic programming (DP) recurrence.
However, DP quickly becomes computationally intractable when the
size of the problem increases. To solve this challenge, we
propose a computationally efficient approximate policy which is
called optimistic knowledge gradient. Our method applies to both
pull crowdsourcing marketplaces with homogeneous workers and
push marketplaces with heterogeneous workers. It can also
incorporate the contextual information of instances when they
are available. The experiments on both simulated and real data
show that our policy achieves a higher labeling quality than
other existing policies at the same budget level.},
author = {Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/ricardo/Downloads/janzing15a.pdf:pdf},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
keywords = {anticausal learning,causality,independence of cause and,information geometry,mechanism,semi-supervised learning},
pages = {1923--1948},
title = {{Semi-Supervised Interpolation in an Anticausal Learning Scenario}},
url = {http://jmlr.org/papers/v16/janzing15a.html},
volume = {16},
year = {2015}
}
@article{Parra2018,
abstract = {How does one find data dimensions that are reliably expressed across repetitions? For exam-ple, in neuroscience one may want to identify combinations of brain signals that are reliably activated across multiple trials or subjects. For a clinical assessment with multiple ratings, one may want to identify an aggregate score that is reliably reproduced across raters. The approach proposed here — " correlated components analysis " — is to identify components that maximally correlate between repetitions (e.g. trials, subjects, raters). This can be expressed as the maximization of the ratio of between-repetition to within-repetition co-variance, resulting in a generalized eigenvalue problem. We show that covariances can be computed efficiently without explicitly considering all pairs of repetitions, that the result is equivalent to multi-class linear discriminant analysis for unbiased signals, and that the approach also maximize reliability, defined as the mean divided by the deviation across repetitions. We also extend the method to non-linear components using kernels, discuss regularization to improve numerical stability, present parametric and non-parametric tests to establish statistical significance, and provide code.},
archivePrefix = {arXiv},
arxivId = {1801.08881},
author = {Parra, Lucas C and Haufe, Stefan and Dmochowski, Jacek P},
eprint = {1801.08881},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Parra, Haufe, Dmochowski - Unknown - Correlated Components Analysis — Extracting Reliable Dimensions in Multivariate Data.pdf:pdf},
journal = {arXiv},
title = {{Correlated Components Analysis — Extracting Reliable Dimensions in Multivariate Data}},
url = {https://arxiv.org/pdf/1801.08881.pdf},
year = {2018}
}
@article{Huang2017,
abstract = {Deep neural networks are known to be difficult to train due to the instability of back-propagation. A deep $\backslash$emph{\{}residual network{\}} (ResNet) with identity loops remedies this by stabilizing gradient computations. We prove a boosting theory for the ResNet architecture. We construct {\$}T{\$} weak module classifiers, each contains two of the {\$}T{\$} layers, such that the combined strong learner is a ResNet. Therefore, we introduce an alternative Deep ResNet training algorithm, $\backslash$emph{\{}BoostResNet{\}}, which is particularly suitable in non-differentiable architectures. Our proposed algorithm merely requires a sequential training of {\$}T{\$} "shallow ResNets" which are inexpensive. We prove that the training error decays exponentially with the depth {\$}T{\$} if the $\backslash$emph{\{}weak module classifiers{\}} that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. Our results apply to general multi-class ResNets. A generalization error bound based on margin theory is proved and suggests ResNet's resistant to overfitting under network with {\$}l{\_}1{\$} norm bounded weights.},
archivePrefix = {arXiv},
arxivId = {1706.04964},
author = {Huang, Furong and Ash, Jordan and Langford, John and Schapire, Robert},
eprint = {1706.04964},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2017 - Learning Deep ResNet Blocks Sequentially using Boosting Theory.pdf:pdf},
keywords = {()},
title = {{Learning Deep ResNet Blocks Sequentially using Boosting Theory}},
url = {https://arxiv.org/pdf/1706.04964.pdf http://arxiv.org/abs/1706.04964},
year = {2017}
}
@inproceedings{Scholkopf2012_causal,
abstract = {We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.},
author = {Sch{\"{o}}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
booktitle = {Int. Conf. Mach. Learn.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Sch{\"{o}}lkopf et al. - Unknown - On Causal and Anticausal Learning(2).pdf:pdf},
pages = {1255--1262},
title = {{On Causal and Anticausal Learning}},
year = {2012}
}
@article{Bedi2015,
abstract = {npj Schizophrenia , (2015). doi:10.1038/npjschz.2015.30},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Bedi, Gillinder and Carrillo, Facundo and Cecchi, Guillermo A and Slezak, Diego Fern{\'{a}}ndez and Sigman, Mariano and Mota, Nat{\'{a}}lia B and Ribeiro, Sidarta and Javitt, Daniel C and Copelli, Mauro and Corcoran, Cheryl M},
doi = {10.1038/npjschz.2015.30},
eprint = {NIHMS150003},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bedi et al. - 2015 - Automated analysis of free speech predicts psychosis onset in high-risk youths.pdf:pdf},
isbn = {2334-265X},
issn = {2334-265X},
journal = {npj Schizophr.},
number = {1},
pages = {15030},
pmid = {27336038},
title = {{Automated analysis of free speech predicts psychosis onset in high-risk youths}},
url = {https://www.nature.com/articles/npjschz201530.pdf http://www.nature.com/articles/npjschz201530},
volume = {1},
year = {2015}
}
@article{Goodman,
abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also effectively create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.},
archivePrefix = {arXiv},
arxivId = {1606.08813},
author = {Goodman, Bryce and Flaxman, Seth},
doi = {10.1609/aimag.v38i3.2741},
eprint = {1606.08813},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Goodman, Flaxman - Unknown - European Union regulations on algorithmic decision-making and a {\&}quot right to explanation {\&}quot.pdf:pdf},
isbn = {978-0-674-36827-9},
issn = {0738-4602},
title = {{European Union regulations on algorithmic decision-making and a "right to explanation"}},
url = {https://arxiv.org/pdf/1606.08813.pdf http://arxiv.org/abs/1606.08813},
year = {2016}
}
@article{Hirayama2016,
abstract = {Characterizing the variability of resting-state functional brain connectivity across subjects and/or over time has recently attracted much attention. Principal component analysis (PCA) serves as a fundamental statistical technique for such analyses. However, performing PCA on high-dimensional connectivity matrices yields complicated "eigenconnectivity" patterns, for which systematic interpretation is a challenging issue. Here, we overcome this issue with a novel constrained PCA method for connectivity matrices by extending the idea of the previously proposed orthogonal connectivity factorization method. Our new method, modular connectivity factorization (MCF), explicitly introduces the modularity of brain networks as a parametric constraint on eigenconnectivity matrices. In particular, MCF analyzes the variability in both intra- and inter-module connectivities, simultaneously finding network modules in a principled, data-driven manner. The parametric constraint provides a compact module-based visualization scheme with which the result can be intuitively interpreted. We develop an optimization algorithm to solve the constrained PCA problem and validate our method in simulation studies and with a resting-state functional connectivity MRI dataset of 986 subjects. The results show that the proposed MCF method successfully reveals the underlying modular eigenconnectivity patterns in more general situations and is a promising alternative to existing methods.},
author = {Hirayama, Jun Ichiro and Hyv{\"{a}}rinen, Aapo and Kiviniemi, Vesa and Kawanabe, Motoaki and Yamashita, Okito},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hirayama et al. - Unknown - Characterizing Variability of Modular Brain Connectivity with Constrained Principal Component Analysis(2).pdf:pdf},
journal = {PLoS One},
number = {12},
title = {{Characterizing variability of modular brain connectivity with constrained principal component analysis}},
volume = {11},
year = {2016}
}
@article{Ramdas,
abstract = {A significant literature studies ways of employing prior knowledge to improve power and precision of multiple testing procedures. Some common forms of prior knowledge may include (a) a priori beliefs about which hypotheses are null, modeled by non-uniform prior weights; (b) differing importances of hypotheses, modeled by differing penalties for false discoveries; (c) multiple arbitrary partitions of the hypotheses into known (possibly overlapping) groups, indicating (dis)similarity of hypotheses; and (d) knowledge of independence, positive or arbitrary dependence between hypotheses or groups, allowing for more aggressive or conservative procedures. We present a unified algorithmic framework called p-filter for global null testing and false discovery rate (FDR) control that allows the scientist to incorporate all four types of prior knowledge (a)-(d) simultaneously, recovering a wide variety of common algorithms as special cases.},
archivePrefix = {arXiv},
arxivId = {1703.06222},
author = {Ramdas, Aaditya and Barber, Rina Foygel and Wainwright, Martin J and Jordan, Michael I},
eprint = {1703.06222},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ramdas et al. - Unknown - A UNIFIED TREATMENT OF MULTIPLE TESTING WITH PRIOR KNOWLEDGE USING THE P-FILTER.pdf:pdf},
title = {{A unified treatment of multiple testing with prior knowledge using the p-filter}},
url = {https://arxiv.org/pdf/1703.06222.pdf http://arxiv.org/abs/1703.06222},
year = {2017}
}
@article{Smith2014,
abstract = {Increasingly-large datasets (for example, the resting-state fMRI data from the Human Connectome Project) are demanding analyses that are problematic because of the sheer scale of the aggregate data. We present two approaches for applying group-level PCA; both give a close approximation to the output of PCA applied to full concatenation of all individual datasets, while having very low memory requirements regardless of the number of datasets being combined. Across a range of realistic simulations, we find that in most situations, both methods are more accurate than current popular approaches for analysis of multi-subject resting-state fMRI studies. The group-PCA output can be used to feed into a range of further analyses that are then rendered practical, such as the estimation of group-averaged voxelwise connectivity, group-level parcellation, and group-ICA.},
author = {Smith, Stephen M and Hyv{\"{a}}rinen, Aapo and Varoquaux, Ga{\"{e}}l and Miller, Karla L. and Beckmann, Christian F.},
doi = {10.1016/j.neuroimage.2014.07.051},
file = {:Users/ricardo/Downloads/Group-PCA for very large fMRI datasets.pdf:pdf},
isbn = {1053-8119},
issn = {10959572},
journal = {Neuroimage},
keywords = {Big data,FMRI,ICA,PCA},
pages = {738--749},
pmid = {25094018},
title = {{Group-PCA for very large fMRI datasets}},
volume = {101},
year = {2014}
}
@article{Kalisch2005,
abstract = {We consider the PC-algorithm Spirtes et. al. (2000) for estimating the skeleton of a very high-dimensional acyclic directed graph (DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible for sparse problems with many nodes, i.e. variables, and it has the attractive property to automatically achieve high computational efficiency as a function of sparseness of the true underlying DAG. We prove consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(n{\^{}}a) for any 0{\textless}a{\textless}infinity. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We empirically demonstrate the PC-algorithm for simulated data and argue that the algorithm is rather insensitive to the choice of its single tuning parameter.},
archivePrefix = {arXiv},
arxivId = {math/0510436},
author = {Kalisch, Markus and Buehlmann, Peter},
eprint = {0510436},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kalisch, Ch, Uhlmann - 2007 - Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {DAG,PC-algorithm,asymptotic consistency,graphical model,skeleton},
pages = {613--636},
primaryClass = {math},
title = {{Estimating high-dimensional directed acyclic graphs with the PC-algorithm}},
url = {http://www.jmlr.org/papers/volume8/kalisch07a/kalisch07a.pdf http://arxiv.org/abs/math/0510436},
volume = {8},
year = {2005}
}
@article{Pan2016,
abstract = {The prediction of salient areas in images has been traditionally addressed with hand-crafted features based on neuroscience principles. This paper, however, addresses the problem with a completely data-driven approach by training a convolutional neural network (convnet). The learning process is formulated as a minimization of a loss function that measures the Euclidean distance of the predicted saliency map with the provided ground truth. The recent publication of large datasets of saliency prediction has provided enough data to train end-to-end architectures that are both fast and accurate. Two designs are proposed: a shallow convnet trained from scratch, and a another deeper solution whose first three layers are adapted from another network trained for classification. To the authors knowledge, these are the first end-to-end CNNs trained and tested for the purpose of saliency prediction.},
archivePrefix = {arXiv},
arxivId = {1603.00845},
author = {Pan, Junting and McGuinness, Kevin and Sayrol, Elisa and O'Connor, Noel and Giro-i-Nieto, Xavier},
doi = {10.1109/CVPR.2016.71},
eprint = {1603.00845},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Pan et al. - Unknown - Shallow and Deep Convolutional Networks for Saliency Prediction.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
journal = {CVPR},
pages = {598--606},
title = {{Shallow and Deep Convolutional Networks for Saliency Prediction}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/papers/Pan{\_}Shallow{\_}and{\_}Deep{\_}CVPR{\_}2016{\_}paper.pdf http://arxiv.org/abs/1603.00845},
year = {2016}
}
@article{Airoldi2008,
abstract = {Consider data consisting of pairwise measurements, such as presence or absence of links between pairs of objects. These data arise, for instance, in the analysis of protein interactions and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing pair-wise measurements with probabilistic models requires special assumptions, since the usual inde-pendence or exchangeability assumptions no longer hold. Here we introduce a class of variance allocation models for pairwise measurements: mixed membership stochastic blockmodels. These models combine global parameters that instantiate dense patches of connectivity (blockmodel) with local parameters that instantiate node-specific variability in the connections (mixed membership). We develop a general variational inference algorithm for fast approximate posterior inference. We demonstrate the advantages of mixed membership stochastic blockmodels with applications to so-cial networks and protein interaction networks.},
author = {Airoldi, Edoardo M and Blei, David M and Fienberg, Stephen E and Xing, Eric P},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Airoldi et al. - 2008 - Mixed Membership Stochastic Blockmodels(2).pdf:pdf},
isbn = {2122633255},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {analysis,hierarchical bayes,latent variables,mean-field approximation,protein interaction networks,social networks,statistical network},
number = {2008},
pages = {1981--2014},
title = {{Mixed Membership Stochastic Blockmodels}},
volume = {9},
year = {2008}
}
@article{Karrer1987,
abstract = {Stochastic blockmodels have been proposed as a tool for detecting community structure in net-works as well as for generating synthetic networks for use as benchmarks. Most blockmodels, however, ignore variation in vertex degree, making them unsuitable for applications to real-world networks, which typically display broad degree distributions that can significantly distort the results. Here we demonstrate how the generalization of blockmodels to incorporate this missing element leads to an improved objective function for community detection in complex networks. We also propose a heuristic algorithm for community detection using this objective function or its non-degree-corrected counterpart and show that the degree-corrected version dramatically outperforms the uncorrected one in both real-world and synthetic networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1008.3926v1},
author = {Karrer, Brian and Newman, M E J},
eprint = {arXiv:1008.3926v1},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Karrer, Newman - 2010 - Stochastic blockmodels and community structure in networks.pdf:pdf},
keywords = {()},
pages = {1--11},
title = {{Stochastic blockmodels and community structure in networks}},
url = {https://arxiv.org/pdf/1008.3926.pdf},
year = {2011}
}
@article{Cunningham2015,
abstract = {Linear dimensionality reduction methods are a cornerstone of analyzing high dimensional data, due to their simple geometric interpretations and typically attractive computational properties. These methods capture many data features of interest, such as covariance, dy-namical structure, correlation between data sets, input-output relationships, and margin between data classes. Methods have been developed with a variety of names and motiva-tions in many fields, and perhaps as a result the connections between all these methods have not been highlighted. Here we survey methods from this disparate literature as opti-mization programs over matrix manifolds. We discuss principal component analysis, factor analysis, linear multidimensional scaling, Fisher's linear discriminant analysis, canonical correlations analysis, maximum autocorrelation factors, slow feature analysis, sufficient di-mensionality reduction, undercomplete independent component analysis, linear regression, distance metric learning, and more. This optimization framework gives insight to some rarely discussed shortcomings of well-known methods, such as the suboptimality of certain eigenvector solutions. Modern techniques for optimization over matrix manifolds enable a generic linear dimensionality reduction solver, which accepts as input data and an ob-jective to be optimized, and returns, as output, an optimal low-dimensional projection of the data. This simple optimization framework further allows straightforward general-izations and novel variants of classical methods, which we demonstrate here by creating an orthogonal-projection canonical correlations analysis. More broadly, this survey and generic solver suggest that linear dimensionality reduction can move toward becoming a blackbox, objective-agnostic numerical technology.},
author = {Cunningham, John P and Ghahramani, Zoubin},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Cunningham, Ghahramani - 2015 - Linear Dimensionality Reduction Survey, Insights, and Generalizations.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {dimensionality reduction,eigenvector problems,matrix manifolds},
pages = {2859--2900},
title = {{Linear Dimensionality Reduction: Survey, Insights, and Generalizations}},
volume = {16},
year = {2015}
}
@article{Spirtes2016,
author = {Spirtes, Peter and Zhang, Kun},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Spirtes, Zhang - 2016 - Causal discovery and inference concepts and recent methodological advances(2).pdf:pdf},
journal = {Appl. Informatics},
keywords = {Causal discovery,Causal inference,Conditional independence,Identifiability,Statistical independence,Structural equation model},
title = {{Causal discovery and inference: concepts and recent methodological advances}},
year = {2016}
}
@book{Pearl2009,
author = {Pearl, Judea},
publisher = {Cambridge University Press},
title = {{Causality}},
year = {2009}
}
@article{Shimizu2006,
abstract = {In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data. Such methods make various assumptions on the data generating process to facilitate its identification from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assump-tions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-Gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis, and does not require any pre-specified time-ordering of the variables. We provide a complete Matlab package for per-forming this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artificially generated data and real-world data.},
author = {Shimizu, Shohei and Hoyer, Patrik O and Hyv{\"{a}}rinen, Aapo and Kerminen, Antti},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Shimizu, Patrik Hoyer PATRIKHOYER, Antti Kerminen ANTTIKERMINEN - 2006 - A Linear Non-Gaussian Acyclic Model for Causal Discovery Aapo.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {causal discovery,directed acyclic graph,independent component analysis,non-Gaussianity,non-experimental data},
pages = {2003--2030},
title = {{A Linear Non-Gaussian Acyclic Model for Causal Discovery}},
volume = {7},
year = {2006}
}
@article{Bien2011,
abstract = {We suggest a method for estimating a covariance matrix on the basis of a sample of vectors drawn from a multivariate normal distribution. In particular, we penalize the likelihood with a lasso penalty on the entries of the covariance matrix. This penalty plays two important roles: it reduces the effective number of parameters, which is important even when the dimension of the vectors is smaller than the sample size since the number of parameters grows quadratically in the number of variables, and it produces an estimate which is sparse. In contrast to sparse inverse covariance estimation, our method's close relative, the sparsity attained here is in the covariance matrix itself rather than in the inverse matrix. Zeros in the covariance matrix correspond to marginal independencies; thus, our method performs model selection while providing a positive definite estimate of the covariance. The proposed penalized maximum likelihood problem is not convex, so we use a majorize-minimize approach in which we iteratively solve convex approximations to the original nonconvex problem. We discuss tuning parameter selection and demonstrate on a flow-cytometry dataset how our method produces an interpretable graphical display of the relationship between variables. We perform simulations that suggest that simple elementwise thresholding of the empirical covariance matrix is competitive with our method for identifying the sparsity structure. Additionally, we show how our method can be used to solve a previously studied special case in which a desired sparsity pattern is prespecified.},
author = {Bien, Jacob and Tibshirani, Robert J},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bien, Tibshirani - 2011 - Sparse estimation of a covariance matrix.pdf:pdf},
journal = {Biometrika},
keywords = {Concave-convex procedure,Covariance graph,Covariance matrix,Generalized gradient descent,Lasso,Majorization-minimization,Regularization,Sparsity},
month = {dec},
number = {4},
pages = {807--820},
publisher = {Oxford University Press},
title = {{Sparse estimation of a covariance matrix}},
volume = {98},
year = {2011}
}
@article{Ktena,
abstract = {Evaluating similarity between graphs is of major importance in several computer vision and pattern recognition problems, where graph representations are often used to model objects or interactions between elements. The choice of a distance or similarity metric is, however, not trivial and can be highly dependent on the application at hand. In this work, we propose a novel metric learning method to evaluate distance between graphs that leverages the power of convolutional neural networks, while exploiting concepts from spectral graph theory to allow these operations on irregular graphs. We demonstrate the potential of our method in the field of connectomics, where neuronal pathways or functional connections between brain regions are commonly modelled as graphs. In this problem, the definition of an appropriate graph similarity function is critical to unveil patterns of disruptions associated with certain brain disorders. Experimental results on the ABIDE dataset show that our method can learn a graph similarity metric tailored for a clinical application, improving the performance of a simple k-nn classifier by 11.9{\%} compared to a traditional distance metric.},
archivePrefix = {arXiv},
arxivId = {1703.02161},
author = {Ktena, Sofia Ira and Parisot, Sarah and Ferrante, Enzo and Rajchl, Martin and Lee, Matthew and Glocker, Ben and Rueckert, Daniel},
doi = {10.1007/978-3-319-66182-7_54},
eprint = {1703.02161},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ktena et al. - Unknown - Distance Metric Learning using Graph Convolutional Networks Application to Functional Brain Networks.pdf:pdf},
isbn = {978-3-319-66184-1},
title = {{Distance Metric Learning using Graph Convolutional Networks: Application to Functional Brain Networks}},
url = {https://arxiv.org/pdf/1703.02161.pdf http://arxiv.org/abs/1703.02161},
year = {2017}
}
@article{Ng,
abstract = {We propose a dynamic edge exchangeable network model that can capture sparse connections observed in real temporal networks, in contrast to existing models which are dense. The model achieved superior link prediction accuracy on multiple data sets when compared to a dynamic variant of the blockmodel, and is able to extract interpretable time-varying community structures from the data. In addition to sparsity, the model accounts for the effect of social influence on vertices' future behaviours. Compared to the dynamic blockmodels, our model has a smaller latent space. The compact latent space requires a smaller number of parameters to be estimated in variational inference and results in a computationally friendly inference algorithm.},
archivePrefix = {arXiv},
arxivId = {1710.04008},
author = {Ng, Yin Cheng and Silva, Ricardo},
eprint = {1710.04008},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ng, Silva - Unknown - A Dynamic Edge Exchangeable Model for Sparse Temporal Networks(2).pdf:pdf},
title = {{A Dynamic Edge Exchangeable Model for Sparse Temporal Networks}},
url = {https://arxiv.org/pdf/1710.04008.pdf http://arxiv.org/abs/1710.04008},
year = {2017}
}
@article{Tan2015,
abstract = {The task of estimating a Gaussian graphical model in the high-dimensional setting is con-sidered. The graphical lasso, which involves maximizing the Gaussian log likelihood sub-ject to a lasso penalty, is a well-studied approach for this task. A surprising connection between the graphical lasso and hierarchical clustering is introduced: the graphical lasso in effect performs a two-step procedure, in which (1) single linkage hierarchical clustering is performed on the variables in order to identify connected components, and then (2) a penalized log likelihood is maximized on the subset of variables within each connected component. Thus, the graphical lasso determines the connected components of the esti-mated network via single linkage clustering. The single linkage clustering is known to per-form poorly in certain finite-sample settings. Therefore, the cluster graphical lasso, which involves clustering the features using an alternative to single linkage clustering, and then performing the graphical lasso on the subset of variables within each cluster, is proposed. Model selection consistency for this technique is established, and its improved perfor-mance relative to the graphical lasso is demonstrated in a simulation study, as well as in applications to a university webpage and a gene expression data sets.},
annote = {General idea: 
- we know by the KKT conditions that we can divide a Glasso problem into sub-problems if the absolute value of correlations across components is smaller than the regularisation parameter
- this would suggest that the Glasso first performs Single Linkage Clustering (SLC) and then runs a constrained optimization problem on each cluster
- SLC may not be the idea form of clustering to emply. Instead, we can use any arbitrary clustering method on the sample covariance matrix and then run Glasso on each cluster.
- this allows for the clustering and penalised optimization steps to be uncoupled; potentially to obtained better results.},
author = {Tan, Kean Ming and Witten, Daniela and Shojaie, Ali},
doi = {10.1016/j.csda.2014.11.015},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Tan, Witten, Shojaie - 2015 - Computational Statistics and Data Analysis The cluster graphical lasso for improved estimation of Gaussian.pdf:pdf},
journal = {Comput. Stat. Data Anal.},
keywords = {Hierarchical clustering,High-dimensional setting,Network,Single linkage clustering,Sparsity},
pages = {23--36},
title = {{The cluster graphical lasso for improved estimation of Gaussian graphical models}},
url = {www.elsevier.com/locate/csda},
volume = {85},
year = {2015}
}
@article{Han2014,
abstract = {Statisticians and quantitative neuroscientists have actively promoted the use of independence relationships for investigating brain networks, genomic networks, and other measurement technologies. Estimation of these graphs depends on two steps. First is a feature extraction by summarizing measurements within a parcellation, regional or set definition to create nodes. Secondly, these summaries are then used to create a graph representing relationships of interest. In this manuscript we study the impact of dimension reduction on graphs that describe different notions of relations among a set of random variables. We are particularly interested in undirected graphs that capture the random variables' independence and conditional independence relations. A dimension reduction procedure can be any mapping from high dimensional spaces to low dimensional spaces. We exploit a general framework for modeling the raw data and advocate that in estimating the undirected graphs, any acceptable dimension reduction procedure should be a graph-homotopic mapping, i.e., the graphical structure of the data after dimension reduction should inherit the main characteristics of the graphical structure of the raw data. We show that, in terms of inferring undirected graphs that characterize the conditional independence relations among random variables, many dimension reduction procedures, such as the mean, median, or principal components, cannot be theoretically guaranteed to be a graph-homotopic mapping. The implications of this work are broad. In the most charitable setting for researchers, where the correct node definition is known, graphical relationships can be contaminated merely via the dimension reduction. The manuscript ends with a concrete example, characterizing a subset of graphical structures such that the dimension reduction procedure using the principal components can be a graph-homotopic mapping.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.7547v2},
author = {Han, Fang and Qiu, Huitong and Liu, Han and Caffo, Brian},
eprint = {arXiv:1404.7547v2},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Han et al. - 2014 - On the Impact of Dimension Reduction on Graphical Structures.pdf:pdf},
keywords = {baltimore,department of biostatistics,dimension reduction,e-mail,edu,fhan,gaussian graphical model,graph-homotopic mapping,hqiu,jhu,johns hopkins university,md 21205,usa},
pages = {1--19},
title = {{On the Impact of Dimension Reduction on Graphical Structures}},
url = {https://arxiv.org/pdf/1404.7547.pdf},
year = {2014}
}
@article{Ding,
abstract = {Current nonnegative matrix factorization (NMF) deals with X = F GT type. We provide a systematic analysis and extensions of NMF to the symmetric W = HHT , and the weighted W = HSHT . We show that (1) W = HHT is equivalent to Kernel K-means clustering and the Laplacian-based spectral clustering. (2) X = F GT is equivalent to simultaneous clustering of rows and columns of a bipartite graph. Algorithms are given for computing these symmetric NMFs.},
author = {Ding, Chris and He, Xiaofeng and Simon, Horst D},
doi = {10.1137/1.9781611972757.70},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ding, He, Simon - Unknown - On the Equivalence of Nonnegative Matrix Factorization and Spectral Clustering(2).pdf:pdf},
journal = {Proc. fifth SIAM Int. Conf. Data Min.},
number = {4},
pages = {606--610},
title = {{On the Equivalence of Nonnegative Matrix Factorization and Spectral Clustering}},
url = {http://ranger.uta.edu/{~}chqding/papers/NMF-SDM2005.pdf http://www.siam.org/proceedings/datamining/2005/dm05.php},
year = {2005}
}
@article{Soltanolkotabi2017,
abstract = {In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form {\$}max(0,{\textless}w,x{\textgreater}){\$} with {\$}w{\$} denoting the weight vector. We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d.{\~{}}from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent, when initialization at 0, converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures.},
archivePrefix = {arXiv},
arxivId = {1705.04591},
author = {Soltanolkotabi, Mahdi},
eprint = {1705.04591},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Soltanolkotabi, Hsieh - 2017 - Learning ReLUs via Gradient Descent.pdf:pdf},
journal = {NIPS},
title = {{Learning ReLUs via Gradient Descent}},
url = {http://www-bcf.usc.edu/{~}soltanol/myrelu.pdf http://arxiv.org/abs/1705.04591},
year = {2017}
}


@article{Wasserstein2017,
abstract = {Additional reading: http://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503},
author = {Wasserstein, Ronald L and Lazar, Nicole A},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Wasserstein, Lazar - 2017 - The ASA's Statement on p-Values Context, Process, and Purpose.pdf:pdf},
isbn = {0003-1305 1537-2731},
journal = {Am. Stat.},
number = {2},
pages = {129--133},
title = {{The ASA's Statement on p -Values: Context, Process, and Purpose}},
volume = {70},
year = {2016}
}

@article{Bird2008,
abstract = {The hippocampus appears to be crucial for long-term episodic memory, yet its precise role remains elusive. Electrophysiological studies in rodents offer a useful starting point for developing models of hippocampal processing in the spatial domain. Here we review one such model that points to an essential role for the hippocampus in the construction of mental images. We explain how this neural-level mechanistic account addresses some of the current controversies in the field, such as the role of the hippocampus in imagery and short-term memory, and discuss its broader implications for the neural bases of episodic memory.},
author = {Bird, Chris M. and Burgess, Neil},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bird, Burgess - 2008 - The hippocampus and memory Insights from spatial processing.pdf:pdf},
journal = {Nat. Rev. Neurosci.},
number = {3},
pages = {182--194},
pmid = {18270514},
title = {{The hippocampus and memory: Insights from spatial processing}},
volume = {9},
year = {2008}
}

@article{Zhang2013,
abstract = {Let X denote the feature and Y the tar-get. We consider domain adaptation under three possible scenarios: (1) the marginal P Y changes, while the conditional P X|Y stays the same (target shift), (2) the marginal P Y is fixed, while the conditional P X|Y changes with certain constraints (conditional shift), and (3) the marginal P Y changes, and the conditional P X|Y changes with constraints (generalized target shift). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweight-ing or sample transformation to find the learning machine that works well on test data, and propose to estimate the weights or transformations by reweighting or trans-forming training data to reproduce the covari-ate distribution on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed ap-proaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and real-world data sets demonstrate the effectiveness of the proposed framework.},
author = {Zhang, Kun and Sch{\"{o}}lkopf, Bernhard and Muandet, Krikamol and Wang, Zhikun},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Sch{\"{o}}lkopf, Wang - 2013 - Domain Adaptation under Target and Conditional Shift.pdf:pdf},
issn = {1938-7228},
journal = {Proc. 30th Int. Conf. Mach. Learn.},
pages = {819--827},
title = {{Domain adaptation under target and conditional shift}},
volume = {28},
year = {2013}
}

@article{Radford,
abstract = {Many tasks in artificial intelligence require the collaboration of multiple agents. We exam deep reinforcement learning for multi-agent domains. Recent research efforts often take the form of two seemingly conflicting perspectives, the decentralized perspective, where each agent is supposed to have its own controller; and the centralized perspective, where one assumes there is a larger model controlling all agents. In this regard, we revisit the idea of the master-slave architecture by incorporating both perspectives within one framework. Such a hierarchical structure naturally leverages advantages from one another. The idea of combining both perspectives is intuitive and can be well motivated from many real world systems, however, out of a variety of possible realizations, we highlights three key ingredients, i.e. composed action representation, learnable communication and independent reasoning. With network designs to facilitate these explicitly, our proposal consistently outperforms latest competing methods both in synthetic experiments and when applied to challenging StarCraft micromanagement tasks.},
archivePrefix = {arXiv},
arxivId = {1712.07305},
author = {Radford, Alec and Metz, Luke and Soumith, Chintala},
doi = {10.1051/0004-6361/201527329},
eprint = {1712.07305},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Radford, Metz, Soumith - 2017 - Unsupervised representation learning with deep convolutional generative adversarial networks.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {ICLR},
pages = {1--16},
pmid = {23459267},
title = {{Unsupervised representation learning with deep convolutional generative adversarial networks}},
url = {https://arxiv.org/pdf/1511.06434.pdf http://arxiv.org/abs/1712.07305},
year = {2017}
}
@article{ZhangKUNZHANG2008,
abstract = {Distinguishing causes from effects is an important problem in many areas. In this paper, we propose a very general but well defined nonlinear acyclic causal model, namely, post-nonlinear acyclic causal model with inner additive noise, to tackle this problem. In this model, each ob-served variable is generated by a nonlinear function of its parents, with additive noise, followed by a nonlinear distortion. The nonlinearity in the second stage takes into account the effect of sensor distortions, which are usually encountered in practice. In the two-variable case, if all the nonlinearities involved in the model are invertible, by relating the proposed model to the post-nonlinear independent component analysis (ICA) problem, we give the conditions under which the causal relation can be uniquely found. We present a two-step method, which is constrained nonlinear ICA followed by statistical independence tests, to distinguish the cause from the ef-fect in the two-variable case. We apply this method to solve the problem " CauseEffectPairs" in the Pot-luck challenge, and successfully identify causes from effects.},
author = {Zhang, Kun and Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang KUNZHANG, Hyv{\"{a}}rinen AAPOHYVARINEN - 2008 - Distinguishing Causes from Effects using Nonlinear Acyclic Causal Models(3).pdf:pdf},
journal = {NIPS 2008 Work. Causality. URL http//www {\ldots}},
keywords = {additive noise,causal discovery,independence tests,nent analysis,nonlinear independent compo-,sensor distortion},
pages = {157--164},
title = {{Distinguishing Causes from Effects using Nonlinear Acyclic Causal Models}},
url = {http://proceedings.mlr.press/v6/zhang10a/zhang10a.pdf http://jmlr.org/proceedings/papers/v6/zhang10a/zhang10a.pdf},
volume = {6},
year = {2009}
}
@article{Liu2011,
abstract = {The ultimate proof of our understanding of natural or technological systems is reflected in our ability to control them. Although control theory offers mathematical tools for steering engineered and natural systems towards a desired state, a framework to control complex self-organized systems is lacking. Here we develop analytical tools to study the controllability of an arbitrary complex directed network, identifying the set of driver nodes with time-dependent control that can guide the system's entire dynamics. We apply these tools to several real networks, finding that the number of driver nodes is determined mainly by the network's degree distribution. We show that sparse inhomogeneous networks, which emerge in many real complex systems, are the most difficult to control, but that dense and homogeneous networks can be controlled using a few driver nodes. Counterintuitively, we find that in both model and real systems the driver nodes tend to avoid the high-degree nodes.},
archivePrefix = {arXiv},
arxivId = {http://www.nature.com/nature/journal/v473/n7346/abs/10.1038-nature10011-unlocked.html{\#}supplementary-information},
author = {Liu, Yang Yu and Slotine, Jean Jacques and Barab{\'{a}}si, Albert L{\'{a}}szl{\'{o}}},
doi = {10.1038/nature10011},
eprint = {/www.nature.com/nature/journal/v473/n7346/abs/10.1038-nature10011-unlocked.html{\#}supplementary-information},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Slotine, Barab{\'{a}}si - 2011 - Controllability of complex networks.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {00280836},
journal = {Nature},
number = {7346},
pages = {167--173},
pmid = {21562557},
primaryClass = {http:},
title = {{Controllability of complex networks}},
url = {https://www.nature.com/articles/nature10011.pdf},
volume = {473},
year = {2011}
}
@article{Kao2013,
abstract = {We consider the problem of learning a linear factor model. We propose a regularized form of principal component analysis (PCA) and demonstrate through experiments with synthetic and real data the superiority of resulting estimates to those produced by pre-existing factor analysis approaches. We also establish theoretical results that explain how our algorithm corrects the biases induced by conventional approaches. An important feature of our algorithm is that its computational requirements are similar to those of PCA, which enjoys wide use in large part due to its efficiency.},
archivePrefix = {arXiv},
arxivId = {1111.6201},
author = {Kao, Yi Hao and {Van Roy}, Benjamin},
doi = {10.1007/s10994-013-5345-8},
eprint = {1111.6201},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kao, Roy - 2013 - Learning a factor model via regularized PCA.pdf:pdf},
issn = {08856125},
journal = {Mach. Learn.},
keywords = {Covariance matrix estimation,Factor model,High-dimensional data,Principal component analysis,Regularization},
number = {3},
pages = {279--303},
title = {{Learning a factor model via regularized PCA}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs10994-013-5345-8.pdf},
volume = {91},
year = {2013}
}
@article{Cooper,
abstract = {This paper describes a Bayesian method for combining an arbitrary mixture of observational and experimental data in order to learn causal Bayesian networks. Observational data are passively observed. Experimental data, such as that produced by randomized controlled trials, result from the experimenter manipulating one or more variables (typically randomly) and observing the states of other variables. The paper presents a Bayesian method for learning the causal structure and parameters of the underlying causal process that is generating the data, given that (1) the data contains a mixture of observational and experimental case records, and (2) the causal process is modeled as a causal Bayesian network. This learning method was applied using as input various mixtures of experimental and observational data that were generated from the ALARM causal Bayesian network. In these experiments, the absolute and relative quantities of experimental and observational data were va...},
archivePrefix = {arXiv},
arxivId = {1301.6686},
author = {Cooper, G and Yoo, C},
doi = {citeulike-article-id:3839452},
eprint = {1301.6686},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Cooper - Unknown - Causal Discovery of Experimental and.pdf:pdf},
isbn = {1-55860-614-9},
journal = {Proc. Fifthteenth Conf. Uncertain. Artif. Intell. {\{}{\{}{\}}(UAI'99){\{}{\}}{\}}},
keywords = {active},
pages = {116--125},
title = {{Causal discovery from a mixture of experimental and observational data}},
url = {https://arxiv.org/pdf/1301.6686.pdf},
year = {1999}
}
@article{Kilbertus2017,
abstract = {Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively. Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from "What is the right fairness criterion?" to "What do we want to assume about the causal data generating process?" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.},
archivePrefix = {arXiv},
arxivId = {1706.02744},
author = {Kilbertus, Niki and Rojas-Carulla, Mateo and Parascandolo, Giambattista and Hardt, Moritz and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
eprint = {1706.02744},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kilbertus et al. - Unknown - Avoiding Discrimination through Causal Reasoning.pdf:pdf},
issn = {10495258},
journal = {Neural Inf. Process. Syst.},
title = {{Avoiding Discrimination through Causal Reasoning}},
url = {https://papers.nips.cc/paper/6668-avoiding-discrimination-through-causal-reasoning.pdf http://arxiv.org/abs/1706.02744},
year = {2017}
}
@article{Hyvarinen2007,
abstract = {Many probabilistic models are only defined up to a normalization constant. This makes maximum likelihood estimation of the model parameters very difficult. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Previously, a method called score matching was proposed for computationally efficient yet (locally) consistent estimation of such models. The basic form of score matching is valid, however, only for models which define a differentiable probability density function over Rn. Therefore, some extensions of the framework are proposed. First, a related method for binary variables is proposed. Second, it is shown how to estimate non-normalized models defined in the non-negative real domain, i.e. R+n. As a further result, it is shown that the score matching estimator can be obtained in closed form for some exponential families. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Downloads/extensions to SM.pdf:pdf},
journal = {Comput. Stat. Data Anal.},
keywords = {Markov Chain Monte Carlo,Non-normalized models,Partition function,Score matching,Statistical estimation},
number = {5},
pages = {2499--2512},
publisher = {North-Holland},
title = {{Some extensions of score matching}},
volume = {51},
year = {2007}
}
@article{Lipton2017,
abstract = {Calls to arms to build interpretable models express a well-founded discomfort with machine learning. Should a software agent that does not even know what a loan is decide who qualifies for one? Indeed, we ought to be cautious about injecting machine learning (or anything else, for that matter) into applications where there may be a significant risk of causing social harm. However, claims that stakeholders "just won't accept that!" do not provide a sufficient foundation for a proposed field of study. For the field of interpretable machine learning to advance, we must ask the following questions: What precisely won't various stakeholders accept? What do they want? Are these desiderata reasonable? Are they feasible? In order to answer these questions, we'll have to give real-world problems and their respective stakeholders greater consideration.},
archivePrefix = {arXiv},
arxivId = {1711.08037},
author = {Lipton, Zachary C},
eprint = {1711.08037},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lipton - 2017 - The Doctor Just Won't Accept That!.pdf:pdf},
title = {{The Doctor Just Won't Accept That!}},
url = {https://arxiv.org/pdf/1711.08037.pdf http://arxiv.org/abs/1711.08037},
year = {2017}
}
@article{Janzing2015a,
abstract = {It has become increasingly popular to obtain machine learning labels through commercial crowdsourcing services. The crowdsourcing workers or annotators are paid for each label they provide, but the task requester usually has only a limited amount of the budget. Since the data instances have different levels of labeling difficulty and the workers have different reliability for the labeling task, it is desirable to wisely allocate the budget among all the instances and workers such that the overall labeling quality is maximized. In this paper, we formulate the budget allocation problem as a Bayesian Markov decision process (MDP), which simultaneously conducts learning and decision making. The optimal allocation policy can be obtained by using the dynamic programming (DP) recurrence. However, DP quickly becomes computationally intractable when the size of the problem increases. To solve this challenge, we propose a computationally efficient approximate policy which is called optimistic knowledge gradient. Our method applies to both pull crowdsourcing marketplaces with homogeneous workers and push marketplaces with heterogeneous workers. It can also incorporate the contextual information of instances when they are available. The experiments on both simulated and real data show that our policy achieves a higher labeling quality than other existing policies at the same budget level.},
author = {Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
keywords = {anticausal learning,causality,independence of cause and mechanism,information geometry,semi-supervised learning},
pages = {1923--1948},
title = {{Semi-Supervised Interpolation in an Anticausal Learning Scenario}},
url = {http://www.jmlr.org/papers/volume16/janzing15a/janzing15a.pdf http://jmlr.org/papers/v16/janzing15a.html},
volume = {16},
year = {2015}
}
@article{Volfovsky2015,
abstract = {Many outcomes of interest in the social and health sciences, as well as in modern applications in computational social science and experimentation on social media platforms, are ordinal and do not have a meaningful scale. Causal analyses that leverage this type of data, termed ordinal non-numeric, require careful treatment, as much of the classical potential outcomes literature is concerned with estimation and hypothesis testing for outcomes whose relative magnitudes are well defined. Here, we propose a class of finite population causal estimands that depend on conditional distributions of the potential outcomes, and provide an interpretable summary of causal effects when no scale is available. We formulate a relaxation of the Fisherian sharp null hypothesis of constant effect that accommodates the scale-free nature of ordinal non-numeric data. We develop a Bayesian procedure to estimate the proposed causal estimands that leverages the rank likelihood. We illustrate these methods with an application to educational outcomes in the General Social Survey.},
archivePrefix = {arXiv},
arxivId = {1501.01234},
author = {Volfovsky, Alexander and Airoldi, Edoardo M and Rubin, Donald B},
eprint = {1501.01234},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Volfovsky, Airoldi, Rubin - 2015 - Causal inference for ordinal outcomes.pdf:pdf},
keywords = {Fisher's exact test,Ordinal non-numeric data,estimation of causal effects,potential outcomes,rank likelihood},
pages = {1--33},
title = {{Causal inference for ordinal outcomes}},
url = {https://arxiv.org/pdf/1501.01234.pdf http://arxiv.org/abs/1501.01234},
year = {2015}
}
@article{Gutmann2012,
abstract = {We consider the task of estimating, from observed data, a probabilistic model that is parameterized by a finite number of parameters. In particular, we are considering the situation where the model probability density function is unnormalized. That is, themodel is only specified up to the partition function. The partition function normalizes a model so that it integrates to one for any choice of the parameters. However, it is often impossible to obtain it in closed form. Gibbs distributions, Markov and multi-layer networks are examples of models where analytical normalization is often impossible. Maximum likelihood estimation can then not be used without resorting to numerical approximations which are often computationally expensive. We propose here a new objective func- tion for the estimation of both normalized and unnormalized models. The basic idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially gener- ated noise. With this approach, the normalizing partition function can be estimated like any other parameter. We prove that the new estimation method leads to a consistent (convergent) estimator of the parameters. For large noise sample sizes, the new estimator is furthermore shown to be- have like the maximum likelihood estimator. In the estimation of unnormalized models, there is a trade-off between statistical and computational performance. We show that the new method strikes a competitive trade-off in comparison to other estimationmethods for unnormalizedmodels. As an application to real data, we estimate novel two-layer models of natural image statistics with spline nonlinearities.},
author = {Gutmann, Michael and Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gutmann, Hyvarinen - 2012 - Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statist.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {computation,estimation,natural image statistics,partition function,unnormalized models},
pages = {307--361},
title = {{Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics}},
volume = {13},
year = {2012}
}
@article{Zhanga,
abstract = {Conditional independence testing is an important problem, especially in Bayesian network learning and causal discovery. Due to the curse of dimensionality, testing for conditional independence of continuous variables is particularly challenging. We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence. The proposed method is computationally efficient and easy to implement. Experimental results show that it outperforms other methods, especially when the conditioning set is large or the sample size is not very large, in which case other methods encounter difficulties.},
archivePrefix = {arXiv},
arxivId = {1202.3775},
author = {Zhang, Kun and Peters, Jonas and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
eprint = {1202.3775},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Kernel-based Conditional Independence Test and Application in Causal Discovery(2).pdf:pdf},
isbn = {978-0-9749039-7-2},
title = {{Kernel-based Conditional Independence Test and Application in Causal Discovery}},
url = {https://arxiv.org/pdf/1202.3775.pdf http://arxiv.org/abs/1202.3775},
year = {2012}
}
@article{Monti2017b,
abstract = {In neuroimaging data analysis, Gaussian graphical models are often used to model statistical dependencies across spatially remote brain regions known as functional connectivity. Typically, data is collected across a cohort of subjects and the scientific objectives consist of estimating population and subject-specific graphical models. A third objective that is often overlooked involves quantifying inter-subject variability and thus identifying regions or sub-networks that demonstrate heterogeneity across subjects. Such information is fundamental in order to thoroughly understand the human connectome. We propose Mixed Neighborhood Selection in order to simultaneously address the three aforementioned objectives. By recasting covariance selection as a neighborhood selection problem we are able to efficiently learn the topology of each node. We introduce an additional mixed effect component to neighborhood selection in order to simultaneously estimate a graphical model for the population of subjects as well as for each individual subject. The proposed method is validated empirically through a series of simulations and applied to resting state data for healthy subjects taken from the ABIDE consortium.},
archivePrefix = {arXiv},
arxivId = {1512.01947},
author = {Monti, Ricardo Pio and Anagnostopoulos, Christoforos and Montana, Giovanni},
doi = {10.1214/17-AOAS1067},
eprint = {1512.01947},
issn = {19417330},
journal = {Ann. Appl. Stat.},
keywords = {Functional connectivity,Graphical models,Inter-subject variability,Neuroimaging},
number = {4},
pages = {2142--2164},
title = {{Learning Population and subject-specific brain connectivity networks via mixed neighborhood selection}},
volume = {11},
year = {2017}
}
@article{Shahriari2016,
abstract = {—Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., rec-ommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable config-uration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {De Freitas}, Nando},
doi = {10.1109/JPROC.2015.2494218},
eprint = {arXiv:1011.1669v3},
file = {:Users/ricardo/Downloads/human out of loop.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proc. IEEE},
keywords = {decision making,design of experiments,genomic medicine,optimization,response surface methodology,statistical learning},
month = {jan},
number = {1},
pages = {148--175},
pmid = {25246403},
title = {{Taking the human out of the loop: A review of Bayesian optimization}},
url = {http://ieeexplore.ieee.org/document/7352306/},
volume = {104},
year = {2016}
}
@article{Varian,
abstract = {This is an elementary introduction to causal inference in economics written for readers familiar with machine learning methods. The critical step in any causal analysis is estimating the counterfactual—a prediction of what would have happened in the absence of the treatment. The powerful techniques used in machine learning may be useful for developing better estimates of the counterfactual, po-tentially improving causal inference.},
author = {Varian, Hal R},
doi = {10.1073/pnas.1510479113},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Varian - Unknown - Causal inference in economics and marketing.pdf:pdf},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci.},
number = {27},
pages = {7310--7315},
pmid = {27382144},
title = {{Causal inference in economics and marketing}},
url = {www.pnas.org/cgi/doi/10.1073/pnas.1510479113 http://www.pnas.org/lookup/doi/10.1073/pnas.1510479113},
volume = {113},
year = {2016}
}
@article{Asteris2014,
abstract = {We introduce a novel algorithm to compute nonnegative sparse principal components of positive semidefinite (PSD) matrices. Our algorithm comes with approximation guarantees contingent on the spectral profile of the input matrix A: the sharper the eigenvalue decay, the better the approximation quality. If the eigenvalues decay like any asymptotically vanishing function, we can approximate nonnegative sparse PCA within any accuracy $\epsilon$in time polynomial in the matrix size n and desired sparsity k, but not in 1/$\epsilon$. Further, we obtain a data-dependent bound that is computed by executing an algorithm on a given data set. This bound is significantly tighter than a-priori bounds and can be used to show that for all tested datasets our algorithm is provably within 40{\%}-90{\%} from the unknown optimum. Our algorithm is combinatorial and explores a subspace defined by the leading eigenvectors of A. We test our scheme on several data sets, showing that it matches or outperforms the previous state of the art.},
author = {Asteris, Megasthenis and Papailiopoulos, Dimitris and Dimakis, Alexandros},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Asteris, Papailiopoulos, Dimakis - Unknown - Nonnegative Sparse PCA with Provable Guarantees.pdf:pdf},
journal = {ICML},
number = {2},
pages = {1728--1736},
title = {{Nonnegative Sparse PCA with Provable Guarantees}},
volume = {32},
year = {2014}
}
@article{Chen2010,
abstract = {We address covariance estimation in the sense of minimum mean-squared error (MMSE) for Gaussian samples. Specifically, we consider shrinkage methods which are suitable for high dimensional problems with a small number of samples (large p small n). First, we improve on the Ledoit-Wolf (LW) method by conditioning on a sufficient statistic. By the Rao-Blackwell theorem, this yields a new estimator called RBLW, whose mean-squared error dominates that of LW for Gaussian variables. Second, to further reduce the estimation error, we propose an iterative approach which approximates the clairvoyant shrinkage estimator. Convergence of this iterative method is established and a closed form expression for the limit is determined, which is referred to as the oracle approximating shrinkage (OAS) estimator. Both RBLW and OAS estimators have simple expressions and are easily implemented. Although the two methods are developed from different persepctives, their structure is identical up to specified constants. The RBLW estimator provably dominates the LW method. Numerical simulations demonstrate that the OAS approach can perform even better than RBLW, especially when n is much less than p. We also demonstrate the performance of these techniques in the context of adaptive beamforming.},
author = {Chen, Yilun and Wiesel, Ami and Eldar, Yonina C. and Hero, Alfred O.},
file = {:Users/ricardo/Downloads/MMSE covariance.pdf:pdf},
journal = {IEEE Trans. Signal Process.},
keywords = {Beamforming,covariance estimation,minimum mean-squared error (MMSE),shrinkage},
number = {10},
pages = {5016--5029},
title = {{Shrinkage algorithms for MMSE covariance estimation}},
volume = {58},
year = {2010}
}
@article{Hoyer2009,
abstract = {The discovery of causal relationships between a set of observed variables is a fun-damental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well under-stood and there are well-known methods to fit them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underly-ing causal system and allow more aspects of the true data-generating mechanisms to be identified. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identification power provided by non-linearities.},
author = {Hoyer, Patrik O and Janzing, Dominik and Mooij, Joris M. and Peters, Jonas and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hoyer et al. - Unknown - Nonlinear causal discovery with additive noise models.pdf:pdf},
journal = {Neural Inf. Process. Syst.},
pages = {689--696},
title = {{Nonlinear causal discovery with additive noise models}},
year = {2009}
}
@article{Friedman2008,
abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm--the graphical lasso--that is remarkably fast: It solves a 1000-node problem ( approximately 500,000 parameters) in at most a minute and is 30-4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and B{\"{u}}hlmann (2006). We illustrate the method on some cell-signaling data from proteomics.},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Friedman, Hastie, Tibshirani - 2008 - Sparse inverse covariance estimation with the graphical lasso.pdf:pdf},
journal = {Biostatistics},
keywords = {Gaussian covariance,Graphical model,L1,Lasso},
number = {3},
pages = {432--441},
title = {{Sparse inverse covariance estimation with the graphical lasso}},
volume = {9},
year = {2008}
}
@article{Molenaar2016,
abstract = {Most connectivity mapping techniques for neuroimaging data assume stationarity (i.e., network parameters are constant across time), but this assumption does not always hold true. The authors provide a description of a new approach for simultaneously detecting time-varying (or dynamic) contemporaneous and lagged relations in brain connectivity maps. Specifically, they use a novel raw data likelihood estimation technique (involving a second-order extended Kalman filter/smoother embedded in a nonlinear optimizer) to determine the variances of the random walks associated with state space model parameters and their autoregressive components. The authors illustrate their approach with simulated and blood oxygen level-dependent functional magnetic resonance imaging data from 30 daily cigarette smokers performing a verbal working memory task, focusing on seven regions of interest (ROIs). Twelve participants had dynamic directed functional connectivity maps: Eleven had one or more time-varying contemporaneous ROI state loadings, and one had a time-varying autoregressive parameter. Compared to smokers without dynamic maps, smokers with dynamic maps performed the task with greater accuracy. Thus, accurate detection of dynamic brain processes is meaningfully related to behavior in a clinical sample.},
author = {Molenaar, Peter C.M. and Beltz, Adriene M. and Gates, Kathleen M. and Wilson, Stephen J.},
doi = {10.1016/j.neuroimage.2015.10.088},
file = {:Users/ricardo/Downloads/1-s2.0-S1053811915010216-main.pdf:pdf},
issn = {10959572},
journal = {Neuroimage},
keywords = {Cigarette smoking,Functional connectivity,Neural networks,Stationarity,Time-varying,Verbal working memory},
month = {jan},
pages = {791--802},
pmid = {26546863},
publisher = {Academic Press},
title = {{State space modeling of time-varying contemporaneous and lagged relations in connectivity maps}},
url = {http://www.sciencedirect.com/science/article/pii/S1053811915010216},
volume = {125},
year = {2016}
}
@article{Thirion2014,
abstract = {Analysis and interpretation of neuroimaging data often require one to divide the brain into a number of regions, or parcels, with homogeneous characteristics, be these regions defined in the brain volume or on on the cortical surface. While predefined brain atlases do not adapt to the signal in the individual subjects images, parcellation approaches use brain activity (e.g. found in some functional contrasts of interest) and clustering techniques to define regions with some degree of signal homogeneity. In this work, we address the question of which clustering technique is appropriate and how to optimize the corresponding model. We use two principled criteria: goodness of fit (accuracy), and reproducibility of the parcellation across bootstrap samples. We study these criteria on both simulated and two task-based functional Magnetic Resonance Imaging datasets for the Ward, spectral and K-means clustering algorithms. We show that in general Ward's clustering performs better than alternative methods with regards to reproducibility and accuracy and that the two criteria diverge regarding the preferred models (reproducibility leading to more conservative solutions), thus deferring the practical decision to a higher level alternative, namely the choice of a trade-off between accuracy and stability.},
author = {Thirion, Bertrand and Varoquaux, Ga{\"{e}}l and Dohmatob, Elvis and Poline, Jean Baptiste},
doi = {10.3389/fnins.2014.00167},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Thirion et al. - 2014 - Which fMRI clustering gives good brain parcellations(2).pdf:pdf},
isbn = {10.3389/fnins.2014.00167},
issn = {1662453X},
journal = {Front. Neurosci.},
keywords = {Brain atlas,Clustering,Cross-validation,Functional neuroimaging,Group studies,Model selection},
number = {8 JUL},
pmid = {25071425},
title = {{Which fMRI clustering gives good brain parcellations?}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4076743/pdf/fnins-08-00167.pdf},
year = {2014}
}
@article{Peters2013,
abstract = {We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional ex-periments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the di-rected acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (regression with sub-sequent independence test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation.},
author = {Peters, Jonas and Mooij, J and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Peters et al. - Unknown - Causal Discovery with Continuous Additive Noise Models.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {additive noise,bayesian networks,causal inference,causal minimality,identifiability,structural equation models},
pages = {2009--2053},
title = {{Causal discovery with continuous additive noise models}},
volume = {15},
year = {2014}
}
@article{Forbes2015,
abstract = {In many families of distributions, maximum likelihood estimation is intractable because the normalization constant for the density which enters into the likelihood function is not easily available. The score matching estimator [35] provides an alternative where this normalization constant is not required. For an exponential family, e.g. a Gaussian linear concentration model, the corresponding estimating equations become linear [2,36] and the score matching estimator is shown to be consistent and asymptotically normally distributed as the number of observations increase to infinity, although not necessarily efficient. For linear concentration models that are also linear in the covariance [37] we show that the score matching estimator is identical to the maximum likelihood estimator, hence in such cases it is also efficient. Gaussian graphical models and graphical models with symmetries [32] form particularly interesting subclasses of linear concentration models and we investigate the potential use of the score matching estimator for this case.},
archivePrefix = {arXiv},
arxivId = {1311.0662},
author = {Forbes, Peter G.M. and Lauritzen, Steffen},
doi = {10.1016/j.laa.2014.08.015},
eprint = {1311.0662},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Forbes, Lauritzen - 2015 - Linear estimating equations for exponential families with application to Gaussian linear concentration models.pdf:pdf},
isbn = {0024-3795},
issn = {00243795},
journal = {Linear Algebra Appl.},
keywords = {Gaussian graphical models,Jordan algebra models,Score matching,Scoring rules,Symmetry},
number = {473},
pages = {261--283},
title = {{Linear estimating equations for exponential families with application to Gaussian linear concentration models}},
url = {www.elsevier.com/locate/laa http://dx.doi.org/10.1016/j.laa.2014.08.015},
volume = {473},
year = {2015}
}
@article{Peters2016,
author = {Peters, Jonas and B{\"{u}}hlmann, Peter and Meinshausen, Nicolai},
file = {:Users/ricardo/Downloads/Peters{\_}et{\_}al-2016-Journal{\_}of{\_}the{\_}Royal{\_}Statistical{\_}Society{\%}3A{\_}Series{\_}B{\_}{\%}28Statistical{\_}Methodology{\%}29 (2).pdf:pdf},
journal = {J. R. Stat. Soc. Ser. B (Statistical Methodology)},
keywords = {causal discovery,causal inference,confidence intervals,invariant prediction},
volume = {78},
pages = {947--1012},
title = {{Causal inference by using invariant prediction: identification and confidence intervals}},
year = {2016}
}
@article{Lin,
abstract = {Nonnegative matrix factorization (NMF) can be formulated as a minimization problem with bound constraints. Although bound-constrained optimization has been studied extensively in both theory and practice, so far no study has formally applied its techniques to NMF. In this letter, we propose two projected gradient methods for NMF, both of which exhibit strong optimization properties. We discuss efficient implementations and demonstrate that one of the proposed methods converges faster than the popular multiplicative update approach. A simple Matlab code is also provided.},
author = {Lin, Chih-Jen},
doi = {10.1162/neco.2007.19.10.2756},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lin - Unknown - Projected Gradient Methods for Nonnegative Matrix Factorization.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Comput.},
number = {10},
pages = {2756--2779},
pmid = {17716011},
title = {{Projected Gradient Methods for Nonnegative Matrix Factorization}},
url = {https://www.mitpressjournals.org/doi/pdf/10.1162/neco.2007.19.10.2756 http://www.mitpressjournals.org/doi/10.1162/neco.2007.19.10.2756},
volume = {19},
year = {2007}
}
@article{Edelman,
abstract = {In this paper we develop new Newton and conjugate gradient algorithms on the Grassmann and Stiefel manifolds. These manifolds represent the constraints that arise in such areas as the symmetric eigenvalue problem, nonlinear eigenvalue problems, electronic structures computations, and signal processing. In addition to the new algorithms, we show how the geometrical framework gives penetrating new insights allowing us to create, understand, and compare algorithms. The theory proposed here provides a taxonomy for numerical linear algebra algorithms that provide a top level mathematical view of previously unrelated algorithms. It is our hope that developers of new algorithms and perturbation theories will benefit from the theory, methods, and examples in this paper. Key words. conjugate gradient, Newton's method, orthogonality constraints, Grassmann man-ifold, Stiefel manifold, eigenvalues and eigenvectors, invariant subspace, Rayleigh quotient iteration, eigenvalue optimization, sequential quadratic programming, reduced gradient method, electronic structures computation, subspace tracking AMS subject classifications. 49M07, 49M15, 53B20, 65F15, 15A18, 51F20, 81V55 PII. S0895479895290954 1. Introduction. Problems on the Stiefel and Grassmann manifolds arise with sufficient frequency that a unifying investigation of algorithms designed to solve these problems is warranted. Understanding these manifolds, which represent orthogonality constraints (as in the symmetric eigenvalue problem), yields penetrating insight into many numerical algorithms and unifies seemingly unrelated ideas from different areas. The optimization community has long recognized that linear and quadratic con-straints have special structure that can be exploited. The Stiefel and Grassmann manifolds also represent special constraints. The main contribution of this paper is a framework for algorithms involving these constraints, which draws upon ideas from numerical linear algebra, optimization, differential geometry, and has been inspired by certain problems posed in engineering, physics, and chemistry. Though we do review the necessary background for our intended audience, this is not a survey paper. This paper uses mathematics as a tool so that we can understand the deeper geometrical structure underlying algorithms. In our first concrete problem we minimize a function F (Y), where Y is constrained to the set of n-by-p matrices such that Y},
author = {Edelman, Alan and Arias, Tomas and Smith, Steven},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Edelman, Arias, Smith - Unknown - The geometry of algorithms with orthogonality constraints.pdf:pdf},
journal = {SIAM},
number = {2},
pages = {303--353},
title = {{The geometry of algorithms with orthogonality constraints}},
url = {http://epubs.siam.org/doi/pdf/10.1137/S0895479895290954},
volume = {20}
}
@misc{Barak2017,
abstract = {Recurrent neural networks (RNNs) are a class of computational models that are often used as a tool to explain neurobiological phenomena, considering anatomical, electrophysiological and computational constraints. RNNs can either be designed to implement a certain dynamical principle, or they can be trained by input–output examples. Recently, there has been large progress in utilizing trained RNNs both for computational tasks, and as explanations of neural phenomena. I will review how combining trained RNNs with reverse engineering can provide an alternative framework for modeling in neuroscience, potentially serving as a powerful hypothesis generation tool. Despite the recent progress and potential benefits, there are many fundamental gaps towards a theory of these networks. I will discuss these challenges and possible methods to attack them.},
author = {Barak, Omri},
booktitle = {Curr. Opin. Neurobiol.},
doi = {10.1016/j.conb.2017.06.003},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Barak - 2017 - Recurrent neural networks as versatile tools of neuroscience research.pdf:pdf},
issn = {18736882},
pages = {1--6},
pmid = {28668365},
title = {{Recurrent neural networks as versatile tools of neuroscience research}},
url = {http://dx.doi.org/10.1016/j.conb.2017.06.003},
volume = {46},
year = {2017}
}
@misc{Varoquaux2017,
abstract = {Predictive models ground many state-of-the-art developments in statistical brain image analysis: decoding, MVPA, searchlight, or extraction of biomarkers. The principled approach to establish their validity and usefulness is cross-validation, testing prediction on unseen data. Here, I would like to raise awareness on error bars of cross-validation, which are often underestimated. Simple experiments show that sample sizes of many neuroimaging studies inherently lead to large error bars, eg ±10{\%} for 100 samples. The standard error across folds strongly underestimates them. These large error bars compromise the reliability of conclusions drawn with predictive models, such as biomarkers or methods developments where, unlike with cognitive neuroimaging MVPA approaches, more samples cannot be acquired by repeating the experiment across many subjects. Solutions to increase sample size must be investigated, tackling possible increases in heterogeneity of the data.},
archivePrefix = {arXiv},
arxivId = {1706.07581},
author = {Varoquaux, Ga{\"{e}}l},
booktitle = {Neuroimage},
doi = {10.1016/j.neuroimage.2017.06.061},
eprint = {1706.07581},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/El Varoquaux - 2017 - Cross-validation failure Small sample sizes lead to large error bars.pdf:pdf},
issn = {10959572},
keywords = {Biomarkers,Cross-validation,Decoding,FMRI,MVPA,Model selection,Statistics},
pmid = {28655633},
title = {{Cross-validation failure: Small sample sizes lead to large error bars}},
url = {https://ac.els-cdn.com/S1053811917305311/1-s2.0-S1053811917305311-main.pdf?{\_}tid=7e378e30-0817-11e8-98fd-00000aab0f02{\&}acdnat=1517575943{\_}a36bd2fdea6eb33df8d544af6124233b},
year = {2017}
}
@article{Eberhardt2005,
abstract = {We show that if any number of variables are allowed to be simultaneously and independently randomized in any one experiment, log2(N) + 1 experiments are sufficient and in the worst case necessary to determine the causal relations among N {\textgreater}= 2 variables when no latent variables, no sample selection bias and no feedback cycles are present. For all K, 0 {\textless} K {\textless} 1/(2N) we provide an upper bound on the number experiments required to determine causal structure when each experiment simultaneously randomizes K variables. For large N, these bounds are significantly lower than the N - 1 bound required when each experiment randomizes at most one variable. For kmax {\textless} N/2, we show that (N/kmax-1)+N/(2kmax)log2(kmax) experiments aresufficient and in the worst case necessary. We over a conjecture as to the minimal number of experiments that are in the worst case sufficient to identify all causal relations among N observed variables that are a subset of the vertices of a DAG.},
author = {Eberhardt, Frederick and Glymour, Clark and Scheines, Richard},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Eberhardt, Glymour, Scheines - Unknown - On the Number of Experiments Sufficient and in the Worst Case Necessary to Identify All Causal.pdf:pdf},
isbn = {0-9749039-1-4},
journal = {Proc. Twenty-First Conf. Uncertain. Artif. Intell.},
pages = {178--184},
title = {{On the number of experiments sufficient and in the worst case necessary to identify all causal relations among n variables}},
year = {2005}
}
@article{Anandkumar2013,
abstract = {This work considers the problem of learn-ing linear Bayesian networks when some of the variables are unobserved. Identifiability and efficient recovery from low-order observ-able moments are established under a novel graphical constraint. The constraint con-cerns the expansion properties of the under-lying directed acyclic graph (DAG) between observed and unobserved variables in the net-work, and it is satisfied by many natural fam-ilies of DAGs that include multi-level DAGs, DAGs with effective depth one, as well as cer-tain families of polytrees.},
author = {Anandkumar, Animashree and Hsu, Daniel},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Anandkumar, Hsu - 2013 - Learning linear bayesian networks with latent variables.pdf:pdf},
journal = {ICML},
pages = {1--27},
title = {{Learning linear bayesian networks with latent variables}},
url = {http://proceedings.mlr.press/v28/anandkumar13.pdf http://jmlr.org/proceedings/papers/v28/anandkumar13.html},
volume = {28},
year = {2013}
}
@article{Kawaguchi2017,
abstract = {This paper explains why deep learning can generalize well, despite large capacity and possible algorithmic instability, nonrobustness, and sharp minima, effectively addressing an open problem in the literature. Based on our theoretical insight, this paper also proposes a family of new regularization methods. Its simplest member was empirically shown to improve base models and achieve state-of-the-art performance on MNIST and CIFAR-10 benchmarks. Moreover, this paper presents both data-dependent and data-independent generalization guarantees with improved convergence rates. Our results suggest several new open areas of research.},
archivePrefix = {arXiv},
arxivId = {1710.05468},
author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
eprint = {1710.05468},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kawaguchi, Kaelbling, Bengio - 2017 - Generalization in Deep Learning.pdf:pdf},
title = {{Generalization in Deep Learning}},
url = {https://arxiv.org/pdf/1710.05468.pdf http://arxiv.org/abs/1710.05468},
year = {2017}
}
@article{Meinshausen2010,
abstract = {Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
archivePrefix = {arXiv},
arxivId = {arXiv:0809.2932v2},
author = {Meinshausen, Nicolai},
doi = {10.1111/j.1467-9868.2010.00740.x},
eprint = {arXiv:0809.2932v2},
journal = {J. R. Stat. Soc. Ser. B (Statistical Methodol.},
keywords = {high dimensional data,resampling,stability selection,structure estimation},
pages = {1--30},
publisher = {WileyRoyal Statistical Society},
title = {{Stability selection}},
url = {http://www.jstor.org/stable/40802220},
volume = {72},
year = {2009}
}
@article{Oates2016,
archivePrefix = {arXiv},
arxivId = {1404.1238},
author = {Oates, Chris J. and Smith, Jim Q. and Mukherjee, Sach and Cussens, James},
doi = {10.1007/s11222-015-9570-9},
eprint = {1404.1238},
file = {:Users/ricardo/Downloads/oates{\_}multipleDAGs.pdf:pdf},
isbn = {1122201595709},
issn = {15731375},
journal = {Stat. Comput.},
keywords = {Bayesian network,Hierarchical model,Integer linear programming,Joint estimation,Multiregression dynamical model},
number = {4},
pages = {797--811},
publisher = {Springer US},
title = {{Exact estimation of multiple directed acyclic graphs}},
volume = {26},
year = {2016}
}
@article{Thompson2016,
abstract = {Assessment of dynamic functional brain connectivity (dFC) based on fMRI data is an increasingly popular strategy to investigate temporal dynamics of the brain's large-scale network architecture. Current practice when deriving connectivity estimates over time is to use the Fisher transform which aims to stabilize the variance of correlation values that fluctuate around varying true correlation values. It is however unclear how well the stabilization of signal variance performed by the Fisher transform works for each connectivity time series, when the true correlation is assumed to be fluctuating. This is of importance because many subsequent analyses either assume or perform better when the time series have stable variance or adheres to an approximate Gaussian distribution. In this paper, using simulations and analysis of resting-state fMRI data, we analyze the effect of applying different variance stabilization strategies on connectivity time-series. We here focus our investigation on the Fisher transform, the Box Cox transform and an approach that combines both transforms. Our results show that, if the intention of stabilizing the variance is to use metrics on the time series where stable variance or a Gaussian distribution is desired (e.g. clustering), the Fisher transform is not optimal and may even skew connectivity time series away from being Gaussian. Further, we show that the suboptimal performance of the Fisher transform can be substantially improved by including an additional Box-Cox transformation after the dFC time series has been Fisher transformed.},
archivePrefix = {arXiv},
arxivId = {1603.00201},
author = {Thompson, William Hedley and Fransson, Peter},
doi = {10.1089/brain.2016.0454},
eprint = {1603.00201},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Thompson, Fransson - Unknown - On Stabilizing the Variance of Dynamic Functional Brain Connectivity Time Series.pdf:pdf},
isbn = {2158-0022 (Electronic) 2158-0014 (Linking)},
issn = {2158-0014},
journal = {Brain Connect.},
keywords = {Box–Cox transformation,Fisher transformation,dynamic functional connectivity,fMRI,time series,variance Introduction},
number = {10},
pages = {735--746},
pmid = {27784176},
title = {{On Stabilizing the Variance of Dynamic Functional Brain Connectivity Time Series}},
url = {http://online.liebertpub.com/doi/pdf/10.1089/brain.2016.0454 http://online.liebertpub.com/doi/10.1089/brain.2016.0454},
volume = {6},
year = {2016}
}
@article{Lawrence2017,
abstract = {In this paper we consider the nature of the machine intelligences we have created in the context of our human intelligence. We suggest that the fundamental difference between human and machine intelligence comes down to $\backslash$emph{\{}embodiment factors{\}}. We define embodiment factors as the ratio between an entity's ability to communicate information vs compute information. We speculate on the role of embodiment factors in driving our own intelligence and consciousness. We briefly review dual process models of cognition and cast machine intelligence within that framework, characterising it as a dominant System Zero, which can drive behaviour through interfacing with us subconsciously. Driven by concerns about the consequence of such a system we suggest prophylactic courses of action that could be considered. Our main conclusion is that it is $\backslash$emph{\{}not{\}} sentient intelligence we should fear but $\backslash$emph{\{}non-sentient{\}} intelligence.},
archivePrefix = {arXiv},
arxivId = {1705.07996},
author = {Lawrence, Neil D},
eprint = {1705.07996},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lawrence - 2017 - Living Together Mind and Machine Intelligence.pdf:pdf},
title = {{Living Together: Mind and Machine Intelligence}},
url = {https://arxiv.org/pdf/1705.07996.pdf http://arxiv.org/abs/1705.07996},
year = {2017}
}
@article{Baron-Cohen2000,
abstract = {Brothers (Brothers L. Concepts in Neuroscience 1990;1:27–51) proposed a network of neural regions that comprise the " social brain " , which includes the amygdala. Since the childhood psychiatric condition of autism involves deficits in " social intelligence " , it is plausible that autism may be caused by an amygdala abnormality. In this paper we review the evidence for a social function of the amygdala. This includes reference to the Kluver–Bucy syndrome (which Hetzler and Griffin suggested may serve as an animal model of autism). We then review evidence for an amygdala deficit in people with autism, who are well known to have deficits in social behaviour. This includes a detailed summary of our recent functional magnetic resonance imaging (fMRI) study involving judging from the expressions of another person's eyes what that other person might be thinking or feeling. In this study, patients with autism or AS did not activate the amygdala when making mentalistic inferences from the eyes, whilst people without autism did show amygdala activity. The amygdala is therefore proposed to be one of several neural regions that are abnormal in autism. We conclude that the amygdala theory of autism contains promise and suggest some new lines of research. ᭧},
author = {Baron-Cohen, S and Ring, H A and Bullmore, E T and Wheelwright, S and Ashwin, C and Williams, S C R},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Baron-Cohen et al. - Unknown - The amygdala theory of autism.pdf:pdf},
journal = {Neurosci. Biobehav. Rev.},
pages = {355--364},
title = {{The amygdala theory of autism}},
year = {2000}
}
@article{Allin2017,
abstract = {Attempts to create measures of national wellbeing and progress have a long his-tory. In the UK, they go back at least as far as the 1790s, with Sir John Sinclair's Statistical Account of Scotland. More recently, worldwide interest has led to the creation of various indices seeking to go beyond familiar economic measures like gross domestic product. We review the 'Measuring national well-being' development programme of the UK's Office for National Statis-tics and explore some of the challenges which need to be faced to bring wider measures into use. These include the importance of getting the measures adopted as policy drivers, how to challenge the continuing dominance of economic measures, sustainability and environmental issues, international comparability and methodological statistical questions.},
author = {Allin, Paul and Hand, David J},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Allin, Hand - 2017 - New statistics for old—measuring the wellbeing of the UK(2).pdf:pdf},
journal = {J. R. Stat. Soc. Ser. A},
keywords = {beyond gross domestic product,progress,public policy,quantum of happiness,sustainable development,wellbeing},
number = {1},
pages = {3--43},
title = {{New Statistics for Old? Measuring the wellbeing of the UK}},
url = {http://www.rss.org.uk/Images/PDF/publications/Allin-Hand-June-15.pdf},
volume = {180},
year = {2017}
}

@article{Pham2001a,
abstract = {Most source separation algorithms are based on a model of stationary sources. However, it is a simple matter to take advantage of possible non-stationarities of the sources to achieve separation. This paper develops novel approaches in this direction, based on the principles of maximum likelihood and minimum mutual information. These principles are exploited by efficient algorithms in both the off-line case (via a new joint diagonalization procedure) and in the on-line case (via a Newton-like procedure). Some experiments are presented showing the good performance of our algorithms and evidencing an interesting feature of our methods: their ability to achieve a kind of super-efficiency. The paper concludes with a discussion contrasting separating methods for non-Gaussian and non-stationary models and emphasizing that, as a matter of fact, ‘what makes the algorithms work' is —strictly speaking— not the non stationarity itself but rather the property that each realization of the source signals has a time-varying envelope.},
author = {Pham, Dinh-Tuan and Cardoso, Jean-Francois},
doi = {10.1016/S0165-1684(00)00260-7},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Pham, Cardoso - 2001 - Blind Separation of Instantaneous Mixtures of Non Stationary Sources.pdf:pdf},
isbn = {3376631263},
issn = {1053587X},
journal = {IEEE Trans. Signal Process.},
number = {9},
pages = {1837--1848},
title = {{Blind Separation of Instantaneous Mixtures of Non Stationary Sources}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.6.7604{\&}rep=rep1{\&}type=pdf},
volume = {49},
year = {2001}
}

@article{Hyvarinen1999,
author = {Hyv{\"{a}}rinen, Aapo and Pajunen, Petteri},
journal = {Neural Networks},
keywords = {Blind source separation,Feature extraction,Independent component analysis,Redundancy reduction},
number = {3},
pages = {429--439},
title = {{Nonlinear independent component analysis: Existence and uniqueness results}},
volume = {12},
year = {1999}
}

@article{Hyvarinen1999a,
abstract = {Independent component analysis (ICA) is a statistical method for transforming an observed multidimensional random vector into components that are statistically as independent from each other as possible. We use a combination of two different approaches for linear ICA: Comon's information theoretic approach and the projection pursuit approach. Using maximum entropy approximations of differential entropy, we introduce a family of new contrast functions for ICA. These contrast functions enable both the estimation of the whole decomposition by minimizing mutual information, and estimation of individual independent components as projection pursuit directions. The statistical properties of the estimators based on such contrast functions are analyzed under the assumption of the linear mixture model, and it is shown how to choose contrast functions that are robust and/or of minimum variance. Finally, we introduce simple fixed-point algorithms for practical optimization of the contrast functions},
author = {Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hyvarinen - 1999 - Fast and robust fixed-point algorithm for independent component analysis.pdf:pdf},
isbn = {1045-9227},
journal = {IEEE Trans. Neural Networks},
number = {3},
pages = {626--634},
title = {{Fast and robust fixed-point algorithm for independent component analysis}},
volume = {10},
year = {1999}
}
@article{Baydin2017,
abstract = {We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice. We analyze the effectiveness of the method by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it improves upon these commonly used algorithms on a range of optimization problems; in particular the kinds of objective functions that arise frequently in deep neural network training. Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself. Computing this "hypergradient" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.},
archivePrefix = {arXiv},
arxivId = {1703.04782},
author = {Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
eprint = {1703.04782},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Baydin et al. - Unknown - Online Learning Rate Adaptation with Hypergradient Descent.pdf:pdf},
journal = {ICLR 2018 Submiss.},
title = {{Online Learning Rate Adaptation with Hypergradient Descent}},
url = {https://arxiv.org/pdf/1703.04782.pdf http://arxiv.org/abs/1703.04782},
year = {2017}
}
@article{Belgrave2017,
abstract = {We are facing a major challenge in bridging the gap between identifying subtypes of asthma to understand causal mechanisms and translating this knowledge into personalized prevention and management strategies. In recent years, “big data” has been sold as a panacea for generating hypotheses and driving new frontiers of health care; the idea that the data must and will speak for themselves is fast becoming a new dogma. One of the dangers of ready accessibility of health care data and computational tools for data analysis is that the process of data mining can become uncoupled from the scientific process of clinical interpretation, understanding the provenance of the data, and external validation. Although advances in computational methods can be valuable for using unexpected structure in data to generate hypotheses, there remains a need for testing hypotheses and interpreting results with scientific rigor. We argue for combining data- and hypothesis-driven methods in a careful synergy, and the importance of carefully characterized birth and patient cohorts with genetic, phenotypic, biological, and molecular data in this process cannot be overemphasized. The main challenge on the road ahead is to harness bigger health care data in ways that produce meaningful clinical interpretation and to translate this into better diagnoses and properly personalized prevention and treatment plans. There is a pressing need for cross-disciplinary research with an integrative approach to data science, whereby basic scientists, clinicians, data analysts, and epidemiologists work together to understand the heterogeneity of asthma.},
author = {Belgrave, Danielle and Henderson, John and Simpson, Angela and Buchan, Iain and Bishop, Christopher and Custovic, Adnan},
doi = {10.1016/j.jaci.2016.11.003},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Belgrave et al. - 2017 - Disaggregating asthma Big investigation versus big data.pdf:pdf},
isbn = {1097-6825},
issn = {10976825},
journal = {J. Allergy Clin. Immunol.},
keywords = {Asthma,big data,birth cohorts,endotypes,machine learning},
number = {2},
pages = {400--407},
pmid = {27871876},
title = {{Disaggregating asthma: Big investigation versus big data}},
url = {http://www.jacionline.org/article/S0091-6749(16)31345-8/pdf},
volume = {139},
year = {2017}
}
@article{Broderick2016,
abstract = {A known failing of many popular random graph models is that the Aldous-Hoover Theorem guarantees these graphs are dense with probability one; that is, the number of edges grows quadratically with the number of nodes. This behavior is considered unrealistic in observed graphs. We define a notion of edge exchangeability for random graphs in contrast to the established notion of infinite exchangeability for random graphs --- which has traditionally relied on exchangeability of nodes (rather than edges) in a graph. We show that, unlike node exchangeability, edge exchangeability encompasses models that are known to provide a projective sequence of random graphs that circumvent the Aldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the number of edges with the number of nodes. We show how edge-exchangeability of graphs relates naturally to existing notions of exchangeability from clustering (a.k.a. partitions) and other familiar combinatorial structures.},
archivePrefix = {arXiv},
arxivId = {1603.06898},
author = {Broderick, Tamara and Cai, Diana},
eprint = {1603.06898},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Campbell, Broderick - Unknown - Edge-exchangeable graphs and sparsity.pdf:pdf},
issn = {10495258},
journal = {NIPS},
title = {{Edge-exchangeable graphs and sparsity}},
url = {https://papers.nips.cc/paper/6586-edge-exchangeable-graphs-and-sparsity.pdf http://arxiv.org/abs/1603.06898},
year = {2016}
}
@article{Sussillo2016,
abstract = {A major hurdle to clinical translation of brain-machine interfaces (BMIs) is that current decoders, which are trained from a small quantity of recent data, become ineffective when neural recording conditions subsequently change. We tested whether a decoder could be made more robust to future neural variability by training it to handle a variety of recording conditions sampled from months of previously collected data as well as synthetic training data perturbations. We developed a new multiplicative recurrent neural network BMI decoder that successfully learned a large variety of neural-to- kinematic mappings and became more robust with larger training datasets. When tested with a non-human primate preclinical BMI model, this decoder was robust under conditions that disabled a state-of-the-art Kalman filter based decoder. These results validate a new BMI strategy in which accumulated data history is effectively harnessed, and may facilitate reliable daily BMI use by reducing decoder retraining downtime.},
archivePrefix = {arXiv},
arxivId = {1610.05872},
author = {Sussillo, David and Stavisky, Sergey D. and Kao, Jonathan C. and Ryu, Stephen I. and Shenoy, Krishna V.},
doi = {10.1038/ncomms13749},
eprint = {1610.05872},
file = {:Users/ricardo/Downloads/Sussillo{\_}et{\_}al-2016-Nature{\_}Communications.pdf:pdf},
issn = {20411723},
journal = {Nat. Commun.},
pages = {1--12},
pmid = {27958268},
publisher = {Nature Publishing Group},
title = {{Making brain-machine interfaces robust to future neural variability}},
url = {http://dx.doi.org/10.1038/ncomms13749},
volume = {7},
year = {2016}
}
@article{Wang2014,
abstract = {Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem, and recently a lot of efforts have been devoted to approximate search. In this paper, we present a survey on one of the main solutions, hashing, which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories: locality sensitive hashing, which designs hash functions without exploring the data distribution and learning to hash, which learns hash functions according the data distribution, and review them from various aspects, including hash function design and distance measure and search scheme in the hash coding space.},
archivePrefix = {arXiv},
arxivId = {1408.2927},
author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
doi = {10.1561/2200000016},
eprint = {1408.2927},
file = {:Users/ricardo/Downloads/2200000016.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Found. Trends Mach. Learn.},
number = {1},
pages = {1--122},
pmid = {17504609},
title = {{Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers}},
url = {http://arxiv.org/abs/1408.2927},
volume = {3},
year = {2014}
}
@article{Shimizu2011,
abstract = {Structural equation models and Bayesian networks have been widely used to analyze causal rela-tions between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identifies the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps. In this pa-per, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is infinite.},
author = {Shimizu, Shohei and Inazumi, Takanori and Sogawa, Yasuhiro and Hyv{\"{a}}rinen, Aapo and Kawahara, Yoshinobu and Washio, Takashi and Hoyer, Patrik O and Bollen, Kenneth},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Shimizu et al. - 2011 - DirectLiNGAM A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {Bayesian networks,causal discovery,independent component analysis,non-Gaussianity,structural equation models},
pages = {1225--1248},
title = {{DirectLiNGAM: A Direct Method for Learning a Linear Non-Gaussian Structural Equation Model}},
volume = {12},
year = {2011}
}
@article{Kuleshov2017,
abstract = {Highly expressive directed latent variable mod-els, such as sigmoid belief networks, are diffi-cult to train on large datasets because exact in-ference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement effi-cient exact sampling from the variational poste-rior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference network gradient is too high-variance to be useful, we make it practi-cal by applying several straightforward model-independent variance reduction techniques. Ap-plying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algo-rithm on MNIST and achieves state-of-the-art re-sults on the Reuters RCV1 document dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.0030v1},
author = {Kuleshov, Volodymyr and Ermon, Stefano;},
eprint = {arXiv:1402.0030v1},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kuleshov, Ermon - Unknown - Neural Variational Inference and Learning in Undirected Graphical Models.pdf:pdf},
journal = {NIPS},
pages = {1},
title = {{Neural Variational Inference and Learning in Belief Networks}},
url = {https://arxiv.org/pdf/1711.02679.pdf},
volume = {32},
year = {2017}
}
@article{Huang2015,
abstract = {Most approaches to causal discovery assume a fixed (or time-invariant) causal model; however, in practical situations, especially in neuroscience and economics, causal relations might be time-dependent for various reasons. This paper aims to identify the time-dependent causal relations from observational data. We consider general formula-tions for time-varying causal modeling on stochas-tic processes, which can also capture the causal influence from a certain type of unobserved con-founders. We focus on two issues: one is whether such a causal model, including the causal direc-tion, is identifiable from observational data; the other is how to estimate such a model in a prin-cipled way. We show that under appropriate as-sumptions, the causal structure is identifiable ac-cording to our formulated model. We then propose a principled way for its estimation by extending Gaussian Process regression, which enables an au-tomatic way to learn how the causal model changes over time. Experimental results on both artificial and real data demonstrate the practical usefulness of time-dependent causal modeling and the effec-tiveness of the proposed approach for estimation.},
author = {Huang, Biwei and Zhang, Kun and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/ricardo/Downloads/11276-50060-1-PB.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
journal = {Proc. 24th IJCAI},
keywords = {Special Track on Machine Learning},
number = {Ijcai},
pages = {3561--3568},
title = {{Identiﬁcation of time-dependent causal model: A Gaussian process treatment}},
url = {http://www.ijcai.org/Proceedings/15/Papers/501.pdf{\%}0Ahttps://www.ijcai.org/Proceedings/15/Papers/501.pdf},
year = {2015}
}
@article{Efron2004,
abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a C p estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Efron et al. - 2004 - Least angle regression.pdf:pdf},
journal = {Ann. Stat.},
number = {2},
pages = {407--499},
title = {{Least angle regression}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aos/1083178935},
volume = {32},
year = {2004}
}
@article{Mackey2010,
abstract = {Discrete mixed membership modeling and continuous latent factor modeling (also known as matrix factorization) are two popular, complementary approaches to dyadic data analysis. In this work, we develop a fully Bayesian framework for integrating the two approaches into unified Mixed Membership Matrix Factorization (M 3 F) models. We introduce two M 3 F models, derive Gibbs sampling inference procedures, and validate our methods on the EachMovie, MovieLens, and Netflix Prize collaborative filtering datasets. We find that, even when fitting fewer parameters, the M 3 F models outperform state-ofthe-art latent factor approaches on all benchmarks, yielding the greatest gains in accuracy on sparsely-rated, high-variance items. 1.},
author = {Mackey, Lester and Weiss, David and Jordan, Michael I},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Mackey, Weiss, Jordan - 2010 - Mixed Membership Matrix Factorization.pdf:pdf},
journal = {ICML},
pages = {1--8},
title = {{Mixed Membership Matrix Factorization}},
url = {http://drona.csa.iisc.ernet.in/{~}shivani/Teaching/E0371/Papers/icml10-mixed-membership-matrix-factorization.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.5487{\&}rep=rep1{\&}type=pdf},
year = {2010}
}
@article{Donoho2004,
abstract = {We interpret non-negative matrix factorization geometrically, as the problem of finding a simplicial cone which contains a cloud of data points and which is contained in the positive orthant. We show that un- der certain conditions, basically requiring that some of the data are spread across the faces of the positive orthant, there is a unique such simplicial cone. We give examples of synthetic image articulation databases which obey these conditions; these require separated support and factorial sam- pling. For such databases there is a generative model in terms of “parts” and NMF correctly identifies the ”parts”. We show that our theoretical results are predictive of the performance of publishedNMFcode, by run- ning the published algorithms on one of our synthetic image articulation databases.},
author = {Donoho, David and Stodden, Victoria},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Donoho, Stodden - Unknown - When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts.pdf:pdf},
journal = {NIPS},
pages = {1141--1148},
title = {{When does non-negative matrix factorization give a correct decomposition into parts?}},
year = {2004}
}
@article{Recht2018,
abstract = {Machine learning is currently dominated by largely experimental work focused on improve-ments in a few key tasks. However, the impressive accuracy numbers of the best performing models are questionable because the same test sets have been used to select these models for multiple years now. To understand the danger of overfitting, we measure the accuracy of CIFAR-10 classifiers by creating a new test set of truly unseen images. Although we ensure that the new test set is as close to the original data distribution as possible, we find a large drop in accuracy (4{\%} to 10{\%}) for a broad range of deep learning models. Yet, more recent models with higher original accuracy show a smaller drop and better overall performance, indicating that this drop is likely not due to overfitting based on adaptivity. Instead, we view our results as evidence that current accuracy numbers are brittle and susceptible to even minute natural variations in the data distribution.},
archivePrefix = {arXiv},
arxivId = {1806.00451},
author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Mit Vaishaal},
eprint = {1806.00451},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Recht et al. - 2018 - Do CIFAR-10 Classifiers Generalize to CIFAR-10.pdf:pdf},
title = {{Do CIFAR-10 Classifiers Generalize to CIFAR-10?}},
url = {https://arxiv.org/pdf/1806.00451.pdf},
year = {2018}
}
@article{Yarkoni2010,
abstract = {Cognitive neuroscientists increasingly recognize that continued progress in understanding human brain function will require not only the acquisition of new data, but also the synthesis and integration of data across studies and laboratories. Here we review ongoing efforts to develop a more cumulative science of human brain function. We discuss the rationale for an increased focus on formal synthesis of the cognitive neuroscience literature, provide an overview of recently developed tools and platforms designed to facilitate the sharing and integration of neuroimaging data, and conclude with a discussion of several emerging developments that hold even greater promise in advancing the study of human brain function. {\textcopyright} 2010 Elsevier Ltd.},
author = {Yarkoni, Tal and Poldrack, Russell A and {Van Essen}, David C and Wager, Tor D},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Yarkoni et al. - 2010 - Cognitive neuroscience 2.0 building a cumulative science of human brain function(2).pdf:pdf},
journal = {Trends Cogn. Sci.},
number = {11},
pages = {489--496},
title = {{Cognitive neuroscience 2.0: Building a cumulative science of human brain function}},
volume = {14},
year = {2010}
}
@article{Gretton2008,
abstract = {Whereas kernel measures of independence have been widely applied in machine learning (notably in kernel ICA), there is as yet no method to determine whether they have detected statistically significant dependence. We provide a novel test of the independence hypothesis for one particular kernel independence measure, the Hilbert-Schmidt independence criterion (HSIC). The resulting test costs O(m{\^{}}2), where m is the sample size. We demonstrate that this test outperforms established contingency table-based tests. Finally, we show the HSIC test also applies to text (and to structured data more generally), for which no other independence test presently exists.},
author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon Hui and Song, Le and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gretton et al. - Unknown - A Kernel Statistical Test of Independence.pdf:pdf},
isbn = {978-1-605-60352-0},
journal = {Neural Inf. Process. Syst.},
keywords = {Brain Computer Interfaces,Computational,Information-Theoretic Learning with Statistics,Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {585--592},
title = {{A kernel statistical test of independence}},
year = {2008}
}
@article{Johnson,
abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this prob-lem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast con-vergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the stor-age of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
author = {Johnson, Rie and Zhang, Tong},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Johnson, Zhang - Unknown - Accelerating Stochastic Gradient Descent using Predictive Variance Reduction.pdf:pdf},
issn = {10495258},
journal = {NIPS},
keywords = {To-Read},
number = {3},
pages = {315--323},
pmid = {880145},
title = {{Accelerating Stochastic Gradient Descent using Predictive Variance Reduction}},
url = {http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf{\%}5Cnhttp://papers.nips.cc/pa},
volume = {1},
year = {2013}
}
@article{Arora2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.05653v2},
author = {Arora, Sanjeev and Liang, Yingyu},
eprint = {arXiv:1511.05653v2},
file = {:Users/ricardo/Downloads/deepnet{\_}main{\_}shorten.pdf:pdf},
journal = {ICLR Work. Track},
pages = {1--23},
title = {{Why are deep nets reversible : Simple theory, with implications for training}},
year = {2016}
}
@article{Jolliffe2003,
abstract = {In many multivariate statistical techniques, a set of linear functions of the original p variables is produced. One of the more dif cult aspects of these techniques is the inter-pretation of the linear functions, as these functions usually have nonzero coeff cients on all p variables. A common approach is to effectively ignore (treat as zero) any coef cients less than some threshold value, so that the function becomes simple and the interpretation becomes easier for the users. Such a procedure can be misleading. There are alternatives to principal component analysis which restrict the coef cients to a smaller number of possible values in the derivationof the linear functions,or replace the principalcomponentsby " prin-cipal variables. " This article introduces a new technique, borrowing an idea proposed by Tibshirani in the context of multiple regression where similar problems arise in interpreting regression equations. This approach is the so-called LASSO, the " least absolute shrinkage and selection operator, " in which a bound is introduced on the sum of the absolute values of the coef cients, and in which some coef cients consequently become zero. We explore some of the propertiesof the new technique,both theoreticallyand using simulation studies, and apply it to an example.},
archivePrefix = {arXiv},
arxivId = {1205.0121v2},
author = {Jolliffe, Ian T and Trendafilov, Nickolay T and Uddin, Mudassir},
doi = {10.1198/1061860032148},
eprint = {1205.0121v2},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Jolliffe, Trendafilov, Uddin - Unknown - A Modii ed Principal Component Technique Based on the LASSO.pdf:pdf},
isbn = {1061-8600},
issn = {1061-8600},
journal = {J. Comput. Graph. Stat.},
keywords = {Interpretation,Principal component analysis,Simplii cation},
number = {3},
pages = {531--547},
pmid = {21811560},
title = {{A Modified Principal Component Technique Based on the LASSO}},
url = {http://www.tandfonline.com/doi/pdf/10.1198/1061860032148 http://www.tandfonline.com/doi/abs/10.1198/1061860032148},
volume = {12},
year = {2003}
}
@article{Theis,
abstract = {Predicting human fixations from images has recently seen large improvements by leveraging deep representations which were pretrained for object recognition. However, as we show in this paper, these networks are highly overparameterized for the task of fixation prediction. We first present a simple yet principled greedy pruning method which we call Fisher pruning. Through a combination of knowledge distillation and Fisher pruning, we obtain much more runtime-efficient architectures for saliency prediction, achieving a 10x speedup for the same AUC performance as a state of the art network on the CAT2000 dataset. Speeding up single-image gaze prediction is important for many real-world applications, but it is also a crucial step in the development of video saliency models, where the amount of data to be processed is substantially larger.},
archivePrefix = {arXiv},
arxivId = {1801.05787},
author = {Theis, Lucas and Korshunova, Iryna and Tejani, Alykhan and Husz{\'{a}}r, Ferenc},
eprint = {1801.05787},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Theis et al. - 2018 - Faster gaze prediction with dense networks and Fisher pruning.pdf:pdf},
title = {{Faster gaze prediction with dense networks and Fisher pruning}},
url = {https://arxiv.org/pdf/1801.05787.pdf http://arxiv.org/abs/1801.05787},
year = {2018}
}
@book{Hyvarinen2001a,
author = {Hyv{\"{a}}rinen, Aapo and Karhunen, Juha and Oja, Erkki},
publisher = {John Wiley {\&} Sons},
title = {{Independent Component Analysis}},
year = {2001}
}
@article{Luo2014,
abstract = {Brain networks has attracted the interests of many neuroscientists. From functional MRI (fMRI) data, statistical tools have been developed to recover brain networks. However, the dimensionality of whole-brain fMRI, usually in hundreds of thousands, challenges the applicability of these methods. We develop a hierarchical graphical model (HGM) to remediate this difficulty. This model introduces a hidden layer of networks based on sparse Gaussian graphical models, and the observed data are sampled from individual network nodes. In fMRI, the network layer models the underlying signals of different brain functional units, and how these units directly interact with each other. The introduction of this hierarchical structure not only provides a formal and interpretable approach, but also enables efficient computation for inferring big networks with hundreds of thousands of nodes. Based on the conditional convexity of our formulation, we develop an alternating update algorithm to compute the HGM model parameters simultaneously. The effectiveness of this approach is demonstrated on simulated data and a real dataset from a stop/go fMRI experiment.},
archivePrefix = {arXiv},
arxivId = {1403.4698},
author = {Luo, Xi},
eprint = {1403.4698},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Luo - 2014 - A Hierarchical Graphical Model for Big Inverse Covariance Estimation with an Application to fMRI.pdf:pdf},
keywords = {Convex optimization,Graphical models,K-means,Lasso,functional MRI},
title = {{A Hierarchical Graphical Model for Big Inverse Covariance Estimation with an Application to fMRI}},
url = {https://arxiv.org/pdf/1403.4698.pdf http://arxiv.org/abs/1403.4698},
year = {2014}
}
@article{Soltanolkotabi2014,
abstract = {Subspace clustering refers to the task of finding a multi-subspace repre-sentation that best fits a collection of points taken from a high-dimensional space. This paper introduces an algorithm inspired by sparse subspace clus-tering (SSC) [In IEEE Conference on Computer Vision and Pattern Recog-nition, CVPR (2009) 2790–2797] to cluster noisy data, and develops some novel theory demonstrating its correctness. In particular, the theory uses ideas from geometric functional analysis to show that the algorithm can accurately recover the underlying subspaces under minimal requirements on their orien-tation, and on the number of samples per subspace. Synthetic as well as real data experiments complement our theoretical study, illustrating our approach and demonstrating its effectiveness.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.2603v2},
author = {Soltanolkotabi, Mahdi and Elhamifar, Ehsan and Cand{\`{e}}s, Emmanuel J},
doi = {10.1214/13-AOS1199},
eprint = {arXiv:1301.2603v2},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Soltanolkotabi, Elhamifar, Cand{\`{e}}s - 2014 - Robust subspace clustering.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Ann. Stat.},
keywords = {Dantzig selector,Geometric functional analysis,LASSO,Multiple hypothesis testing,Nonasymptotic random matrix theory,Spectral clustering,Subspace clustering,True and false discoveries,ℓ1 minimization},
number = {2},
pages = {669--699},
title = {{Robust subspace clustering}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aos/1400592174},
volume = {42},
year = {2014}
}
@article{Balduzzi2016,
abstract = {Modern convolutional networks, incorporating rectifiers and max-pooling, are neither smooth nor convex. Standard guarantees therefore do not apply. Nevertheless, methods from convex optimization such as gradient descent and Adam are widely used as building blocks for deep learning algorithms. This paper provides the first convergence guarantee applicable to modern convnets. The guarantee matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor approximation -- a straightforward application of Taylor expansions to neural networks -- and the associated Taylor loss. Experiments on a range of optimizers, layers, and tasks provide evidence that the analysis accurately captures the dynamics of neural optimization. The second half of the paper applies the Taylor approximation to isolate the main difficulty in training rectifier nets: that gradients are shattered. We investigate the hypothesis that, by exploring the space of activation configurations more thoroughly, adaptive optimizers such as RMSProp and Adam are able to converge to better solutions.},
archivePrefix = {arXiv},
arxivId = {1611.02345},
author = {Balduzzi, David and McWilliams, Brian and Butler-Yeoman, Tony},
eprint = {1611.02345},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Balduzzi, Mcwilliams, Butler-Yeoman - Unknown - Neural Taylor Approximations Convergence and Exploration in Rectifier Networks.pdf:pdf},
issn = {1938-7228},
title = {{Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks}},
url = {https://arxiv.org/pdf/1611.02345.pdf http://arxiv.org/abs/1611.02345},
year = {2016}
}
@article{MingTan2014,
abstract = {We consider the problem of learning a high-dimensional graphical model in which there are a few hub nodes that are densely-connected to many other nodes. Many authors have studied the use of an 1 penalty in order to learn a sparse graph in the high-dimensional setting. However, the 1 penalty implicitly assumes that each edge is equally likely and independent of all other edges. We propose a general framework to accommodate more realistic networks with hub nodes, using a convex formulation that involves a row-column overlap norm penalty. We apply this general framework to three widely-used probabilistic graphical models: the Gaussian graphical model, the covariance graph model, and the binary Ising model. An alternating direction method of multipliers algorithm is used to solve the corresponding convex optimization problems. On synthetic data, we demonstrate that our proposed framework outperforms competitors that do not explicitly model hub nodes. We illustrate our proposal on a webpage data set and a gene expression data set.},
author = {{Ming Tan}, Kean and London, Palma and Mohan, Karthik and Lee, Su-In and Fazel, Maryam and Witten, Daniela and {Witten Tan}, Daniela},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ming Tan et al. - 2014 - Learning Graphical Models With Hubs.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {Gaussian graphical model,alternating direction method of multipliers,binary network,covariance graph,hub,lasso},
pages = {3297--3331},
title = {{Learning Graphical Models With Hubs}},
url = {http://www.jmlr.org/papers/volume15/tan14b/tan14b.pdf},
volume = {15},
year = {2014}
}
@book{Peters2017,
author = {Peters, Jonas and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
publisher = {MIT Press},
title = {{Elements of Causal Inference: Foundations and Learning Algorithms}},
year = {2017}
}
@article{He2016,
abstract = {Deep residual networks [1] have emerged as a family of ex-tremely deep architectures showing compelling accuracy and nice con-vergence behaviors. In this paper, we analyze the propagation formu-lations behind the residual building blocks, which suggest that the for-ward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connec-tions and after-addition activation. A series of ablation experiments sup-port the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/ resnet-1k-layers.},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - Unknown - Identity Mappings in Deep Residual Networks.pdf:pdf},
title = {{Identity Mappings in Deep Residual Networks}},
url = {https://arxiv.org/pdf/1603.05027.pdf},
year = {2016}
}
@article{Welling2008,
abstract = {Derive deterministic models as Variational EM solution to some probabilistic models, this establishes relationship.},
author = {Welling, Max and Chemudugunta, Chaitanya and Sutter, Nathan},
doi = {10.1137/1.9781611972788.18},
file = {:Users/ricardo/Downloads/Deterministic{\_}Latent{\_}Variable{\_}Models{\_}and{\_}Their{\_}Pit.pdf:pdf},
isbn = {9781605603179},
journal = {SIAM Conf. Data Min.},
number = {April 2008},
pages = {196--207},
title = {{Deterministic latent variable models and their pitfalls}},
url = {http://www.appliedmathematician.org/proceedings/datamining/2008/dm08{\_}18{\_}Welling.pdf},
year = {2008}
}
@article{Seung1999,
abstract = {Learning the parts of objects by non-negative matrix factorization},
author = {Seung, H. Sebastian and Lee, Daniel D.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Seung, Lee - 1999 - Learning the parts of objects by non-negative matrix factorization.pdf:pdf},
journal = {Nature},
number = {6755},
pages = {788--791},
publisher = {Nature Publishing Group},
title = {{Learning the parts of objects by non-negative matrix factorization}},
volume = {401},
year = {1999}
}
@article{Khaligh-Razavi2014,
abstract = {Inferior temporal (IT) cortex in human and nonhuman primates serves visual object recognition. Computational object-vision models, although continually improving, do not yet reach human performance. It is unclear to what extent the internal representations of computational models can explain the IT representation. Here we investigate a wide range of computational model representations (37 in total), testing their categorization performance and their ability to account for the IT representational geometry. The models include well-known neuroscientific object-recognition models (e.g. HMAX, VisNet) along with several models from computer vision (e.g. SIFT, GIST, self-similarity features, and a deep convolutional neural network). We compared the representational dissimilarity matrices (RDMs) of the model representations with the RDMs obtained from human IT (measured with fMRI) and monkey IT (measured with cell recording) for the same set of stimuli (not used in training the models). Better performing models were more similar to IT in that they showed greater clustering of representational patterns by category. In addition, better performing models also more strongly resembled IT in terms of their within-category representational dissimilarities. Representational geometries were significantly correlated between IT and many of the models. However, the categorical clustering observed in IT was largely unexplained by the unsupervised models. The deep convolutional network, which was trained by supervision with over a million category-labeled images, reached the highest categorization performance and also best explained IT, although it did not fully explain the IT data. Combining the features of this model with appropriate weights and adding linear combinations that maximize the margin between animate and inanimate objects and between faces and other objects yielded a representation that fully explained our IT data. Overall, our results suggest that explaining IT requires computational features trained through supervised learning to emphasize the behaviorally important categorical divisions prominently reflected in IT.},
author = {Khaligh-Razavi, Seyed Mahdi and Kriegeskorte, Nikolaus},
doi = {10.1371/journal.pcbi.1003915},
editor = {Diedrichsen, J{\"{o}}rn},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Khaligh-Razavi, Kriegeskorte - 2014 - Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation(2).pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {15537358},
journal = {PLoS Comput. Biol.},
month = {nov},
number = {11},
pages = {e1003915},
pmid = {25375136},
publisher = {Public Library of Science},
title = {{Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1003915},
volume = {10},
year = {2014}
}
@article{Zhang2017,
abstract = {Measurement error in the observed values of the variables can greatly change the output of various causal discovery methods. This problem has received much attention in multiple fields, but it is not clear to what extent the causal model for the measurement-error-free variables can be identified in the presence of measurement error with unknown variance. In this paper, we study precise sufficient identifiability conditions for the measurement-error-free causal model and show what information of the causal model can be recovered from observed data. In particular, we present two different sets of identifiability conditions, based on the second-order statistics and higher-order statistics of the data, respectively. The former was inspired by the relationship between the generating model of the measurement-error-contaminated data and the factor analysis model, and the latter makes use of the identifiability result of the over-complete independent component analysis problem.},
archivePrefix = {arXiv},
arxivId = {1706.03768},
author = {Zhang, Kun and Gong, Mingming and Ramsey, Joseph and Batmanghelich, Kayhan and Spirtes, Peter and Glymour, Clark},
eprint = {1706.03768},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Causal Discovery in the Presence of Measurement Error Identifiability Conditions.pdf:pdf},
title = {{Causal Discovery in the Presence of Measurement Error: Identifiability Conditions}},
url = {https://www.cs.purdue.edu/homes/eb/causal-uai17/papers/9.pdf http://arxiv.org/abs/1706.03768},
year = {2017}
}
@book{Hayking2008,
author = {Haykin, Simon},
publisher = {Pearson Education},
title = {{Adaptive Filter Theory}},
year = {2008}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar-ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Medhi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Coureville, Aaron and Bengio, Yoshua},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - Unknown - Generative Adversarial Nets.pdf:pdf},
journal = {Neural Inf. Process. Syst.},
pages = {2672--2680},
title = {{Generative adversarial nets}},
year = {2014}
}
@article{Marlin2009,
abstract = {Recent work has shown that one can learn the structure of Gaussian Graphical Models by imposing an L1 penalty on the precision matrix, and then using efficient convex opti-mization methods to find the penalized max-imum likelihood estimate. This is similar to performing MAP estimation with a prior that prefers sparse graphs. In this paper, we use the stochastic block model as a prior. This prefer graphs that are blockwise sparse, but unlike previous work, it does not require that the blocks or groups be specified a priori. The resulting problem is no longer convex, but we devise an efficient variational Bayes algo-rithm to solve it. We show that our method has better test set likelihood on two differ-ent datasets (motion capture and gene ex-pression) compared to independent L1, and can match the performance of group L1 us-ing manually created groups.},
author = {Marlin, Benjamin and Murphy, Kevin},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Marlin, Murphy - 2009 - Sparse {\{}Gaussian{\}} Graphical Models with Unknown Block Structure.pdf:pdf},
journal = {ICML},
pages = {705--712},
title = {{Sparse Gaussian Graphical Models with Unknown Block Structure}},
year = {2009}
}
@article{Guest2017,
abstract = {{\textless}p{\textgreater}The success of fMRI places constraints on the nature of the neural code. The fact that researchers can infer similarities between neural representations, despite fMRI's limitations, implies that certain neural coding schemes are more likely than others. For fMRI to succeed given its low temporal and spatial resolution, the neural code must be smooth at the voxel and functional level such that similar stimuli engender similar internal representations. Through proof and simulation, we determine which coding schemes are plausible given both fMRI's successes and its limitations in measuring neural activity. Deep neural network approaches, which have been forwarded as computational accounts of the ventral stream, are consistent with the success of fMRI, though functional smoothness breaks down in the later network layers. These results have implications for the nature of the neural code and ventral stream, as well as what can be successfully investigated with fMRI.{\textless}/p{\textgreater}},
author = {Guest, Olivia and Love, Bradley C},
doi = {10.7554/eLife.21397},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Guest, Love - 2017 - What the success of brain imaging implies about the neural code(2).pdf:pdf},
issn = {2050-084X},
journal = {Elife},
keywords = {BOLD response,convolutional neural network,deep artificial neural network,neural code,neural plausibility,ventral stream},
month = {jan},
pages = {e21397},
publisher = {eLife Sciences Publications Limited},
title = {{What the success of brain imaging implies about the neural code}},
url = {https://elifesciences.org/articles/21397},
volume = {6},
year = {2017}
}
@inproceedings{Zhang2017a,
abstract = {It is commonplace to encounter nonstationary or heterogeneous data, of which the underlying generating process changes over time or across data sets (the data sets may have different experimental conditions or data collection conditions). Such a distribution shift feature presents both challenges and opportunities for causal discovery. In this paper we develop a principled framework for causal discovery from such data, called Constraint-based causal Discovery from Nonstationary/heterogeneous Data (CD-NOD), which addresses two important questions. First, we propose an enhanced constraint-based procedure to detect variables whose local mechanisms change and recover the skeleton of the causal structure over observed variables. Second, we present a way to determine causal orientations by making use of independence changes in the data distribution implied by the underlying causal model, benefiting from information carried by changing distributions. Experimental results on various synthetic and real-world data sets are presented to demonstrate the efficacy of our methods.},
author = {Zhang, Kun and Huang, Biwei and Zhang, Jiji and Glymour, Clark and Sch{\"{o}}lkopf, Bernhard},
booktitle = {Int. Jt. Conf. Artif. Intell.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Causal Discovery from NonstationaryHeterogeneous Data Skeleton Estimation and Orientation Determination(2).pdf:pdf},
pages = {1347--1353},
title = {{Causal discovery from Nonstationary/heterogeneous data: Skeleton estimation and orientation determination}},
year = {2017}
}
@article{Damoiseaux2006,
abstract = {Functional MRI (fMRI) can be applied to study the functional connectivity of the human brain. It has been suggested that fluctuations in the blood oxygenation level-dependent (BOLD) signal during rest reflect the neuronal baseline activity of the brain, representing the state of the human brain in the absence of goal-directed neuronal action and external input, and that these slow fluctuations correspond to functionally relevant resting-state networks. Several studies on resting fMRI have been conducted, reporting an apparent similarity between the identified patterns. The spatial consistency of these resting patterns, however, has not yet been evaluated and quantified. In this study, we apply a data analysis approach called tensor probabilistic independent component analysis to resting-state fMRI data to find coherencies that are consistent across subjects and sessions. We characterize and quantify the consistency of these effects by using a bootstrapping approach, and we estimate the BOLD amplitude modulation as well as the voxel-wise cross-subject variation. The analysis found 10 patterns with potential functional relevance, consisting of regions known to be involved in motor function, visual processing, executive functioning, auditory processing, memory, and the so-called default-mode network, each with BOLD signal changes up to 3{\%}. In general, areas with a high mean percentage BOLD signal are consistent and show the least variation around the mean. These findings show that the baseline activity of the brain is consistent across subjects exhibiting significant temporal dynamics, with percentage BOLD signal change comparable with the signal changes found in task-related experiments.},
author = {Damoiseaux, J S and Rombouts, S A R B and Barkhof, F and Scheltens, P and Stam, C J and Smith, S M and Beckmann, C F},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Damoiseaux et al. - 2006 - Consistent resting-state networks across healthy subjects.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci.},
number = {37},
pages = {13848--13853},
title = {{Consistent resting-state networks across healthy subjects}},
volume = {103},
year = {2006}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
volume = {12},
year = {2011}
}
@article{Rule2018,
abstract = {M.H.H. secured funding and conceived of the project. M.S. and M.H.H. developed the analysis of Fisher information. M.R. and M.H.H. developed and analyzed the energy and entropy decomposition of latent representations. All authors contributed to the manuscript. e authors declare no connicting interests. keywords: information theory | encoding | neural networks | sensory systems Abstract We examine the problem of optimal sparse encoding of sensory stimuli by latent vari-ables in stochastic models. Analyzing restricted Boltzmann machines with a communications the-ory approach, we search for the minimal model size that correctly conveys the correlations in stimulus paaerns in an information-theoretic sense. We show that the Fisher information Matrix (FIM) reveals the optimal model size. In larger models the FIM reveals that irrelevant parame-ters are associated with individual latent variables, displaying a surprising amount of order. For well--t models, we observe the emergence of statistical criticality as diverging generalized suscep-tibility of the model. In this case, an encoding strategy is adopted where highly informative, but rare stimuli selectively suppress variability in the encoding units. .e information content of the encoded stimuli acts as an unobserved variable leading to criticality. Together, these results can explain the stimulus-dependent variability suppression observed in sensory systems, and suggest a simple, correlation-based measure to reduce the size of artiicial neural networks. Signiicance Currently liile is known about the statistical structure of representations in stochas-tic latent encoders, which serve as models of neuronal sensory systems and are widely used in ma-chine learning. Using approaches from statistical physics and information theory, we show that it is possible to evaluate an optimal size of the latent space, and observe emergence of statistical criticality at this model size. Criticality corresponds to an encoding strategy for handling variable information-content of stimuli in a stochastic channel with a axed hidden-layer size. .ese results yield testable hypotheses about encoding in neuronal sensory systems, and suggest strategies for improving machine learning models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.10361v1},
author = {Rule, M and Sorbaro, M and Hennig, M},
eprint = {arXiv:1802.10361v1},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Rule, Sorbaro, Hennig - 2018 - Optimal encoding in stochastic latent-variable Models.pdf:pdf},
keywords = {ables in stochastic models,abstract we examine the,analyzing restricted boltzmann machines,by latent vari-,correctly conveys the correlations,encoding,encoding of sensory stimuli,in,information theory,minimal model size that,neural networks,ory approach,problem of optimal sparse,sensory systems,we search for the,with a communications the-},
pages = {1--17},
title = {{Optimal encoding in stochastic latent-variable Models}},
url = {https://arxiv.org/pdf/1802.10361.pdf},
year = {2018}
}
@article{Tononi1994,
abstract = {In brains of higher vertebrates, the functional segregation of local areas that differ in their anatomy and physiology contrasts sharply with their global integration during perception and behavior. In this paper, we introduce a measure, called neural complexity (CN), that captures the interplay between these two fundamental aspects of brain organization. We express functional segregation within a neural system in terms of the relative statistical independence of small subsets of the system and functional integration in terms of significant deviations from independence of large subsets. CN is then obtained from estimates of the average deviation from statistical independence for subsets of increasing size. CN is shown to be high when functional segregation coexists with integration and to be low when the components of a system are either completely independent (segregated) or completely dependent (integrated). We apply this complexity measure in computer simulations of cortical areas to examine how some basic principles of neuroanatomical organization constrain brain dynamics. We show that the connectivity patterns of the cerebral cortex, such as a high density of connections, strong local connectivity organizing cells into neuronal groups, patchiness in the connectivity among neuronal groups, and prevalent reciprocal connections, are associated with high values of CN. The approach outlined here may prove useful in analyzing complexity in other biological domains such as gene regulation and embryogenesis.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Tononi, Giulio and Sporns, Olaf and Edelman, Gerald M},
doi = {10.1073/pnas.91.11.5033},
eprint = {NIHMS150003},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Tononi, Sporns, Edelman - 1994 - A measure for brain complexity Relating functional segregation and integration in the nervous system.pdf:pdf},
isbn = {0027-8424 (Print){\$}\backslash{\$}n0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci.},
number = {11},
pages = {5033--5037},
pmid = {8197179},
title = {{A measure for brain complexity: relating functional segregation and integration in the nervous system.}},
url = {http://www.pnas.org/content/91/11/5033.long http://www.pnas.org/cgi/doi/10.1073/pnas.91.11.5033},
volume = {91},
year = {1994}
}
@article{Zhang2018a,
abstract = {AI plays an increasingly prominent role in society since decisions that were once made by humans are now delegated to automated systems. These systems are currently in charge of deciding bank loans, criminals' incarceration, and the hiring of new employees, and it's not difficult to envision that they will in the future underpin most of the decisions in society. Despite the high complexity entailed by this task, there is still not much understanding of basic properties of such systems. For instance, we currently cannot detect (neither explain nor correct) whether an AI system is operating fairly (i.e., is abiding by the decision-constraints agreed by society) or it is reinforcing biases and perpetuating a preceding prejudicial practice. Issues of discrimination have been discussed extensively in legal circles, but there exists still not much understanding of the formal conditions that an automated system must adhere to be deemed fair. In this paper, we use the language of structural causality (Pearl, 2000) to fill in this gap. We start by introducing three new fine-grained measures of transmission of change from stimulus to effect called counterfactual direct (Ctf-DE), indirect (Ctf-IE), and spurious (Ctf-SE) effects. Building on these measures, we derive the causal explanation formula, which allows the AI designer to quantitatively evaluate fairness and explain the total observed disparity of decisions through different discriminatory mechanisms. We apply these results to various discrimination analysis tasks and run extensive simulations, including detection, evaluation, and optimization of decision-making under fairness constraints. We conclude studying the trade-off between different types of fairness criteria (outcome and procedural), and provide a quantitative approach to policy implementation and the design of fair decision-making systems.},
author = {Zhang, Junzhe and Bareinboim, Elias},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Bareinboim - Unknown - Fairness in Decision-Making-The Causal Explanation Formula.pdf:pdf},
journal = {AAAI Conf. Artif. Intell.},
title = {{Fairness in Decision-Making-The Causal Explanation Formula}},
year = {2018}
}
@article{Zass2007,
abstract = {We describe a nonnegative variant of the ”Sparse PCA” problem. The goal is to create a low dimensional representation from a collection of points which on the one hand maximizes the variance of the projected points and on the other uses only parts of the original coordinates, and thereby creating a sparse representa- tion. What distinguishes our problem from other Sparse PCA formulations is that the projection involves only nonnegative weights of the original coordinates—a desired quality in various fields, including economics, bioinformatics and com- puter vision. Adding nonnegativity contributes to sparseness, where it enforces a partitioning of the original coordinates among the new axes. We describe a sim- ple yet efficient iterative coordinate-descent type of scheme which converges to a local optimum of our optimization criteria, giving good results on large real world datasets.},
author = {Zass, Ron and Shashua, Amnon},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zass, Shashua - Unknown - Nonnegative Sparse PCA.pdf:pdf},
journal = {NIPS},
pages = {1561--1568},
title = {{Nonnegative sparse PCA}},
volume = {19},
year = {2007}
}
@article{Shvartsman2018,
abstract = {Multivariate analysis of fMRI data has benefited sub-stantially from advances in machine learning. Most recently, a range of probabilistic latent variable models applied to fMRI data have been successful in a variety of tasks, including identifying similarity patterns in neural data (Representational Similarity Analysis and its empirical Bayes variant, RSA and BRSA; Inter-subject Functional Connectivity, ISFC), combining multi-subject datasets (Shared Response Mapping; SRM), and mapping between brain and behavior (Si-multaneous Modeling). Although these methods share some underpinnings, they have been developed as dis-tinct methods, with distinct algorithms and software tools. We show how the matrix-variate normal (MN) formalism can unify some of these methods into a single framework. In doing so, we gain the ability to reuse noise modeling assumptions, algorithms, and code across models. Our primary theoretical con-tribution shows how some of these methods can be written as instantiations of the same model, allow-ing us to generalize them to flexibly modeling struc-tured noise covariances. Our formalism permits novel model variants and improved estimation strategies: in contrast to SRM, the number of parameters for MN-SRM does not scale with the number of voxels or subjects; in contrast to BRSA, the number of pa-rameters for MN-RSA scales additively rather than * Corresponding author: ms44@princeton.edu multiplicatively in the number of voxels. We empir-ically demonstrate advantages of two new methods derived in the formalism: for MN-RSA, we show up to 10x improvement in runtime, up to 6x improve-ment in RMSE, and more conservative behavior under the null. For MN-SRM, our method grants a modest improvement to out-of-sample reconstruction while re-laxing an orthonormality constraint of SRM. We also provide a software prototyping tool for MN models that can flexibly reuse noise covariance assumptions and algorithms across models.},
author = {Shvartsman, Michael and Sundaram, Narayanan and Aoi, Mikio C and Charles, Adam and Wilke, Theodore L and Cohen, Jonathan D},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Shvartsman et al. - 2017 - Matrix-normal models for fMRI analysis.pdf:pdf},
journal = {AISTATS},
title = {{Matrix-normal models for fMRI analysis}},
url = {https://arxiv.org/pdf/1711.03058.pdf},
year = {2018}
}
@article{Peters2013a,
abstract = {Causal inference relies on the structure of a graph, often a directed acyclic graph (DAG). Different graphs may result in different causal inference statements and different intervention distributions. To quantify such differences, we propose a (pre-) distance between DAGs, the structural intervention distance (SID). The SID is based on a graphical criterion only and quantifies the closeness between two DAGs in terms of their corresponding causal inference statements. It is therefore well-suited for evaluating graphs that are used for computing interventions. Instead of DAGs it is also possible to compare CPDAGs, completed partially directed acyclic graphs that represent Markov equivalence classes. Since it differs significantly from the popular Structural Hamming Distance (SHD), the SID constitutes a valuable additional measure. We discuss properties of this distance and provide an efficient implementation with software code available on the first author's homepage (an R package is under construction).},
archivePrefix = {arXiv},
arxivId = {1306.1043},
author = {Peters, Jonas and B{\"{u}}hlmann, Peter},
doi = {10.1162/NECO_a_00708},
eprint = {1306.1043},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Peters, Uhlmann - Unknown - Structural Intervention Distance for Evaluating Causal Graphs(2).pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Comput.},
pages = {771--799},
pmid = {25602775},
title = {{Structural Intervention Distance (SID) for Evaluating Causal Graphs}},
volume = {27},
year = {2015}
}
@article{Kemker2017,
abstract = {Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved.},
archivePrefix = {arXiv},
arxivId = {1708.02072},
author = {Kirkpatrick, James and Pascanu, Razvan and Clopath, Claudia and Hadsell, Raia},
doi = {10.1073/pnas.1611835114},
eprint = {1708.02072},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kirkpatrick et al. - Unknown - Overcoming catastrophic forgetting in neural networks.pdf:pdf},
isbn = {1215421109},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci.},
pmid = {28292907},
title = {{Measuring Catastrophic Forgetting in Neural Networks}},
url = {http://arxiv.org/abs/1708.02072},
year = {2017}
}
@article{Hyvarinen2001,
abstract = {Blind separation of source signals usually relies either on the nonGaussianity of the signals or on their linear autocorrelations. A third approach was introduced by Matsuoka et al. (1995), who showed that source separation can be performed by using the nonstationarity of the signals, in particular the nonstationarity of their variances. In this paper, we show how to interpret the nonstationarity due to a smoothly changing variance in terms of higher order cross-cumulants. This is based on the time-correlation of the squares (energies) of the signals and leads to a simple optimization criterion. Using this criterion, we construct a fixed-point algorithm that is computationally very efficient.},
author = {Hyv{\"{a}}rinen, Aapo},
doi = {10.1109/72.963782},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hyv{\"{a}}rinen - 2001 - Blind Source Separation by Nonstationarity of Variance A Cumulant-Based Approach.pdf:pdf},
issn = {10459227},
journal = {IEEE Trans. Neural Networks},
keywords = {Blind source separation,Cumulants,Independent component analysis,Nonstationarity,Statistical signal processing},
number = {6},
pages = {1471--1474},
pmid = {18249975},
title = {{Blind source separation by nonstationarity of variance: A cumulant-based approach}},
url = {https://www.cs.helsinki.fi/u/ahyvarin/papers/TNN01.pdf},
volume = {12},
year = {2001}
}
@article{Ghoshal,
abstract = {The problem of learning structural equation models (SEMs) from data is a fundamental problem in causal inference. We develop a new algorithm --- which is computationally and statistically efficient and works in the high-dimensional regime --- for learning linear SEMs from purely observational data with arbitrary noise distribution. We consider three aspects of the problem: identifiability, computational efficiency, and statistical efficiency. We show that when data is generated from a linear SEM over {\$}p{\$} nodes and maximum degree {\$}d{\$}, our algorithm recovers the directed acyclic graph (DAG) structure of the SEM under an identifiability condition that is more general than those considered in the literature, and without faithfulness assumptions. In the population setting, our algorithm recovers the DAG structure in {\$}\backslashmathcal{\{}O{\}}(p(d{\^{}}2 + \backslashlog p)){\$} operations. In the finite sample setting, if the estimated precision matrix is sparse, our algorithm has a smoothed complexity of {\$}\backslashwidetilde{\{}\backslashmathcal{\{}O{\}}{\}}(p{\^{}}3 + pd{\^{}}7){\$}, while if the estimated precision matrix is dense, our algorithm has a smoothed complexity of {\$}\backslashwidetilde{\{}\backslashmathcal{\{}O{\}}{\}}(p{\^{}}5){\$}. For sub-Gaussian noise, we show that our algorithm has a sample complexity of {\$}\backslashmathcal{\{}O{\}}(\backslashfrac{\{}d{\^{}}8{\}}{\{}\backslashvarepsilon{\^{}}2{\}} \backslashlog (\backslashfrac{\{}p{\}}{\{}\backslashsqrt{\{}\backslashdelta{\}}{\}})){\$} to achieve {\$}\backslashvarepsilon{\$} element-wise additive error with respect to the true autoregression matrix with probability at most {\$}1 - \backslashdelta{\$}, while for noise with bounded {\$}(4m){\$}-th moment, with {\$}m{\$} being a positive integer, our algorithm has a sample complexity of {\$}\backslashmathcal{\{}O{\}}(\backslashfrac{\{}d{\^{}}8{\}}{\{}\backslashvarepsilon{\^{}}2{\}} (\backslashfrac{\{}p{\^{}}2{\}}{\{}\backslashdelta{\}}){\^{}}{\{}1/m{\}}){\$}.},
archivePrefix = {arXiv},
arxivId = {1707.04673},
author = {Ghoshal, Asish and Honorio, Jean},
eprint = {1707.04673},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ghoshal, Honorio - Unknown - Learning linear structural equation models in polynomial time and sample complexity(2).pdf:pdf},
title = {{Learning linear structural equation models in polynomial time and sample complexity}},
url = {https://arxiv.org/pdf/1707.04673.pdf http://arxiv.org/abs/1707.04673},
year = {2017}
}
@inproceedings{Ng2011,
abstract = {The estimation of intra-subject functional connectivity is greatly complicated by the small sample size and complex noise structure in functional magnetic resonance imaging (fMRI) data. Pooling samples across subjects im-proves the conditioning of the estimation, but loses subject-specific connec-tivity information. In this paper, we propose a new sparse group Gaussian graphical model (SGGGM) that facilitates joint estimation of intra-subject and group-level connectivity. This is achieved by casting functional connectivity es-timation as a regularized consensus optimization problem, in which information across subjects is aggregated in learning group-level connectivity and group in-formation is propagated back in estimating intra-subject connectivity. On syn-thetic data, we show that incorporating group information using SGGGM sig-nificantly enhances intra-subject connectivity estimation over existing tech-niques. More accurate group-level connectivity is also obtained. On real data from a cohort of 60 subjects, we show that integrating intra-subject connectivity estimated with SGGGM significantly improves brain activation detection over connectivity priors derived from other graphical modeling approaches.},
author = {Ng, Bernard and Varoquaux, Ga{\"{e}}l and Poline, Jean Baptiste and Thirion, Bertrand},
booktitle = {MICCAI},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ng et al. - 2011 - A Novel Sparse Group Gaussian Graphical Model for Functional Connectivity Estimation.pdf:pdf},
keywords = {Gaussian graphical model,brain connectivity,fMRI,regularized consensus optimization,sparse inverse covariance estimation},
number = {1},
title = {{A Novel Sparse Group Gaussian Graphical Model for Functional Connectivity Estimation}},
url = {https://pdfs.semanticscholar.org/a6d5/fbf007c5555221f8fa239219d65a048be699.pdf},
year = {2011}
}
@article{Bottou2013a,
abstract = {This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the sys-tem. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.},
archivePrefix = {arXiv},
arxivId = {1209.2355},
author = {Bottou, L{\'{e}}on and Peters, Jonas and Ch, Peters and Qui{\~{n}}onero-Candela, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
doi = {10.1145/2740908.2742564},
eprint = {1209.2355},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bottou et al. - 2013 - Counterfactual Reasoning and Learning Systems The Example of Computational Advertising(2).pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {J. Mach. Learn. Res.},
keywords = {causation,computational advertising,counterfactual reasoning},
pages = {3207--3260},
title = {{Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising}},
url = {http://www.jmlr.org/papers/volume14/bottou13a/bottou13a.pdf},
volume = {14},
year = {2013}
}
@article{BaldiP&Hornik1998,
abstract = {autoencoder; linearNN},
author = {Baldi, Pierre and Hornik, Kurt},
file = {:Users/ricardo/Downloads/PII{\_} 0893-6080(89)90014-2.pdf:pdf},
journal = {Neural Networks},
keywords = {-neural networks,back propagation,learning,principal component analysis},
number = {10},
pages = {53--58},
title = {{Neural networks and principal component analysis: Learning from examples without local minima}},
volume = {2},
year = {1998}
}
@inproceedings{Zeiler2010,
abstract = {Building robust low and mid-level image representations, beyond edge primitives, is a long-standing goal in vision. Many existing feature detectors spatially pool edge information which destroys cues such as edge intersections, parallelism and symmetry. We present a learning framework where features that capture these mid-level cues spontaneously emerge from image data. Our approach is based on the convolutional decomposition of images under a spar-sity constraint and is totally unsupervised. By building a hierarchy of such decompositions we can learn rich feature sets that are a robust image representation for both the analysis and synthesis of images.},
archivePrefix = {arXiv},
arxivId = {1302.1700},
author = {Zeiler, Matthew D. and Krishnan, Dilip and Taylor, Graham W. and Fergus, Rob},
booktitle = {CVPR},
doi = {10.1109/CVPR.2010.5539957},
eprint = {1302.1700},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zeiler et al. - Unknown - Deconvolutional Networks.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
pages = {2528--2535},
pmid = {16190471},
title = {{Deconvolutional networks}},
url = {http://www.matthewzeiler.com/wp-content/uploads/2017/07/cvpr2010.pdf},
year = {2010}
}
@article{Ahlheim2017,
abstract = {Recent advances in multivariate fMRI analysis stress the importance of infor-mation inherent to voxel patterns. Key to interpreting these patterns is esti-mating the underlying dimensionality of neural representations. Dimensions may correspond to psychological dimensions, such as length and orientation, or involve other coding schemes. Unfortunately, the noise structure of fMRI data inflates dimensionality estimates and thus makes it difficult to assess the true underlying dimensionality of a pattern. To address this challenge, we developed a novel approach to identify brain regions that carry reliable task-modulated signal and to derive an estimate of the signal's functional dimen-sionality. We combined singular value decomposition with cross-validation to find the best low-dimensional projection of a pattern of voxel-responses at a single-subject level. Goodness of the low-dimensional reconstruction is measured as Pearson correlation with a test set, which allows to test for significance of the low-dimensional reconstruction across participants. Using hierarchical Bayesian modeling, we derive the best estimate and associated uncertainty of underlying dimensionality across participants. We validated our method on simulated data of varying underlying dimensionality, show-ing that recovered dimensionalities match closely true dimensionalities. We then applied our method to three published fMRI data sets all involving pro-cessing of visual stimuli. The results highlight three possible applications of estimating the functional dimensionality of neural data. Firstly, it can aid evaluation of model-based analyses by revealing which areas express reliable, task-modulated signal that could be missed by specific models. Secondly, it can reveal functional differences across brain regions. Thirdly, knowing the functional dimensionality allows assessing task-related differences in the complexity of neural patterns.},
author = {Ahlheim, Christiane and Love, Bradley C},
doi = {10.1101/232454},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ahlheim, Love - 2017 - Estimating the functional dimensionality of neural representations.pdf:pdf},
keywords = {dimensionality reduction,multivariate analysis 2,neural representations},
pages = {95},
title = {{Estimating the functional dimensionality of neural representations}},
url = {http://dx.doi.org/10.1101/232454},
year = {2017}
}
@article{Zhu2017,
abstract = {Scientists routinely compare gene expression levels in cases versus controls in part to determine genes associated with a disease. Similarly, detecting case-control differences in co-expression among genes can be critical to understanding complex human diseases; however statistical methods have been limited by the high dimensional nature of this problem. In this paper, we construct a sparse-Leading-Eigenvalue-Driven (sLED) test for comparing two high-dimensional covariance matrices. By focusing on the spectrum of the differential matrix, sLED provides a novel perspective that accommodates what we assume to be common, namely sparse and weak signals in gene expression data, and it is closely related with Sparse Principal Component Analysis. We prove that sLED achieves full power asymptotically under mild assumptions, and simulation studies verify that it outperforms other existing procedures under many biologically plausible scenarios. Applying sLED to the largest gene-expression dataset obtained from post-mortem brain tissue from Schizophrenia patients and controls, we provide a novel list of genes implicated in Schizophrenia and reveal intriguing patterns in gene co-expression change for Schizophrenia subjects. We also illustrate that sLED can be generalized to compare other gene-gene " relationship " matrices that are of practical interest, such as the weighted adjacency matrices.},
author = {Zhu, Lingxue and Lei, Jing and Devlin, Bernie and Roeder, Kathryn},
doi = {10.1214/17-AOAS1062},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - Unknown - TESTING HIGH-DIMENSIONAL COVARIANCE MATRICES, WITH APPLICATION TO DETECTING SCHIZOPHRENIA RISK GENES.pdf:pdf},
journal = {Ann. Appl. Stat.},
keywords = {and phrases Permutation test,covariance matrix,high-dimensional data,sparse principal component analysis},
title = {{Testing high-dimensional covariance matrices, with application to detecting schizophrenia risk genes}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5655846/pdf/nihms909736.pdf},
year = {2017}
}
@article{Hyvarinen2016a,
abstract = {Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL), finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique --- thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general.},
author = {Hyv{\"{a}}rinen, Aapo and Morioka, Hiroshi},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hyv{\"{a}}rinen, Morioka - Unknown - Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA.pdf:pdf},
journal = {Neural Inf. Process. Syst.},
title = {{Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA}},
year = {2016}
}
@article{Lopes2004,
abstract = {Factor analysis has been one of the most powerful and flexible tools for assessment of multivariate dependence and codependence. Loosely speaking, it could be argued that the origin of its success rests in its very exploratory nature, where various kinds of data-relationships amongst the variables at study can be iteratively verified and/or refuted. Bayesian inference in factor analytic models has received renewed attention in recent years, partly due to computational advances but also partly to applied focuses generating factor structures as exemplified by recent work in financial time series modeling. The focus of our current work is on exploring questions of uncertainty about the number of latent factors in a multivariate factor model, combined with methodological and computational issues of model specification and model fitting. We explore reversible jump MCMC methods that build on sets of parallel Gibbs sampling-based analyses to generate suitable empirical proposal distributions and that address the challenging problem of finding efficient proposals in high-dimensional models. Alternative MCMC methods based on bridge sampling are discussed, and these fully Bayesian MCMC approaches are compared with a collection of popular model selection methods in empirical studies. Various additional computational issues are discussed, including situations where prior information is scarce, and the methods are explored in studies of some simulated data sets and an econometric time series example.},
author = {Lopes, Hedibert Freitas and West, Mike},
doi = {10.1.1.10.8242},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lopes, West - 2004 - BAYESIAN MODEL ASSESSMENT IN FACTOR ANALYSIS.pdf:pdf},
isbn = {1017-0405},
issn = {10170405},
journal = {Stat. Sin.},
keywords = {Bayesian inference,Key words and phrases: Bayes factors,bridge sampling,ex- pected posterior prior,latent factor models,model selection criteria,model uncer- tainty,reversible jump MCMC},
pages = {41--67},
title = {{Bayesian model assessment in factor analysis}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.8242{\&}rep=rep1{\&}type=pdf},
volume = {14},
year = {2004}
}
@article{Lai2014,
author = {Lai, Rongjie and Osher, Stanley},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lai, Osher - 2014 - A Splitting Method for Orthogonality Constrained Problems(2).pdf:pdf},
journal = {J. Sci. Comput.},
month = {feb},
number = {2},
pages = {431--449},
publisher = {Springer US},
title = {{A Splitting Method for Orthogonality Constrained Problems}},
volume = {58},
year = {2014}
}
@inproceedings{Eaton2007,
abstract = {We show how to extend the dynamic programming algorithm of Koivisto [KS04, Koi06], which computes the exact posterior marginal edge probabilities p(Gij = 1|D) of a DAG G given data D, to the case where the data is obtained by interventions (experiments). In particular, we consider the case where the targets of the interventions are a priori unknown. We show that it is possible to learn the targets of intervention at the same time as learning the causal structure. We apply our exact technique to a biological data set that had previously been analyzed using MCMC [SPP+05, EW06].},
author = {Eaton, Daniel and Murphy, Kevin},
booktitle = {Int. Conf. Artif. Intell. Stat.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Eaton, Murphy - Unknown - Exact Bayesian structure learning from uncertain interventions.pdf:pdf},
issn = {15324435},
pages = {107--114},
title = {{Exact Bayesian structure learning from uncertain interventions}},
url = {http://proceedings.mlr.press/v2/eaton07a/eaton07a.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Exact+Bayesian+structure+learning+from+uncertain+interventions{\#}0},
year = {2007}
}
@article{Garrigues2009,
abstract = {It has been shown that the problem of 1 -penalized least-square regression com-monly referred to as the Lasso or Basis Pursuit DeNoising leads to solutions that are sparse and therefore achieves model selection. We propose in this paper Re-cLasso, an algorithm to solve the Lasso with online (sequential) observations. We introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point. We com-pare our method to Lars and Coordinate Descent, and present an application to compressive sensing with sequential observations. Our approach can easily be extended to compute an homotopy from the current solution to the solution that corresponds to removing a data point, which leads to an efficient algorithm for leave-one-out cross-validation. We also propose an algorithm to automatically update the regularization parameter after observing a new data point.},
author = {Garrigues, Pierre J and Ghaoui, Laurent El},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Garrigues, Ghaoui - Unknown - An Homotopy Algorithm for the Lasso with Online Observations.pdf:pdf},
journal = {NIPS},
title = {{An Homotopy Algorithm for the Lasso with Online Observations}},
url = {http://papers.nips.cc/paper/3431-an-homotopy-algorithm-for-the-lasso-with-online-observations.pdf},
year = {2009}
}
@inproceedings{Varoquaux2010a,
abstract = {Functional brain connectivity, as revealed through distant correlations in the signals measured by functional Magnetic Resonance Imaging (fMRI), is a promising source of biomarkers of brain pathologies. However, establishing and using diagnostic markers requires probabilistic inter-subject comparisons. Principled comparison of functional-connectivity structures is still a challenging issue. We give a new matrix-variate probabilistic model suitable for inter-subject comparison of functional connectivity matrices on the manifold of Symmetric Positive Definite (SPD) matrices. We show that this model leads to a new algorithm for principled comparison of connectivity coefficients between pairs of regions. We apply this model to comparing separately post-stroke patients to a group of healthy controls. We find neurologically-relevant connection differences and show that our model is more sensitive that the standard procedure. To the best of our knowledge, these results are the first report of functional connectivity differences between a single-patient and a group and thus establish an important step toward using functional connectivity as a diagnostic tool.},
archivePrefix = {arXiv},
arxivId = {1008.5070},
author = {Varoquaux, Ga{\"{e}}l and Baronnet, Flore and Kleinschmidt, Andreas and Fillard, Pierre and Thirion, Bertrand},
booktitle = {MICCAI},
doi = {10.1007/978-3-642-15705-9_25},
eprint = {1008.5070},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Varoquaux et al. - 2010 - Detection of brain functional-connectivity difference in post-stroke patients using group-level covariance mod.pdf:pdf},
isbn = {3642157041},
issn = {03029743},
keywords = {()},
pages = {200--208},
pmid = {20879232},
title = {{Detection of brain functional-connectivity difference in post-stroke patients using group-level covariance modeling}},
url = {https://arxiv.org/pdf/1008.5070.pdf},
volume = {LNCS},
year = {2010}
}
@article{Ginestet2013,
abstract = {Comparing weighted networks in neuroscience is hard, because the topological properties of a given network are necessarily dependent on the number of edges of that network. This problem arises in the analysis of both weighted and unweighted networks. The term density is often used in this context, in order to refer to the mean edge weight of a weighted network, or to the number of edges in an unweighted one. Comparing families of networks is therefore statistically difficult because differences in topology are necessarily associated with differences in density. In this review paper, we consider this problem from two different perspectives, which include (i) the construction of summary networks, such as how to compute and visualize the mean network from a sample of network-valued data points; and (ii) how to test for topological differences, when two families of networks also exhibit significant differences in density. In the first instance, we show that the issue of summarizing a family of networks can be conducted by adopting a mass-univariate approach, which produces a statistical parametric network (SPN). In the second part of this review, we then highlight the inherent problems associated with the comparison of topological functions of families of networks that differ in density. In particular, we show that a wide range of topological summaries, such as global efficiency and network modularity are highly sensitive to differences in density. Moreover, these problems are not restricted to unweighted metrics, as we demonstrate that the same issues remain present when considering the weighted versions of these metrics. We conclude by encouraging caution, when reporting such statistical comparisons, and by emphasizing the importance of constructing summary networks.},
author = {Ginestet, Cedric E. and Fournel, Arnaud P. and Simmons, Andrew},
file = {:Users/ricardo/Downloads/fncom-08-00051.pdf:pdf},
journal = {Front. Comput. Neurosci.},
keywords = {N-back,density-integrated metrics,n-back,networks,small-world topology,spn,statistical parametric network,statistical parametric network (SPN),weighted density,working memory},
number = {May},
pages = {1--10},
title = {{Statistical Network Analysis for Functional MRI: Summary Networks and Group Comparisons}},
volume = {8},
year = {2013}
}
@article{Belilovsky,
abstract = {We consider structure discovery of undirected graphical models from observational data. Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures. Popular methods rely on estimating a penalized maximum likelihood of the precision matrix. However, in these approaches structure recovery is an indirect consequence of the data-fit term, the penalty can be difficult to adapt for domain-specific knowledge, and the inference is computationally demanding. By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties. We propose here to leverage this latter source of information as training data to learn a function, parametrized by a neural network that maps empirical covariance matrices to estimated graph structures. Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood. Applying this framework, we find our learnable graph-discovery method trained on synthetic data generalizes well: identifying relevant edges in both synthetic and real data, completely unknown at training time. We find that on genetics, brain imaging, and simulation data we obtain performance generally superior to analytical methods.},
archivePrefix = {arXiv},
arxivId = {1605.06359},
author = {Belilovsky, Eugene and Kastner, Kyle and Varoquaux, Ga{\"{e}}l and Blaschko, Matthew},
eprint = {1605.06359},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Belilovsky et al. - Unknown - Learning to Discover Sparse Graphical Models.pdf:pdf},
issn = {1938-7228},
title = {{Learning to Discover Sparse Graphical Models}},
url = {https://arxiv.org/pdf/1605.06359.pdf http://arxiv.org/abs/1605.06359},
year = {2016}
}
@article{Dmochowski2012,
abstract = {Recent evidence from functional magnetic resonance imaging suggests that cortical hemodynamic responses coincide in different subjects experiencing a common naturalistic stimulus. Here we utilize neural responses in the electroencephalogram (EEG) evoked by multiple presentations of short film clips to index brain states marked by high levels of correlation within and across subjects. We formulate a novel signal decomposition method which extracts maximally correlated signal components from multiple EEG records. The resulting components capture correlations down to a one-second time resolution, thus revealing that peak correlations of neural activity across viewings can occur in remarkable correspondence with arousing moments of the film. Moreover, a significant reduction in neural correlation occurs upon a second viewing of the film or when the narrative is disrupted by presenting its scenes scrambled in time. We also probe oscillatory brain activity during periods of heightened correlation, and observe during such times a significant increase in the theta band for a frontal component and reductions in the alpha and beta frequency bands for parietal and occipital components. Low-resolution EEG tomography of these components suggests that the correlated neural activity is consistent with sources in the cingulate and orbitofrontal cortices. Put together, these results suggest that the observed synchrony reflects attention- and emotion-modulated cortical processing which may be decoded with high temporal resolution by extracting maximally correlated components of neural activity.},
author = {Dmochowski, Jacek P. and Sajda, Paul and Dias, Joao and Parra, Lucas C.},
file = {:Users/ricardo/Downloads/fnhum-06-00112.pdf:pdf},
journal = {Front. Hum. Neurosci.},
keywords = {attention,brain reading,canonical correlation analysis,electroencephalography,engagement},
pages = {1--9},
title = {{Correlated Components of Ongoing EEG Point to Emotionally Laden Attention – A Possible Marker of Engagement?}},
volume = {6},
year = {2012}
}
@article{Gretton2005a,
abstract = {We introduce two new functionals, the constrained covariance and the kernel mutual information, to measure the degree of independence of random variables. These quantities are both based on the covariance between functions of the random variables in reproducing kernel Hilbert spaces (RKHSs). We prove that when the RKHSs are universal, both functionals are zero if and only if the random variables are pairwise independent. We also show that the kernel mutual information is an upper bound near independence on the Parzen window estimate of the mutual information. Analogous results apply for two correlation-based dependence functionals introduced earlier: we show the kernel canonical correlation and the kernel generalised variance to be independence measures for universal kernels, and prove the latter to be an upper bound on the mutual information near independence. The performance of the kernel dependence functionals in measuring independence is verified in the context of independent component analysis.},
author = {Gretton, A. and Herbrich, R. and Smola, A. J. and Bousquet, Olivier and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gretton ARTHUR et al. - 2005 - Kernel Methods for Measuring Independence.pdf:pdf},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {Parzen window esti-mate,covariance operator,independence,independent component analysis,kernel,mutual information},
pages = {2075--2129},
title = {{Kernel Methods for Measuring Independence}},
url = {http://www.jmlr.org/papers/volume6/gretton05a/gretton05a.pdf http://eprints.pascal-network.org/archive/00001702/},
volume = {6},
year = {2005}
}
@incollection{Zhang,
author = {Zhang, Kun and Hyv{\"{a}}rinen, Aapo},
booktitle = {Stat. Causality Methods Appl. Empir. Res.},
doi = {10.1002/9781118947074.ch8},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Hyv{\"{a}}rinen - Unknown - NONLINEAR FUNCTIONAL CAUSAL MODELS FOR DISTINGUISHING CAUSE FROM EFFECT(2).pdf:pdf},
isbn = {9781118947074},
keywords = {causal discovery,nonlinear additive noise model,post‐nonlinear causal model},
pages = {185--201},
title = {{Nonlinear Functional Causal Models for Distinguishing Cause from Effect}},
url = {https://www.cs.helsinki.fi/u/ahyvarin/papers/Zhang16.pdf http://doi.wiley.com/10.1002/9781118947074.ch8},
year = {2016}
}
@article{Anandkumar2012,
abstract = {Unsupervised estimation of latent variable models is a fundamental problem central to numerous applications of machine learning and statistics. This work presents a principled approach for estimating broad classes of such models, including probabilistic topic models and latent linear Bayesian networks, using only second-order observed moments. The sufficient conditions for identifiability of these models are primarily based on weak expansion constraints on the topic-word matrix, for topic models, and on the directed acyclic graph, for Bayesian networks. Because no assumptions are made on the distribution among the latent variables, the approach can handle arbitrary correlations among the topics or latent factors. In addition, a tractable learning method via {\$}\backslashell{\_}1{\$} optimization is proposed and studied in numerical experiments.},
archivePrefix = {arXiv},
arxivId = {1209.5350},
author = {Anandkumar, Animashree and Hsu, Daniel and Javanmard, Adel and Kakade, Sham M},
eprint = {1209.5350},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Anandkumar et al. - 2013 - Learning Topic Models and Latent Bayesian Networks Under Expansion Constraints.pdf:pdf},
title = {{Learning Topic Models and Latent Bayesian Networks Under Expansion Constraints}},
url = {https://arxiv.org/pdf/1209.5350.pdf http://arxiv.org/abs/1209.5350},
year = {2012}
}
@article{Shalev-Shwartz2017,
abstract = {In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four families of problems for which some of the commonly used existing algorithms fail or suffer significant difficulty. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.},
archivePrefix = {arXiv},
arxivId = {1703.07950},
author = {Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
eprint = {1703.07950},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Shalev-Shwartz, Shamir, Shammah - Unknown - Failures of Gradient-Based Deep Learning.pdf:pdf},
issn = {1938-7228},
journal = {arXiv Prepr.},
number = {07950},
pages = {1--24},
title = {{Failures of Deep Learning}},
url = {https://arxiv.org/pdf/1703.07950.pdf http://arxiv.org/abs/1703.07950},
volume = {1703},
year = {2017}
}
@article{Hyvarinen2010,
abstract = {Analysis of causal effects between continuous-valued variables typically uses either autoregressive models or structural equation models with instantaneous effects. Estimation of Gaussian, linear structural equation models poses serious identifiability problems, which is why it was recently pro-posed to use non-Gaussian models. Here, we show how to combine the non-Gaussian instantaneous model with autoregressive models. This is effectively what is called a structural vector autoregres-sion (SVAR) model, and thus our work contributes to the long-standing problem of how to estimate SVAR's. We show that such a non-Gaussian model is identifiable without prior knowledge of net-work structure. We propose computationally efficient methods for estimating the model, as well as methods to assess the significance of the causal influences. The model is successfully applied on financial and brain imaging data.},
author = {Hyv{\"{a}}rinen, Aapo and Zhang, Kun and Shimizu, Shohei and Hoyer, Patrik O},
doi = {10.1016/j.ijhydene.2010.06.032},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Hyv{\"{a}}rinen et al. - 2010 - Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {causality,independent component analysis,non-Gaussianity,structural equation models,structural vector autoregression},
pages = {1709--1731},
title = {{Estimation of a Structural Vector Autoregression Model Using Non-Gaussianity}},
url = {http://www.jmlr.org/papers/volume11/hyvarinen10a/hyvarinen10a.pdf},
volume = {11},
year = {2010}
}
@article{Gutmann2011,
abstract = {We show that the Bregman divergence provides a rich framework to estimate unnormalized statistical models for continuous or discrete random variables, that is, models which do not integrate or sum to one, respectively. We prove that recent estimation methods such as noise-contrastive estimation, ratio matching, and score matching belong to the proposed framework, and explain their interconnection based on supervised learning. Further, we discuss the role of boosting in unsupervised learning.},
author = {Gutmann, Michael and Hirayama, Jun-Ichiro},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gutmann, Hirayama - Unknown - Bregman divergence as general framework to estimate unnormalized statistical models.pdf:pdf},
isbn = {978-0-9749039-7-2},
journal = {Proc. Twenty Seventh Conf. Annu. Conf. Uncertain. Artif. Intell. UAI},
number = {1},
pages = {283----290},
title = {{Bregman divergence as general framework to estimate unnormalized statistical models}},
year = {2011}
}
@article{Gatys2017,
abstract = {The eye fixation patterns of human observers are a fundamental indicator of the aspects of an image to which humans attend. Thus, manipulating fixation patterns to guide human attention is an exciting challenge in digital image processing. Here, we present a new model for manipulating images to change the distribution of human fixations in a controlled fashion. We use the state-of-the-art model for fixation prediction to train a convolutional neural network to transform images so that they satisfy a given fixation distribution. For network training, we carefully design a loss function to achieve a perceptual effect while preserving naturalness of the transformed images. Finally, we evaluate the success of our model by measuring human fixations for a set of manipulated images. On our test images we can in-/decrease the probability to fixate on selected objects on average by 43/22{\%} but show that the effectiveness of the model depends on the semantic content of the manipulated images.},
archivePrefix = {arXiv},
arxivId = {1712.06492},
author = {Gatys, Leon A and K{\"{u}}mmerer, Matthias and Wallis, Thomas S A and Bethge, Matthias},
eprint = {1712.06492},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gatys et al. - 2017 - Guiding human gaze with convolutional neural networks(2).pdf:pdf},
title = {{Guiding human gaze with convolutional neural networks}},
url = {https://arxiv.org/pdf/1712.06492v1.pdf http://arxiv.org/abs/1712.06492},
year = {2017}
}
@article{Li,
abstract = {Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.},
archivePrefix = {arXiv},
arxivId = {1511.07543},
author = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
eprint = {1511.07543},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - CONVERGENT LEARNING DO DIFFERENT NEURAL NETWORKS LEARN THE SAME REPRESENTATIONS.pdf:pdf},
isbn = {9789036774123},
title = {{Convergent Learning: Do different neural networks learn the same representations?}},
url = {https://arxiv.org/pdf/1511.07543.pdf http://arxiv.org/abs/1511.07543},
year = {2015}
}
@article{Chen2015,
abstract = {Multi-subject fMRI data is critical for evaluating the generality and validity of findings across subjects, and its effective utilization helps improve analysis sensitivity. We develop a shared response model for aggregating multi-subject fMRI data that accounts for different functional topographies among anatomically aligned datasets. Our model demonstrates improved sensitivity in identifying a shared response for a variety of datasets and anatomical brain regions of interest. Furthermore, by removing the identified shared response, it allows improved de- tection of group differences. The ability to identify what is shared and what is not shared opens the model to a wide range of multi-subject fMRI studies. 1},
author = {Chen, Po-Hsuan and Chen, Janice and Yeshurun, Yaara and Hasson, Uri and Haxby, James V and Ramadge, Peter J},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - Unknown - A Reduced-Dimension fMRI Shared Response Model.pdf:pdf},
journal = {NIPS},
pages = {460--468},
title = {{A Reduced-Dimension fMRI Shared Response Model}},
year = {2015}
}
@article{Watanabe2013,
abstract = {A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive definite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is defined by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold. Recently, it was proved that the Bayes free energy of a singular model is asymptotically given by a generalized formula using a birational invariant, the real log canonical threshold (RLCT), instead of half the number of parameters in BIC. Theoretical values of RLCTs in several statistical models are now being discovered based on algebraic geometrical methodology. However, it has been difficult to estimate the Bayes free energy using only training samples, because an RLCT depends on an unknown true distribution. In the present paper, we define a widely applicable Bayesian information criterion (WBIC) by the average log likelihood function over the posterior distribution with the inverse temperature {\$}1/\backslashlog n{\$}, where {\$}n{\$} is the number of training samples. We mathematically prove that WBIC has the same asymptotic expansion as the Bayes free energy, even if a statistical model is singular for and unrealizable by a statistical model. Since WBIC can be numerically calculated without any information about a true distribution, it is a generalized version of BIC onto singular statistical models.},
archivePrefix = {arXiv},
arxivId = {1208.6338},
author = {Watanabe, Sumio},
eprint = {1208.6338},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Watanabe, Jp - 2013 - A Widely Applicable Bayesian Information Criterion.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {J. Mach. Learn. Res.},
keywords = {Bayes marginal likelihood,widely applicable Bayes information criterion},
pages = {867--897},
title = {{A Widely Applicable Bayesian Information Criterion}},
url = {http://www.jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf http://arxiv.org/abs/1208.6338},
volume = {14},
year = {2012}
}
@article{Blobaum2016,
abstract = {It is generally difficult to make any statements about the expected prediction error in an univariate setting without further knowledge about how the data were generated. Recent work showed that knowledge about the real underlying causal structure of a data generation process has implications for various machine learning settings. Assuming an additive noise and an independence between data generating mechanism and its input, we draw a novel connection between the intrinsic causal relationship of two variables and the expected prediction error. We formulate the theorem that the expected error of the true data generating function as prediction model is generally smaller when the effect is predicted from its cause and, on the contrary, greater when the cause is predicted from its effect. The theorem implies an asymmetry in the error depending on the prediction direction. This is further corroborated with empirical evaluations in artificial and real-world data sets.},
archivePrefix = {arXiv},
arxivId = {1610.03263},
author = {Bl{\"{o}}baum, Patrick and Washio, Takashi and Shimizu, Shohei},
doi = {10.1007/s41237-017-0022-z},
eprint = {1610.03263},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bl{\"{o}}baum, Washio, Shimizu - Unknown - Error asymmetry in causal and anticausal regression.pdf:pdf},
issn = {0385-7417},
journal = {Behaviormetrika},
keywords = {Calibration,Causal and anticausal prediction,Causality,Error asymmetry,Inverse prediction,Prediction error},
title = {{Error Asymmetry in Causal and Anticausal Regression}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs41237-017-0022-z.pdf http://arxiv.org/abs/1610.03263{\%}0Ahttp://dx.doi.org/10.1007/s41237-017-0022-z},
volume = {44},
year = {2016}
}
@article{Benidis2016,
abstract = {The problem of estimating sparse eigenvectors of a symmetric matrix attracts a lot of attention in many applications, especially those with high dimensional data set. While classical eigenvectors can be obtained as the solution of a maximization problem, existing approaches formulate this problem by adding a penalty term into the objective function that encourages a sparse solution. However, the resulting methods achieve sparsity at the expense of sacrificing the orthogonality property. In this paper, we develop a new method to estimate dominant sparse eigenvectors without trading off their orthogonality. The problem is highly non-convex and hard to handle. We apply the MM framework where we iteratively maximize a tight lower bound (surrogate function) of the objective function over the Stiefel manifold. The inner maximization problem turns out to be a rectangular Procrustes problem, which has a closed form solution. In addition, we propose a method to improve the covariance estimation problem when its underlying eigenvectors are known to be sparse. We use the eigenvalue decomposition of the covariance matrix to formulate an optimization problem where we impose sparsity on the corresponding eigenvectors. Numerical experiments show that the proposed eigenvector extraction algorithm matches or outperforms existing algorithms in terms of support recovery and explained variance, while the covariance estimation algorithms improve significantly the sample covariance estimator.},
archivePrefix = {arXiv},
arxivId = {1602.03992},
author = {Benidis, Konstantinos and Sun, Ying and Babu, Prabhu and Palomar, Daniel P.},
doi = {10.1109/TSP.2016.2605073},
eprint = {1602.03992},
file = {:Users/ricardo/Downloads/07558183.pdf:pdf},
isbn = {1053-587X VO - 64},
issn = {1053587X},
journal = {IEEE Trans. Signal Process.},
keywords = {Covariance estimation,minorization-maximization,procrustes,sparse PCA,stiefel manifold},
number = {23},
pages = {6211--6226},
title = {{Orthogonal Sparse PCA and Covariance Estimation via Procrustes Reformulation}},
volume = {64},
year = {2016}
}
@article{Huang2017a,
abstract = {{\textcopyright} 2017 IEEE. We address two important issues in causal discovery from nonstationary or heterogeneous data, where parameters associated with a causal structure may change over time or across data sets. First, we investigate how to efficiently estimate the 'driving force' of the nonstationarity of a causal mechanism. That is, given a causal mechanism that varies over time or across data sets and whose qualitative structure is known, we aim to extract from data a low-dimensional and interpretable representation of the main components of the changes. For this purpose we develop a novel kernel embedding of nonstationary conditional distributions that does not rely on sliding windows. Second, the embedding also leads to a measure of dependence between the changes of causal modules that can be used to determine the directions of many causal arrows. We demonstrate the power of our methods with experiments on both synthetic and real data.},
author = {Huang, Biwei and Zhang, Kun and Zhang, Jiji and Sanchez-Romero, Ruben and Glymour, Clark and Sch{\"{o}}lkopf, Bernhard},
doi = {10.1109/ICDM.2017.114},
file = {:Users/ricardo/Downloads/Mining Driving Forces of Changes and Causal Arrows.pdf:pdf},
isbn = {9781538638347},
issn = {15504786},
journal = {Proc. - IEEE Int. Conf. Data Mining, ICDM},
keywords = {Causal discovery,Data distribution shift,Kernel mean embedding,Nonstationary driving force},
pages = {913--918},
title = {{Behind distribution shift: Mining driving forces of changes and causal arrows}},
volume = {2017-Novem},
year = {2017}
}
@article{Frassle2018,
abstract = {The development of whole-brain models that can infer effective (directed) connection strengths from fMRI data represents a central challenge for computational neuroimaging. A recently introduced generative model of fMRI data, regression dynamic causal modeling (rDCM), moves towards this goal as it scales gracefully to very large networks. However, large-scale networks with thousands of connections are difficult to interpret; additionally, one typically lacks information (data points per free parameter) for precise estimation of all model parameters. This paper introduces sparsity constraints to the variational Bayesian framework of rDCM as a solution to these problems in the domain of task-based fMRI. This sparse rDCM approach enables highly efficient effective connectivity analyses in whole-brain networks and does not require a priori assumptions about the network's connectivity structure but prunes fully (all-to-all) connected networks as part of model inversion. Following the derivation of the variational Bayesian update equations for sparse rDCM, we use both simulated and empirical data to assess the face validity of the model. In particular, we show that it is feasible to infer effective connection strengths from fMRI data using a network with more than 100 regions and 10,000 connections. This demonstrates the feasibility of whole-brain inference on effective connectivity from fMRI data – in single subjects and with a run-time below 1 min when using parallelized code. We anticipate that sparse rDCM may find useful application in connectomics and clinical neuromodeling – for example, for phenotyping individual patients in terms of whole-brain network structure.},
author = {Fr{\"{a}}ssle, Stefan and Lomakina, Ekaterina I. and Kasper, Lars and Manjaly, Zina M. and Leff, Alex and Pruessmann, Klaas P. and Buhmann, Joachim M. and Stephan, Klaas E.},
doi = {10.1016/j.neuroimage.2018.05.058},
file = {:Users/ricardo/Downloads/1-s2.0-S1053811918304762-main.pdf:pdf},
isbn = {4971168587824},
issn = {10538119},
journal = {Neuroimage},
number = {December 2017},
pages = {505--529},
pmid = {25653388},
title = {{A generative model of whole-brain effective connectivity}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811918304762},
volume = {179},
year = {2018}
}
@article{Bickel,
abstract = {Prompted by the increasing interest in networks in many fields, we present an attempt at unifying points of view and analyses of these objects coming from the social sciences, statistics, probability and physics communities. We apply our approach to the Newman-Girvan modularity, widely used for "community" detection, among others. Our analysis is asymptotic but we show by simulation and application to real examples that the theory is a reasonable guide to practice.},
author = {Bickel, Peter J and Chen, Aiyou},
doi = {10.1073/pnas.0907096106},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bickel, Chen, Fienberg - Unknown - A nonparametric view of network models and Newman–Girvan and other modularities.pdf:pdf},
issn = {0027-8424},
journal = {Proc. Natl. Acad. Sci.},
number = {50},
pages = {21068--21073},
pmid = {19934050},
title = {{A nonparametric view of network models and Newman–Girvan and other modularities}},
url = {http://www.pnas.org/content/106/50/21068.full.pdf http://www.pnas.org/lookup/doi/10.1073/pnas.0907096106},
volume = {106},
year = {2009}
}
@article{Shimizu2008,
abstract = {Many methods have been proposed for discovery of causal relations among observed variables. But one often wants to discover causal relations among latent factors rather than observed variables. Some methods have been proposed to estimate linear acyclic models for latent factors that are measured by observed variables. However, most of the methods use data covariance structure alone for model identification, and this leads to a number of indistinguishable models. In this paper, we show that a linear acyclic model for latent factors is identifiable when the data are non-Gaussian. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Shimizu, Shohei and Hoyer, Patrik O and Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Shimizu, Hoyer, Hyv{\"{a}}rinen - 2009 - Estimation of linear non-Gaussian acyclic models for latent factors.pdf:pdf},
journal = {Neurocomputing},
keywords = {Causal analysis,Independent component analysis,Latent factors},
number = {7-9},
pages = {2024--2027},
title = {{Estimation of linear non-Gaussian acyclic models for latent factors}},
volume = {72},
year = {2009}
}
@article{Paatero1994,
abstract = {A new variant 'PMF' of factor analysis is described. It is assumed that X is a matrix of observed data and sigma is the known matrix of standard deviations of elements of X. Both X and sigma are of dimensions n x m. The method solves the bilinear matrix problem X = GF + E where G is the unknown left hand factor matrix (scores) of dimensions n x p, F is the unknown right hand factor matrix (loadings) of dimensions p x m, and E is the matrix of residuals. The problem is solved in the weighted least squares sense: G and F are determined so that the Frobenius norm of E divided (element-by-element) by sigma is minimized. Furthermore, the solution is constrained so that all the elements of G and F are required to be non-negative. It is shown that the solutions by PMF are usually different from any solutions produced by the customary factor analysis (FA, i.e. principal component analysis (PCA) followed by rotations). Usually PMF produces a better fit to the data than FA. Also, the result of PF is guaranteed to be non-negative, while the result of FA often cannot be rotated so that all negative entries would be eliminated. Different possible application areas of the new method are briefly discussed. In environmental data, the error estimates of data can be widely varying and non-negativity is often an essential feature of the underlying models. Thus it is concluded that PMF is better suited than FA or PCA in many environmental applications. Examples of successful applications of PMF are shown in companion papers.},
author = {Paatero, Pentti and Tapper, Unto},
file = {:Users/ricardo/Downloads/Paatero{\_}et{\_}al-1994-Environmetrics.pdf:pdf},
journal = {Environmetrics},
keywords = {Alternating regression,Error estimates,Factor analysis,Principal component analysis,Repetitive measurements,Scaling,Weighted least squares},
number = {2},
pages = {111--126},
title = {{Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values}},
volume = {5},
year = {1994}
}
@article{Papernot,
abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24{\%} of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19{\%} and 88.94{\%}. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
archivePrefix = {arXiv},
arxivId = {1602.02697},
author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
doi = {10.1145/3052973.3053009},
eprint = {1602.02697},
file = {:Users/ricardo/Downloads/1602.02697.pdf:pdf},
isbn = {9781450349444},
title = {{Practical Black-Box Attacks against Machine Learning}},
url = {https://arxiv.org/pdf/1602.02697.pdf http://arxiv.org/abs/1602.02697},
year = {2016}
}
@article{Bottou2010,
abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
author = {Bottou, Le{\'{o}}n},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bottou - Unknown - Large-Scale Machine Learning with Stochastic Gradient Descent.pdf:pdf},
isbn = {0269-2155},
issn = {0269-2155},
journal = {Proc. COMPSTAT'2010},
keywords = {efficiency,online learning,stochastic gradient descent},
pages = {177--186},
title = {{Large-Scale Machine Learning with Stochastic Gradient Descent}},
year = {2010}
}
@article{Weber2017,
abstract = {Previous research has provided qualitative evidence for overlap in a number of brain regions across the subjective value network (SVN) and the default mode network (DMN). In order to quantitatively assess this overlap, we conducted a series of coordinate-based meta-analyses (CBMA) of results from 466 functional magnetic resonance imaging experiments on task-negative or subjective value-related activations in the human brain. In these analyses, we first identified significant overlaps and dissociations across activation foci related to SVN and DMN. Second, we investigated whether these overlapping subregions also showed similar patterns of functional connectivity, suggesting a shared functional subnetwork. We find considerable overlap between SVN and DMN in subregions of central ventromedial prefrontal cortex (cVMPFC) and dorsal posterior cingulate cortex (dPCC). Further, our findings show that similar patterns of bidirectional functional connectivity between cVMPFC and dPCC are present in both networks. We discuss ways in which our understanding of how subjective value (SV) is computed and represented in the brain can be synthesized with what we know about the DMN, mind-wandering, and self-referential processing in light of our findings.},
author = {Weber, Bernd and Smith, David V and Kahnt, Thorsten and {Yavuz Acikalin}, M and Gorgolewski, Krzysztof J and Poldrack, Russell A},
doi = {10.3389/fnins.2017.00001},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Weber et al. - 2017 - A Coordinate-Based Meta-Analysis of Overlaps in Regional Specialization and Functional Connectivity across Subject.pdf:pdf},
journal = {Front. Neurosci.},
keywords = {decision making,default mode network,fMRI BOLD,metaanalysis,subjective value},
number = {111},
pages = {3389--1},
title = {{A Coordinate-Based Meta-Analysis of Overlaps in Regional Specialization and Functional Connectivity across Subjective Value and Default Mode Networks}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5243799/pdf/fnins-11-00001.pdf},
volume = {11},
year = {2017}
}
@article{Thompson2017,
abstract = {Network neuroscience has become an established paradigm to tackle questions related to the functional and structural connectome of the brain. Recently, interest has been growing in examining the temporal dynamics of the brain's network activity. Although different approaches to capturing fluctuations in brain connectivity have been proposed, there have been few attempts to quantify these fluctuations using temporal network theory. This theory is an extension of network theory that has been successfully applied to the modeling of dynamic processes in economics, social sciences, and engineering article but it has not been adopted to a great extent within network neuroscience. The objective of this article is twofold: (i) to present a detailed description of the central tenets of temporal network theory and describe its measures, and; (ii) to apply these measures to a resting-state fMRI dataset to illustrate their utility. Furthermore, we discuss the interpretation of temporal network theory in the context o...},
author = {Thompson, William Hedley and Brantefors, Per and Fransson, Peter},
doi = {10.1162/NETN_a_00011},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Thompson, Brantefors, Fransson - Unknown - From static to temporal network theory Applications to functional brain connectivity.pdf:pdf},
issn = {2472-1751},
journal = {Netw. Neurosci.},
keywords = {Dynamic functional connectivity,Functional connectome,Resting-state,Temporal network theory,Temporal networks},
number = {2},
pages = {69--99},
title = {{From static to temporal network theory: Applications to functional brain connectivity}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/NETN{\_}a{\_}00011 http://www.mitpressjournals.org/doi/abs/10.1162/NETN{\_}a{\_}00011},
volume = {1},
year = {2017}
}
@article{Rosset2007,
abstract = {We consider the generic regularized optimization proble $\beta$($\lambda$) = arg min $\beta$ L(y, X$\beta$) + $\lambda$J ($\beta$). Efron, Hastie, Johnstone and Tibshirani [Ann. Statist. 32 (2004) 407–499] have shown that for the LASSO—that is, if L is squared error loss and J ($\beta$) = =$\beta$ 1 is the 1 norm of $\beta$—the optimal coef-ficient path is piecewise linear, that is, $\beta$($\lambda$)/∂$\lambda$ is piecewise constant. We derive a general characterization of the properties of (loss L, penalty J) pairs which give piecewise linear coefficient paths. Such pairs allow for efficient generation of the full regularized coefficient paths. We investigate the na-ture of efficient path following algorithms which arise. We use our results to suggest robust versions of the LASSO for regression and classification, and to develop new, efficient algorithms for existing problems in the literature, including Mammen and van de Geer's locally adaptive regression splines. 1. Introduction. Regularization is an essential component in modern data analysis, in particular when the number of predictors is large, possibly larger than the number of observations, and nonregularized fitting is likely to give badly over-fitted and useless models. In this paper we consider the generic regularized optimization problem. The inputs we have are:},
author = {Rosset, Saharon and Zhu, Ji},
doi = {10.1214/009053606000001370},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Rosset, Zhu - 2007 - PIECEWISE LINEAR REGULARIZED SOLUTION PATHS(2).pdf:pdf},
journal = {Ann. Stat.},
number = {3},
pages = {1012--1030},
title = {{Piecewise linear regularized solution paths}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aos/1185303996},
volume = {35},
year = {2007}
}
@article{Bottou2013,
abstract = {This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the sys-tem. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.},
archivePrefix = {arXiv},
arxivId = {1209.2355},
author = {Bottou, L{\'{e}}on and Peters, Jonas and Ch, Peters and Qui{\~{n}}onero-Candela, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
doi = {10.1145/2740908.2742564},
eprint = {1209.2355},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bottou et al. - 2013 - Counterfactual Reasoning and Learning Systems The Example of Computational Advertising(2).pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {J. Mach. Learn. Res.},
keywords = {causation,computational advertising,counterfactual reasoning},
pages = {3207--3260},
title = {{Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising}},
url = {http://jmlr.org/papers/volume14/bottou13a/bottou13a.pdf},
volume = {14},
year = {2013}
}
@article{Anderson1992,
author = {Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
file = {:Users/ricardo/Downloads/hyperGrad{\_}deepLearning.pdf:pdf},
journal = {ICLR},
title = {{Online learning rate adaptation with hypergradient descent}},
year = {2018}
}
@article{GuneBaydin2018,
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "autodiff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
archivePrefix = {arXiv},
arxivId = {1502.05767},
author = {Baydin, Atilim Gunes and Pearlmutter, Barak A and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
eprint = {1502.05767},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/G{\"{u}}ne Baydin et al. - 2018 - Automatic Differentiation in Machine Learning a Survey.pdf:pdf},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
keywords = {Backpropagation,Differentiable Programming},
pages = {1--43},
title = {{Automatic differentiation in machine learning: a survey}},
url = {http://jmlr.org/papers/v18/17-468.html. http://arxiv.org/abs/1502.05767},
volume = {18},
year = {2015}
}
@article{Bauer2016,
abstract = {We prove that a time series satisfying a (linear) multivariate autoregressive moving average (VARMA) model satisfies the same model assumption in the reversed time direction, too, if all innovations are normally distributed. This reversibility breaks down if the innovations are non-Gaussian. This means that under the assumption of a VARMA process with non-Gaussian noise, the arrow of time becomes detectable. Our work thereby provides a theoretic justification of an algorithm that has been used for inferring the direction of video snippets. We present a slightly modified practical algorithm that estimates the time direction for a given sample and prove its consistency. We further investigate how the performance of the algorithm depends on sample size, number of dimensions of the time series and the order of the process. An application to real world data from economics shows that considering multivariate processes instead of univariate processes can be beneficial for estimating the time direction. Our result extends earlier work on univariate time series. It relates to the concept of causal inference, where recent methods exploit non-Gaussianity of the error terms for causal structure learning.},
archivePrefix = {arXiv},
arxivId = {1603.00784},
author = {Bauer, Stefan and Sch{\"{o}}lkopf, Bernhard and Peters, Jonas},
eprint = {1603.00784},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bauer STEFANBAUER, Sch{\"{o}}lkopf, Jonas Peters JONASPETERS - Unknown - The Arrow of Time in Multivariate Time Series.pdf:pdf},
isbn = {9781510829008},
journal = {Icml},
pages = {1--18},
title = {{The Arrow of Time in Multivariate Time Series}},
url = {http://proceedings.mlr.press/v48/bauer16.pdf http://arxiv.org/abs/1603.00784},
year = {2016}
}
@article{Winkler2015,
abstract = {Under weak and reasonable assumptions, mainly that data are exchangeable under the null hypothesis, permutation tests can provide exact control of false positives and allow the use of various non-standard statistics. There are, however, various common examples in which global exchangeability can be violated, including paired tests, tests that involve repeated measurements, tests in which subjects are relatives (members of pedigrees) - any dataset with known dependence among observations. In these cases, some permutations, if performed, would create data that would not possess the original dependence structure, and thus, should not be used to construct the reference (null) distribution. To allow permutation inference in such cases, we test the null hypothesis using only a subset of all otherwise possible permutations, i.e., using only the rearrangements of the data that respect exchangeability, thus retaining the original joint distribution unaltered. In a previous study, we defined exchangeability for blocks of data, as opposed to each datum individually, then allowing permutations to happen within block, or the blocks as a whole to be permuted. Here we extend that notion to allow blocks to be nested, in a hierarchical, multi-level definition. We do not explicitly model the degree of dependence between observations, only the lack of independence; the dependence is implicitly accounted for by the hierarchy and by the permutation scheme. The strategy is compatible with heteroscedasticity and variance groups, and can be used with permutations, sign flippings, or both combined. We evaluate the method for various dependence structures, apply it to real data from the Human Connectome Project (HCP) as an example application, show that false positives can be avoided in such cases, and provide a software implementation of the proposed approach.},
author = {Winkler, Anderson M and Webster, Matthew A and Vidaurre, Diego and Nichols, Thomas E and Smith, Stephen M},
doi = {10.1016/j.neuroimage.2015.05.092},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Winkler et al. - 2015 - Multi-level block permutation.pdf:pdf},
isbn = {1095-9572; 1053-8119},
issn = {10959572},
journal = {Neuroimage},
keywords = {General linear model,Multiple regression,Permutation inference,Repeated measurements},
pages = {253--268},
pmid = {26074200},
title = {{Multi-level block permutation}},
url = {https://ac.els-cdn.com/S105381191500508X/1-s2.0-S105381191500508X-main.pdf?{\_}tid=ae48481c-07fe-11e8-82a9-00000aab0f02{\&}acdnat=1517565286{\_}67034b1cca097745c54850da58cf84e6},
volume = {123},
year = {2015}
}
@article{Pascanu2012,
abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {1211.5063},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {1211.5063},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Pascanu, Mikolov, Bengio - Unknown - On the difficulty of training recurrent neural networks.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
journal = {ICML},
pmid = {18267787},
title = {{On the difficulty of training Recurrent Neural Networks}},
url = {http://proceedings.mlr.press/v28/pascanu13.pdf http://arxiv.org/abs/1211.5063},
year = {2012}
}
@article{Boutsidis2008,
abstract = {We describe Nonnegative Double Singular Value Decomposition (NNDSVD), a new method designed to enhance the initialization stage of nonnegative matrix factorization (NMF). NNDSVD can readily be combined with existing NMF algorithms. The basic algorithm contains no randomization and is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. Simple practical variants for NMF with dense factors are described. NNDSVD is also well suited to initialize NMF algorithms with sparse factors. Many numerical examples suggest that NNDSVD leads to rapid reduction of the approximation error of many NMF algorithms. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Boutsidis, C. and Gallopoulos, E.},
doi = {10.1016/j.patcog.2007.09.010},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognit.},
keywords = {Low rank,NMF,Nonnegative matrix factorization,Perron-Frobenius,SVD,Singular value decomposition,Sparse NMF,Sparse factorization,Structured initialization},
number = {4},
pages = {1350--1362},
title = {{SVD based initialization: A head start for nonnegative matrix factorization}},
volume = {41},
year = {2008}
}
@article{Lin2018,
abstract = {Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9{\%} of the gradient exchange in distributed SGD are redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communi-cation bandwidth. To preserve accuracy during this compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270× to 600× without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.},
archivePrefix = {arXiv},
arxivId = {1712.01887},
author = {Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
eprint = {1712.01887},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - Unknown - DEEP GRADIENT COMPRESSION REDUCING THE COMMUNICATION BANDWIDTH FOR DISTRIBUTED TRAINING.pdf:pdf},
pages = {1--13},
title = {{Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training}},
url = {https://arxiv.org/pdf/1712.01887.pdf},
year = {2018}
}
@article{Chaudhari,
abstract = {Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such "out-of-equilibrium" behavior is a consequence of the fact that the gradient noise in SGD is highly non-isotropic; the covariance matrix of mini-batch gradients has a rank as small as 1{\%} of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.},
archivePrefix = {arXiv},
arxivId = {1710.11029},
author = {Chaudhari, Pratik and Soatto, Stefano},
eprint = {1710.11029},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Chaudhari, Soatto - 2017 - Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks.pdf:pdf},
keywords = {Fokker-Planck equation,Markov chain Monte Carlo,Wasserstein metric,deep networks,gradient noise,out-of-equilibrium,stochastic gradient descent,thermodynamics,variational inference,wide minima},
title = {{Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks}},
url = {https://arxiv.org/pdf/1710.11029.pdf http://arxiv.org/abs/1710.11029},
year = {2017}
}
@article{Hyvarinen2017,
author = {Hyv{\"{a}}rinen, Aapo and Turner, Richard E},
file = {:Users/ricardo/Downloads/newnica1.pdf:pdf},
number = {2},
pages = {1--12},
title = {{A general framework for nonlinear ICA using auxiliary variables and contrastive learning}},
year = {2017}
}
@inproceedings{Mackey2008,
abstract = {In analogy to the PCA setting, the sparse PCA problem is often solved by iter- atively alternating between two subtasks: cardinality-constrained rank-one vari- ance maximization and matrix deflation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justification from the PCA context. In this work, we demon- strate that the standard PCA deflation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we first develop several deflation al- ternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCAoptimization problemto explicitly reflect themaximumadditional variance objective on each round. The result is a generalized deflation procedure that typically outperforms more standard techniques on real-world datasets.},
author = {Mackey, Lester},
booktitle = {Adv. Neural Inf. Process. Syst.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Mackey - Unknown - Deflation Methods for Sparse PCA.pdf:pdf},
isbn = {9781605609492},
pages = {1017--1024},
title = {{Deflation Methods for Sparse PCA.}},
url = {http://papers.nips.cc/paper/3575-deflation-methods-for-sparse-pca.pdf https://papers.nips.cc/paper/3575-deflation-methods-for-sparse-pca.pdf},
year = {2008}
}
@article{Oja2003,
abstract = {The instantaneous noise-free linear mixing model in inde- pendent component analysis is largely a solved problemun- der the usual assumption of independent nongaussian sources and full rank mixing matrix. However, with some prior in- formation on the sources, like positivity, new analysis and perhaps simplified solutionmethodsmay yet become possi- ble. In this paper, we consider the task of independent com- ponent analysis when the independent sources are known to be non-negative and well-grounded, which means that they have a non-zero pdf in the region of zero. We propose the use of a ‘Non-Negative PCA' algorithm which is a special case of the nonlinear PCA algorithm, but with a rectifica- tion nonlinearity, and we show that this algorithm will find such non-negative well-grounded independent sources. Al- though the algorithm has proved difficult to analyze in the general case, we give an analytical convergence result here, complemented by a numerical simulation which illustrates its operation. 1.},
author = {Oja, Erkki and Plumbley, Mark},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Oja, Plumbley - Unknown - BLIND SEPARATION OF POSITIVE SOURCES USING NON-NEGATIVE PCA.pdf:pdf},
journal = {4th Int. Symp. Indep. Compon. Anal. Blind Signal Sep.},
number = {April},
pages = {11--16},
title = {{Blind Separation of Positive Sources Using Non-Negative Pca}},
url = {https://pdfs.semanticscholar.org/968b/6524ba58e5eafac700b86bf24c0061be400d.pdf},
year = {2003}
}
@article{Tramer,
abstract = {Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase ro-bustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations , rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks (Kurakin et al., 2017c).},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.07204v4},
author = {Tram{\`{e}}r, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and Mcdaniel, Patrick},
eprint = {arXiv:1705.07204v4},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Tram{\`{e}}r et al. - Unknown - ENSEMBLE ADVERSARIAL TRAINING ATTACKS AND DEFENSES.pdf:pdf},
journal = {ICLR},
title = {{Ensemble adversarial training: attacks and defences}},
url = {https://arxiv.org/pdf/1705.07204.pdf?},
year = {2018}
}
@inproceedings{Rubin-Delanchy2016,
abstract = {—Network data is ubiquitous in cyber-security ap-plications. Accurately modelling such data allows discovery of anomalous edges, subgraphs or paths, and is key to many signature-free cyber-security analytics. We present a recurring property of graphs originating from cyber-security applications, often considered a 'corner case' in the main literature on network data analysis, that greatly affects the performance of standard 'off-the-shelf' techniques. This is the property that similarity, in terms of network behaviour, does not imply connectivity, and in fact the reverse is often true. We call this disassortivity. The phenomenon is illustrated using network flow data collected on an enterprise network. Improved procedures are proposed, that take explicit account of this property, for spectral analysis and link prediction.},
author = {Rubin-Delanchy, Patrick and Adams, Niall M and Heard, Nicholas A},
booktitle = {IEEE Int. Conf. Intell. Secur. Informatics Cybersecurity Big Data, ISI 2016},
doi = {10.1109/ISI.2016.7745482},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Rubin-Delanchy, Adams, Heard - Unknown - Disassortativity of computer networks.pdf:pdf},
isbn = {9781509038657},
pages = {243--247},
title = {{Disassortativity of computer networks}},
url = {https://spiral.imperial.ac.uk/bitstream/10044/1/42764/2/dissasortivity.pdf},
year = {2016}
}
@article{Salimi-Khorshidi2011,
abstract = {The purpose of neuroimaging meta-analysis is to lo- calize the brain regions that are activated consistently in response to a certain intervention. As a commonly used technique, current coordinate-based meta-analyses (CBMA) of neuroimaging studies utilize relatively sparse information from published studies, typ- ically only using (x,y,z) coordinates of the activation peaks. Such CBMA methods have several limitations. First, there is no way to jointly incorporate deactivation information when available, which has been shown to result in an inaccurate statistic image when assessing a difference contrast. Second, the scale of a kernel re- flecting spatial uncertainty must be set without taking the effect size (e.g., Z-stat) into account. To address these problems, we em- ploy Gaussian-process regression (GPR), explicitly estimating the unobserved statistic image given the sparse peak activation “co- ordinate” and “standardized effect-size estimate” data. In partic- ular, our model allows estimation of effect size at each voxel, some- thing existing CBMA methods cannot produce. Our results show thatGPRoutperforms existingCBMAtechniques and is capable of more accurately reproducing the (usually unavailable) full-image analysis results. Index},
archivePrefix = {arXiv},
arxivId = {1102.1101},
author = {Salimi-Khorshidi, Gholamreza and Nichols, Thomas E. and Smith, Stephen M. and Woolrich, Mark W.},
doi = {10.1109/TMI.2011.2122341},
eprint = {1102.1101},
file = {:Users/ricardo/Downloads/GP{\_}neurosynth.pdf:pdf},
isbn = {1558-254X (Electronic)$\backslash$r0278-0062 (Linking)},
issn = {02780062},
journal = {IEEE Trans. Med. Imaging},
keywords = {Bayesian inference,Gaussian processes,functional neuroimaging,meta-analysis},
number = {7},
pages = {1401--1416},
pmid = {21335308},
title = {{Using gaussian-process regression for meta-analytic neuroimaging inference based on sparse observations}},
volume = {30},
year = {2011}
}
@article{Greenewald2017a,
abstract = {The Bigraphical Lasso estimator was proposed to parsimoniously model the precision matrices of matrix-normal data based on the Cartesian product of graphs. By enforcing extreme sparsity (the number of parameters) and explicit structures on the precision matrix, this model has excellent potential for improving scalability of the computation and interpretability of complex data analysis. As a result, this model significantly reduces the size of the sample in order to learn the precision matrices, and hence the conditional probability models along different coordinates such as space, time and replicates. In this work, we extend the Bigraphical Lasso (BiGLasso) estimator to the TEnsor gRAphical Lasso (TeraLasso) estimator and propose an analogous method for modeling the precision matrix of tensor-valued data. We establish consistency for both the BiGLasso and TeraLasso estimators and obtain the rates of convergence in the operator and Frobenius norm for estimating the precision matrix. We design a scalable gradient descent method for solving the objective function and analyze the computational convergence rate, showing that the composite gradient descent algorithm is guaranteed to converge at a geometric rate to the global minimizer. Finally, we provide simulation evidence and analysis of a meteorological dataset, showing that we can recover graphical structures and estimate the precision matrices, as predicted by theory.},
archivePrefix = {arXiv},
arxivId = {1705.03983},
author = {Greenewald, Kristjan and Zhou, Shuheng and Hero, Alfred},
eprint = {1705.03983},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Greenewald, Zhou, Hero Iii - Unknown - Tensor Graphical Lasso (TeraLasso).pdf:pdf},
journal = {arXiv},
title = {{Tensor Graphical Lasso (TeraLasso)}},
url = {https://arxiv.org/pdf/1705.03983.pdf http://arxiv.org/abs/1705.03983},
year = {2017}
}
@article{Perrone2016,
abstract = {We present the Wright-Fisher Indian buffet process (WF-IBP), a probabilistic model for time-dependent data assumed to have been generated by an unknown number of latent features. This model is suitable as a prior in Bayesian nonparametric feature allocation models in which the features underlying the observed data exhibit a dependency structure over time. More specifically, we establish a new framework for generating dependent Indian buffet processes, where the Poisson random field model from population genetics is used as a way of constructing dependent beta processes. Inference in the model is complex, and we describe a sophisticated Markov Chain Monte Carlo algorithm for exact posterior simulation. We apply our construction to develop a nonparametric focused topic model for collections of time-stamped text documents and test it on the full corpus of NIPS papers published from 1987 to 2015.},
archivePrefix = {arXiv},
arxivId = {1611.07460},
author = {Perrone, Valerio and Jenkins, Paul A and Spano, Dario and Teh, Yee Whye},
eprint = {1611.07460},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Perrone et al. - Unknown - Poisson Random Fields for Dynamic Feature Models.pdf:pdf},
journal = {J. Mach. Learn. Res.},
pages = {1--34},
title = {{Poisson Random Fields for Dynamic Feature Models}},
url = {https://arxiv.org/pdf/1611.07460.pdf http://arxiv.org/abs/1611.07460},
year = {2016}
}
@article{Yang2010,
abstract = {A variant of nonnegative matrix factorization (NMF) which was proposed earlier is analyzed here. It is called projective nonnegative matrix factorization (PNMF). The new method approximately factorizes a projection matrix, minimizing the reconstruction error, into a positive low-rank matrix and its transpose. The dissimilarity between the original data matrix and its approximation can be measured by the Frobenius matrix norm or the modified Kullback-Leibler divergence. Both measures are minimized by multiplicative update rules, whose convergence is proven for the first time. Enforcing orthonormality to the basic objective is shown to lead to an even more efficient update rule, which is also readily extended to nonlinear cases. The formulation of the PNMF objective is shown to be connected to a variety of existing NMF methods and clustering approaches. In addition, the derivation using Lagrangian multipliers reveals the relation between reconstruction and sparseness. For kernel principal component analysis (PCA) with the binary constraint, useful in graph partitioning problems, the nonlinear kernel PNMF provides a good approximation which outperforms an existing discretization approach. Empirical study on three real-world databases shows that PNMF can achieve the best or close to the best in clustering. The proposed algorithm runs more efficiently than the compared NMF methods, especially for high-dimensional data. Moreover, contrary to the basic NMF, the trained projection matrix can be readily used for newly coming samples and demonstrates good generalization.},
author = {Yang, Zhirong and Oja, Erkki},
doi = {10.1109/TNN.2010.2041361},
file = {:Users/ricardo/Downloads/05438836.pdf:pdf},
isbn = {1045-9227 VO - 21},
issn = {10459227},
journal = {IEEE Trans. Neural Networks},
keywords = {Clustering,Kernel,Multiplicative updates,Nonnegative matrix factorization (NMF),Projection recovery,Projective},
month = {may},
number = {5},
pages = {734--749},
pmid = {20350841},
title = {{Linear and nonlinear projective nonnegative matrix factorization}},
url = {http://ieeexplore.ieee.org/document/5438836/},
volume = {21},
year = {2010}
}
@article{Fan2017,
abstract = {Motivated by applications in genomics, finance, and biomolecular simulation, we introduce a Bayesian framework for modeling changepoints that tend to co-occur across multiple related data sequences. We infer the locations and sequence memberships of changepoints in our hierarchical model by developing efficient Markov chain Monte Carlo sampling and posterior mode finding algorithms based on dynamic programming recursions. We further propose an empirical Bayesian Monte Carlo expectation-maximization procedure for estimating unknown prior parameters from data. The resulting framework accommodates a broad range of data and changepoint types, including real-valued sequences with changing mean or variance and sequences of counts or binary observations. We demonstrate on simulated data that our changepoint estimation accuracy is competitive with the best methods in the literature, and we apply our methodology to the discovery of DNA copy number variations in cancer cell lines and the analysis of historical price volatility in U.S. stocks.},
archivePrefix = {arXiv},
arxivId = {1508.01280},
author = {Fan, Zhou and Mackey, Lester},
doi = {arXiv:1508.01280v2},
eprint = {1508.01280},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Fan, Mackey - 2017 - An Empirical Bayesian Analysis of Simultaneous Changepoints in Multiple Data Sequences.pdf:pdf},
journal = {Ann. Appl. Stat.},
pages = {1--28},
title = {{An Empirical Bayesian Analysis of Simultaneous Changepoints in Multiple Data Sequences}},
url = {https://arxiv.org/pdf/1508.01280.pdf http://arxiv.org/abs/1508.01280},
year = {2017}
}
@article{Manning2014,
abstract = {The neural patterns recorded during a neuroscientific experiment reflect complex interactions between many brain regions, each comprising millions of neurons. However, the measurements themselves are typically abstracted from that underlying structure. For example, functional magnetic resonance imaging (fMRI) datasets comprise a time series of three-dimensional images, where each voxel in an image (roughly) reflects the activity of the brain structure(s)-located at the corresponding point in space-at the time the image was collected. FMRI data often exhibit strong spatial correlations, whereby nearby voxels behave similarly over time as the underlying brain structure modulates its activity. Here we develop topographic factor analysis (TFA), a technique that exploits spatial correlations in fMRI data to recover the underlying structure that the images reflect. Specifically, TFA casts each brain image as a weighted sum of spatial functions. The parameters of those spatial functions, which may be learned by applying TFA to an fMRI dataset, reveal the locations and sizes of the brain structures activated while the data were collected, as well as the interactions between those structures.},
author = {Manning, Jeremy R and Ranganath, Rajesh and Norman, Kenneth A and Blei, David M},
doi = {10.1371/journal.pone.0094914},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Manning et al. - 2014 - Topographic Factor Analysis A Bayesian Model for Inferring Brain Networks from Neural Data.pdf:pdf},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS One},
number = {5},
pmid = {24804795},
title = {{Topographic factor analysis: A Bayesian model for inferring brain networks from neural data}},
url = {http://caligari.dartmouth.edu/{~}jmanning/pubs/MannEtal14c.pdf},
volume = {9},
year = {2014}
}
@article{Sunnaker2013,
abstract = {Approximate Bayesian computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics. In all model-based statistical inference, the likelihood function is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evalu-ate. ABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection. ABC has rapidly gained popularity over the last years and in particular for the analysis of complex problems arising in biological sciences (e.g., in population genetics, ecology, epidemiology, and systems biology). This is a ''Topic Page'' article for PLOS Computational Biology. History The first Approximate Bayesian computation (ABC)-related ideas date back to the 1980s. Donald Rubin, when discussing the interpretation of Bayesian statements in 1984 [1], described a hypothetical sampling mechanism that yields a sample from the posterior distribution. This scheme was more of a conceptual thought experiment to demonstrate what type of manipulations are done when inferring the posterior distributions of parameters. The description of the sampling mechanism coincides exactly with that of the ABC-rejection scheme, and this article can be considered to be the first to describe approximate Bayesian computation. Another prescient point was made when Rubin argued that in Bayesian inference, applied statisticians should not settle for analytically tractable models only but instead consider computational methods that allow them to estimate the posterior distribution of interest. This way, a wider range of models can be considered. These arguments are particularly relevant in the context of ABC. In 1984, Peter Diggle and Richard Gratton suggested using a systematic simulation scheme to approximate the likelihood function in situations where its analytic form is intractable [2]. Their method was based on defining a grid in the parameter space and using it to approximate the likelihood by running several simulations for each grid point. The approximation was then improved by applying smoothing techniques to the outcomes of the simulations. While the idea of using simulation for hypothesis testing was not new [3,4], Diggle and Gratton seemingly introduced the first procedure using simulation to do statistical inference under a circumstance where the likelihood is intractable. Although Diggle and Gratton's approach had opened a new frontier, their method was not yet exactly identical to what is now known as ABC, as it aimed at approximating the likelihood rather than the posterior distribution. An article of Simon Tavar{\'{e}} et al. [5] was first to propose an ABC algorithm for posterior inference. In their seminal work, inference about the genealogy of DNA sequence data was considered, and in particular the problem of deciding the posterior distribution of the time to the most recent common ancestor of the sampled individuals. Such inference is analytically intractable for many demographical models, but the authors presented ways of simulating coalescent trees under the putative models. A sample from the posterior of model parameters was obtained by accepting/rejecting proposals based on compar-ing the number of segregating sites in the synthetic and real data. This work was followed by an applied study on modeling the variation in human Y chromosome by Jonathan K. Pritchard et al. [6] using the ABC method. Finally, the term Approximate Bayesian Computation was established by Mark Beaumont et al. [7], extending further the ABC methodology and discussing the suitability of the ABC-approach more specifically for problems in population genetics. Since then, ABC has spread to applications},
author = {Sunn{\aa}ker, Mikael and Busetto, Alberto Giovanni and Numminen, Elina and Corander, Jukka and Foll, Matthieu and Dessimoz, Christophe and Wodak, Shoshana},
doi = {10.1371/journal.pcbi.1002803},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Sunn{\aa}ker et al. - 2013 - Approximate Bayesian Computation Method Motivation(2).pdf:pdf},
journal = {Approx. Bayesian Comput. PLoS Comput Biol PLOS Comput. Biol. | www.ploscompbiol.org 1},
number = {1},
title = {{Approximate Bayesian Computation Method Motivation}},
url = {http://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1002803{\&}type=printable},
volume = {9},
year = {2013}
}
@article{Zhang2009,
abstract = {Distinguishing causes from effects is an important problem in many areas. In this paper, we propose a very general but well defined nonlinear acyclic causal model, namely, post-nonlinear acyclic causal model with inner additive noise, to tackle this problem. In this model, each ob-served variable is generated by a nonlinear function of its parents, with additive noise, followed by a nonlinear distortion. The nonlinearity in the second stage takes into account the effect of sensor distortions, which are usually encountered in practice. In the two-variable case, if all the nonlinearities involved in the model are invertible, by relating the proposed model to the post-nonlinear independent component analysis (ICA) problem, we give the conditions under which the causal relation can be uniquely found. We present a two-step method, which is constrained nonlinear ICA followed by statistical independence tests, to distinguish the cause from the ef-fect in the two-variable case. We apply this method to solve the problem " CauseEffectPairs" in the Pot-luck challenge, and successfully identify causes from effects.},
author = {Zhang, Kun and Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang KUNZHANG, Hyv{\"{a}}rinen AAPOHYVARINEN - 2008 - Distinguishing Causes from Effects using Nonlinear Acyclic Causal Models(3).pdf:pdf},
journal = {NIPS 2008 Work. Causality},
keywords = {additive noise,causal discovery,independence tests,nonlinear independent compo-nent analysis,sensor distortion},
pages = {157--164},
title = {{Distinguishing Causes from Effects using Nonlinear Acyclic Causal Models}},
url = {http://proceedings.mlr.press/v6/zhang10a/zhang10a.pdf},
volume = {6},
year = {2008}
}
@article{Arjovsky,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Arjovsky, Chintala, Bottou - Unknown - Wasserstein GAN.pdf:pdf},
isbn = {1406.2661},
issn = {1701.07875},
title = {{Wasserstein GAN}},
url = {https://arxiv.org/pdf/1701.07875.pdf http://arxiv.org/abs/1701.07875},
year = {2017}
}
@book{Murphy2012,
author = {Murphy, Kevin},
publisher = {MIT Press},
title = {{Machine Learning: A Probabilistic Perspective}},
year = {2012}
}
@article{Lin2016,
abstract = {Graphical models are widely used to model stochastic depen-dences among large collections of variables. We introduce a new method of estimating undirected conditional independence graphs based on the score matching loss, introduced by Hyv{\"{a}}rinen (2005), and subsequently extended in Hyv{\"{a}}rinen (2007). The regularized score matching method we propose applies to settings with continuous observations and allows for computa-tionally efficient treatment of possibly non-Gaussian exponential family models. In the well-explored Gaussian setting, regularized score matching avoids issues of asymmetry that arise when applying the technique of neigh-borhood selection, and compared to existing methods that directly yield symmetric estimates, the score matching approach has the advantage that the considered loss is quadratic and gives piecewise linear solution paths under 1 regularization. Under suitable irrepresentability conditions, we show that 1 -regularized score matching is consistent for graph estimation in sparse high-dimensional settings. Through numerical experiments and an application to RNAseq data, we confirm that regularized score matching achieves state-of-the-art performance in the Gaussian case and provides a valuable tool for computationally efficient estimation in non-Gaussian graphical models.},
author = {Lin, Lina and Drton, Mathias and Shojaie, Ali},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Drton, Shojaie - 2016 - Estimation of high-dimensional graphical models using regularized score matching.pdf:pdf},
journal = {Electron. J. Stat.},
keywords = {62F12Conditional independence graph,62H12,exponential family,graphical model,high-dimensional statistics,score matching,sparsity},
pages = {806--854},
title = {{Estimation of high-dimensional graphical models using regularized score matching}},
volume = {10},
year = {2016}
}
@article{Levy2017,
abstract = {We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. We release an open source TensorFlow implementation of the algorithm.},
archivePrefix = {arXiv},
arxivId = {1711.09268},
author = {Levy, Daniel and Hoffman, Matthew D and Sohl-Dickstein, Jascha},
eprint = {1711.09268},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Levy, Hoffman, Sohl-Dickstein - Unknown - GENERALIZING HAMILTONIAN MONTE CARLO WITH NEURAL NETWORKS.pdf:pdf},
title = {{Generalizing Hamiltonian Monte Carlo with Neural Networks}},
url = {https://arxiv.org/pdf/1711.09268.pdf http://arxiv.org/abs/1711.09268},
year = {2017}
}
@article{Gutmann2016,
abstract = {Our paper deals with inferring simulator-based statistical models given some observed data. A simulator-based model is a parametrized mechanism which specifies how data are gener-ated. It is thus also referred to as generative model. We assume that only a finite number of parameters are of interest and allow the generative process to be very general; it may be a noisy nonlinear dynamical system with an unrestricted number of hidden variables. This weak assumption is useful for devising realistic models but it renders statistical inference very difficult. The main challenge is the intractability of the likelihood function. Several likelihood-free inference methods have been proposed which share the basic idea of iden-tifying the parameters by finding values for which the discrepancy between simulated and observed data is small. A major obstacle to using these methods is their computational cost. The cost is largely due to the need to repeatedly simulate data sets and the lack of knowledge about how the parameters affect the discrepancy. We propose a strategy which combines probabilistic modeling of the discrepancy with optimization to facilitate likelihood-free inference. The strategy is implemented using Bayesian optimization and is shown to accelerate the inference through a reduction in the number of required simulations by several orders of magnitude.},
author = {Gutmann, Michael and Corander, Jukka},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gutmann, Corander - 2016 - Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models.pdf:pdf},
journal = {J. Mach. Learn. Res.},
pages = {1--47},
title = {{Bayesian Optimization for Likelihood-Free Inference of Simulator-Based Statistical Models}},
url = {http://jmlr.org/papers/volume17/15-017/15-017.pdf},
volume = {17},
year = {2016}
}
@article{Tian2002,
abstract = {Abstract This paper concerns the assessment of the effects of actions or policy interventions from a combination of:(i) nonexperimental data, and (ii) substantive assumptions. The assumptions are encoded in the form of a directed acyclic graph, also called “ causal ...},
author = {Tian, Jin and Pearl, Judea},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Tian, Pearl - Unknown - A General Identification Condition for Causal Effects(2).pdf:pdf},
isbn = {0-262-51129-0},
journal = {AAAI},
number = {August},
pages = {567--573},
title = {{A General Identification Condition for Causal Effects}},
url = {http://www.aaai.org/Papers/AAAI/2002/AAAI02-085.pdf},
year = {2002}
}
@article{Ledoit,
abstract = {This paper proposes to estimate the covariance matrix of stock returns by an optimally weighted average of two existing estimators: the sample covariance matrix and single-index covariance matrix. This method is generally known as shrinkage, and it is standard in decision theory and in empirical Bayesian statistics. Our shrinkage estimator can be seen as a way to account for extra-market covariance without having to specify an arbitrary multifactor structure. For NYSE and AMEX stock returns from 1972 to 1995, it can be used to select portfolios with significantly lower out-of-sample variance than a set of existing estimators, including multifactor models. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Ledoit, Olivier and Wolf, Michael},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ledoit, Wolf - Unknown - Improved estimation of the covariance matrix of stock returns with an application to portfolio selection.pdf:pdf},
journal = {J. Empir. Financ.},
keywords = {Covariance matrix estimation,Factor models,Portfolio selection,Shrinkage method},
number = {5},
pages = {603--621},
title = {{Improved estimation of the covariance matrix of stock returns with an application to portfolio selection}},
volume = {10},
year = {2003}
}
@article{Colombo2014,
abstract = {We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al. (2000, 1993), Richardson (1996), Colombo et al. (2012), Claassen et al. (2013)). The first step of all these algorithms consists of the PC-algorithm. This algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modifications of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modifications are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modifications in simulation studies and on a yeast gene expression data set. We show that our modifications yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.},
archivePrefix = {arXiv},
arxivId = {1211.3295},
author = {Colombo, Diego and Maathuis, Marloes H},
doi = {papers3://publication/uuid/CC1A353F-4C95-4058-93CC-05855FD6E5E3},
eprint = {1211.3295},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Colombo, Maathuis - 2014 - Order-Independent Constraint-Based Causal Structure Learning(2).pdf:pdf},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
keywords = {CCD-algorithm,FCI-algorithm,PC-algorithm,consistency,directed acyclic graph,high-dimensional data,order-dependence},
pages = {3921--3962},
title = {{Order-independent constraint-based causal structure learning}},
url = {http://www.jmlr.org/papers/volume15/colombo14a/colombo14a.pdf http://arxiv.org/abs/1211.3295},
volume = {15},
year = {2012}
}
@article{Titsias2009,
abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
author = {Titsias, Michalis},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Titsias - Unknown - Variational Learning of Inducing Variables in Sparse Gaussian Processes(2).pdf:pdf},
issn = {15324435},
journal = {AISTATS},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {567--574},
title = {{Variational Learning of Inducing Variables in Sparse Gaussian Processes}},
url = {http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf http://eprints.pascal-network.org/archive/00006353/},
volume = {5},
year = {2009}
}
@article{Sasaki2017,
abstract = {The statistical dependencies which independent component analysis (ICA) cannot remove often provide rich information beyond the linear independent components. It would thus be very useful to estimate the dependency structure from data. While such models have been proposed, they usually concentrated on higher-order correlations such as energy (square) correlations. Yet, linear correlations are a most fundamental and informative form of dependency in many real data sets. Linear correlations are usually completely removed by ICA and related methods, so they can only be analyzed by developing new methods which explicitly allow for linearly correlated components. In this paper, we propose a probabilistic model of linear non-Gaussian components which are allowed to have both linear and energy correlations. The precision matrix of the linear components is assumed to be randomly generated by a higher-order process and explicitly parametrized by a parameter matrix. The estimation of the parameter matrix is shown to be particularly simple because using score matching, the objective function is a quadratic form. Using simulations with artificial data, we demonstrate that the proposed method improves identifiability of non-Gaussian components by simultaneously learning their correlation structure. Applications on simulated complex cells with natural image input, as well as spectrograms of natural audio data show that the method finds new kinds of dependencies between the components.},
author = {Sasaki, Hiroaki and Gutmann, Michael and Shouno, Hayaru and Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Sasaki et al. - Unknown - Simultaneous Estimation of Nongaussian Components and Their Correlation Structure.pdf:pdf},
issn = {1530888X},
journal = {Neural Comput.},
pages = {2887--2924},
title = {{Simultaneous Estimation of Non-Gaussian Components and their Correlation Structure}},
volume = {29},
year = {2017}
}
@article{Friston2011,
abstract = {Abstract Over the past 20 years, neuroimaging has become a predominant technique in systems neuroscience. One might envisage that over the next 20 years the neuroimaging of distributed processing and connectivity will play a major role in disclosing the brain's ...},
author = {Friston, Karl J.},
file = {:Users/ricardo/Downloads/out.pdf:pdf},
journal = {Brain Connect.},
keywords = {brain connectivity,causal modeling,effective connectivity,functional connectivity},
number = {1},
pages = {13--36},
title = {{Functional and Effective Connectivity: A Review}},
volume = {1},
year = {2011}
}
@article{Spielman,
abstract = {We consider the problem of learning sparsely used dictionaries with an arbitrary square dictionary and a random, sparse coefficient matrix. We prove that O(n log n) samples are sufficient to uniquely determine the coefficient matrix. Based on this proof, we design a polynomial-time algorithm, called Exact Recovery of Sparsely-Used Dictionaries (ER-SpUD), and prove that it probably recovers the dictionary and coefficient matrix when the coefficient matrix is sufficiently sparse. Simulation results show that ER-SpUD reveals the true dictionary as well as the coefficients with probability higher than many state-of-the-art algorithms.},
author = {Spielman, Daniel A and Wang, Huan and Wright, John},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Spielman et al. - Unknown - Exact Recovery of Sparsely-Used Dictionaries.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {Dictionary learning,matrix decomposition,matrix sparsification},
pages = {1--35},
title = {{Exact Recovery of Sparsely-Used Dictionaries}},
url = {https://arxiv.org/pdf/1206.5882.pdf}
}
@article{Tsagkrasoulis2017,
abstract = {An increasing array of biomedical and computer vision applications requires the predictive modeling of complex data, for example images and shapes. The main challenge when predicting such objects lies in the fact that they do not comply to the assumptions of Euclidean geometry. Rather, they occupy non-linear spaces, a.k.a. manifolds, where it is difficult to define concepts such as coordinates, vectors and expected values. In this work, we construct a non-parametric predictive methodology for manifold-valued objects, based on a distance modification of the Random Forest algorithm. Our method is versatile and can be applied both in cases where the response space is a well-defined manifold, but also when such knowledge is not available. Model fitting and prediction phases only require the definition of a suitable distance function for the observed responses. We validate our methodology using simulations and apply it on a series of illustrative image completion applications, showcasing superior predictive performance, compared to various established regression methods.},
archivePrefix = {arXiv},
arxivId = {1701.08381},
author = {Tsagkrasoulis, Dimosthenis and Montana, Giovanni},
eprint = {1701.08381},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Tsagkrasoulis, Montana - Unknown - Random Forest regression for manifold-valued responses.pdf:pdf},
journal = {Pattern Recognit. Lett.},
pages = {1--10},
title = {{Random Forest regression for manifold-valued responses}},
url = {https://arxiv.org/pdf/1701.08381.pdf http://arxiv.org/abs/1701.08381},
year = {2017}
}
@article{Taylor2017,
abstract = {There are a variety of challenges that come with producing a large number of forecasts across a variety time series. Our approach to fore-casting at scale is a combination of configurable models and thorough analyst-in-the-loop performance analysis. We present a forecasting ap-proach based on a decomposable model with interpretable parameters that can be intuitively adjusted by the analyst. We describe performance analysis that we use compare and evaluate forecasting procedures, as well as automatically flag forecasts for manual review and adjustment. Tools that help analysts to use their expertise most effectively enable reliable forecasting of a large variety of business time series.},
author = {Taylor, Sean J and Letham, Benjamin},
doi = {10.7287/peerj.preprints.3190v1},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Taylor, Letham - Unknown - Forecasting at Scale(2).pdf:pdf},
issn = {2167-9843},
journal = {PeerJ Prepr.},
keywords = {nonlinear regression,statistical practice,time series},
pages = {1--17},
title = {{Forecasting at Scale}},
url = {https://peerj.com/preprints/3190.pdf},
year = {2017}
}
@article{Haufe2014,
abstract = {The increase in spatiotemporal resolution of neuroimaging devices is accompanied by a trend towards more powerful multivariate analysis methods. Often it is desired to interpret the outcome of these methods with respect to the cognitive processes under study. Here we discuss which methods allow for such interpretations, and provide guidelines for choosing an appropriate analysis for a given experimental goal: For a surgeon who needs to decide where to remove brain tissue it is most important to determine the origin of cognitive functions and associated neural processes. In contrast, when communicating with paralyzed or comatose patients via brain-computer interfaces, it is most important to accurately extract the neural processes specific to a certain mental state. These equally important but complementary objectives require different analysis methods. Determining the origin of neural processes in time or space from the parameters of a data-driven model requires what we call a forward model of the data; such a model explains how the measured data was generated from the neural sources. Examples are general linear models (GLMs). Methods for the extraction of neural information from data can be considered as backward models, as they attempt to reverse the data generating process. Examples are multivariate classifiers. Here we demonstrate that the parameters of forward models are neurophysiologically interpretable in the sense that significant nonzero weights are only observed at channels the activity of which is related to the brain process under study. In contrast, the interpretation of backward model parameters can lead to wrong conclusions regarding the spatial or temporal origin of the neural signals of interest, since significant nonzero weights may also be observed at channels the activity of which is statistically independent of the brain process under study. As a remedy for the linear case, we propose a procedure for transforming backward models into forward models. This procedure enables the neurophysiological interpretation of the parameters of linear backward models. We hope that this work raises awareness for an often encountered problem and provides a theoretical basis for conducting better interpretable multivariate neuroimaging analyses. {\textcopyright} 2013 The Authors.},
author = {Haufe, Stefan and Meinecke, Frank and G{\"{o}}rgen, Kai and D{\"{a}}hne, Sven and Haynes, John Dylan and Blankertz, Benjamin and Bie{\ss}mann, Felix},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Haufe et al. - 2014 - On the interpretation of weight vectors of linear models in multivariate neuroimaging.pdf:pdf},
isbn = {1053-8119},
journal = {Neuroimage},
keywords = {Activation patterns,Decoding,EEG,Encoding,Extraction filters,FMRI,Forward/backward models,Generative/discriminative models,Interpretability,Multivariate,Neuroimaging,Regularization,Sparsity,Univariate},
pages = {96--110},
publisher = {Academic Press},
title = {{On the interpretation of weight vectors of linear models in multivariate neuroimaging}},
volume = {87},
year = {2014}
}
@article{Blobaum2018,
abstract = {We address the problem of inferring the causal relation between two variables by comparing the least-squares errors of the predictions in both possible causal directions. Under the assumption of an independence between the function relating cause and effect, the conditional noise distribution, and the distribution of the cause, we show that the errors are smaller in causal direction if both variables are equally scaled and the causal relation is close to deterministic. Based on this, we provide an easily applicable method that only requires a regression in both possible causal directions. The performance of this method is compared with different related causal inference methods in various artificial and real-world data sets.},
author = {Bl{\"{o}}baum, Patrick and Janzing, Dominik and Washio, Takashi and Shimizu, Shohei and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bl{\"{o}}baum et al. - 2018 - Cause-Effect Inference by Comparing Regression Errors.pdf:pdf},
journal = {AISTATS},
title = {{Cause-Effect Inference by Comparing Regression Errors}},
year = {2018}
}
@article{Zhang2018,
abstract = {Representations of probability measures in reproducing kernel Hilbert spaces provide a flexible framework for fully nonparametric hypothesis tests of independence, which can capture any type of departure from independence, including nonlinear associations and multivariate interactions. However, these approaches come with an at least quadratic computational cost in the number of observations, which can be prohibitive in many applications. Arguably, it is exactly in such large-scale datasets that capturing any type of dependence is of interest, so striking a favourable tradeoff between computational efficiency and test performance for kernel independence tests would have a direct impact on their applicability in practice. In this contribution, we provide an extensive study of the use of large-scale kernel approximations in the context of independence testing, contrasting block-based, Nystrom and random Fourier feature approaches. Through a variety of synthetic data experiments, it is demonstrated that our novel large scale methods give comparable performance with existing methods whilst using significantly less computation time and memory.},
archivePrefix = {arXiv},
arxivId = {1606.07892},
author = {Zhang, Qinyi and Filippi, Sarah and Gretton, Arthur and Sejdinovic, Dino},
doi = {10.1007/s11222-016-9721-7},
eprint = {1606.07892},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - Large-scale kernel methods for independence testing.pdf:pdf},
issn = {15731375},
journal = {Stat. Comput.},
keywords = {Hilbert–Schmidt independence criteria,Independence testing,Large-scale kernel method,Nystr{\"{o}}m method,Random Fourier features},
number = {1},
pages = {113--130},
title = {{Large-scale kernel methods for independence testing}},
url = {https://doi.org/10.1007/s11222-016-9721-7},
volume = {28},
year = {2018}
}
@article{Ng2014,
abstract = {L'archive ouverte pluridisciplinaire HAL, est destin{\'{e}}e au d{\'{e}}p{\^{o}}t et {\`{a}} la diffusion de documents scientifiques de niveau recherche, publi{\'{e}}s ou non, {\'{e}}manant des {\'{e}}tablissements d'enseignement et de recherche fran{\c{c}}ais ou {\'{e}}trangers, des laboratoires publics ou priv{\'{e}}s. Abstract. We present a Riemannian approach for classifying fMRI connectivity patterns before and after intervention in longitudinal studies. A fundamental dif-ficulty with using connectivity as features is that covariance matrices live on the positive semi-definite cone, which renders their elements inter-related. The im-plicit independent feature assumption in most classifier learning algorithms is thus violated. In this paper, we propose a matrix whitening transport for project-ing the covariance estimates onto a common tangent space to reduce the statis-tical dependencies between their elements. We show on real data that our ap-proach provides significantly higher classification accuracy than directly using Pearson's correlation. We further propose a non-parametric scheme for identi-fying significantly discriminative connections from classifier weights. Using this scheme, a number of neuroanatomically meaningful connections are found, whereas no significant connections are detected with pure permutation testing.},
author = {Ng, Bernard and Dressler, Martin and Varoquaux, Ga{\"{e}}l and Poline, Jean-Baptiste and Greicius, Michael and Thirion, Bertrand and Poline, Jean Baptiste},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ng et al. - 2014 - Transport on Riemannian Manifold for Functional Connectivity-based Classification.pdf:pdf},
journal = {MICCAI},
keywords = {Classification,Connectivity,Riemannian manifold,fMRI},
number = {1},
publisher = {Springer},
title = {{Transport on Riemannian Manifold for Functional Connectivity-based Classification}},
url = {https://hal.inria.fr/file/index/docid/1058521/filename/camReady.pdf},
year = {2014}
}
@article{McCormick2010,
abstract = {The analysis of climate data has relied heavily on hypothesis-driven statistical methods, while projections of future climate are based primarily on physics-based computational models. However, in recent years a wealth of new datasets has become available. Therefore, we take a more data-centric approach and propose a unified framework for studying climate, with an aim toward characterizing observed phenomena as well as discovering new knowledge in climate science. Specifically, we posit that complex networks are well suited for both descriptive analysis and predictive modeling tasks. We show that the structural properties of 'climate networks' have useful interpretation within the domain. Further, we extract clusters from these networks and demonstrate their predictive power as climate indices. Our experimental results establish that the network clusters are statistically significantly better predictors than clusters derived using a more traditional clustering approach. Using complex networks as data representation thus enables the unique opportunity for descriptive and predictive modeling to inform each other. 2010 Wiley Periodicals, Inc.},
archivePrefix = {arXiv},
arxivId = {1206.3552},
author = {Anagnostopoulos, Christoforos and Tasoulis, Dimitris and Adams, Niall M and Pavlidis, Nicos and Hand, David J},
doi = {10.1002/sam},
eprint = {1206.3552},
file = {:Users/ricardo/Downloads/Anagnostopoulos{\_}et{\_}al-2012-Statistical{\_}Analysis{\_}and{\_}Data{\_}Mining.pdf:pdf},
isbn = {1932-1872},
issn = {19321872},
journal = {Stat. Anal. Data Min.},
keywords = {climate data,community detection,complex networks,multivariate predictive modeling,network analysis},
number = {5},
pages = {497--511},
pmid = {21824845},
title = {{Online Linear and Quadratic Discriminant Analysis with Adaptive Forgetting for Streaming Classification}},
volume = {4},
year = {2012}
}
@article{Qin,
abstract = {The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.},
archivePrefix = {arXiv},
arxivId = {1705.10033},
author = {Qin, Chao and Klabjan, Diego and Russo, Daniel},
eprint = {1705.10033},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Qin, Klabjan, Russo - Unknown - Improving the Expected Improvement Algorithm.pdf:pdf},
title = {{Improving the Expected Improvement Algorithm}},
url = {http://papers.nips.cc/paper/7122-improving-the-expected-improvement-algorithm.pdf http://arxiv.org/abs/1705.10033},
year = {2017}
}
@article{Kuang2015,
abstract = {Nonnegative matrix factorization (NMF) provides a lower rank approximation of a matrix by a product of two nonnegative factors. NMF has been shown to produce clustering results that are often superior to those by other methods such as K-means. In this paper, we provide further interpretation of NMF as a clustering method and study an extended formulation for graph clustering called Symmetric NMF (SymNMF). In contrast to NMF that takes a data matrix as an input, SymNMF takes a nonnegative similarity matrix as an input, and a symmetric nonnegative lower rank approximation is computed. We show that SymNMF is related to spectral clustering, justify SymNMF as a general graph clustering method, and discuss the strengths and shortcomings of SymNMF and spectral clustering. We propose two optimization algorithms for SymNMF and discuss their convergence properties and computational efficiencies. Our experiments on document clustering, image clustering, and image segmentation support SymNMF as a graph clustering method that captures latent linear and nonlinear relationships in the data. {\textcopyright} 2014 Springer Science+Business Media New York},
author = {Kuang, Da and Yun, Sangwoon and Park, Haesun},
doi = {10.1007/s10898-014-0247-2},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kuang et al. - Unknown - SymNMF Nonnegative Low-Rank Approximation of a Similarity Matrix for Graph Clustering.pdf:pdf},
isbn = {1089801402472},
issn = {15732916},
journal = {J. Glob. Optim.},
keywords = {Graph clustering,Low-rank approximation,Spectral clustering,Symmetric nonnegative matrix factorization},
number = {3},
pages = {545--574},
title = {{SymNMF: nonnegative low-rank approximation of a similarity matrix for graph clustering}},
url = {http://dx.doi.org/10.1007/s10898-014-0247-2.},
volume = {62},
year = {2015}
}
@article{Drineas2004,
abstract = {We consider the problem of partitioning a set of m points in the n-dimensional Euclidean space into k clusters (usually m and n are variable, while k is fixed), so as to minimize the sum of squared distances between each point and its cluster center. This formulation is usually the objective of the k-means clustering algorithm (Kanungo et al. (2000)). We prove that this problem in NP-hard even for k = 2, and we consider a continuous relaxation of this discrete problem: find the k-dimensional subspace V that minimizes the sum of squared distances to V of the m points. This relaxation can be solved by computing the Singular Value Decomposition (SVD) of the m × n matrix A that represents the m points; this solution can be used to get a 2-approximation algorithm for the original problem. We then argue that in fact the relaxation provides a generalized clustering which is useful in its own right. Finally, we show that the SVD of a random submatrix—chosen according to a suitable probability distribution—of a given matrix provides an approximation to the SVD of the whole matrix, thus yielding a very fast randomized algorithm. We expect this algorithm to be the main contribution of this paper, since it can be applied to problems of very large size which typically arise in modern applications.},
author = {Drineas, Petros and Frieze, Alan and Kannan, Ravi and Vempala, Santosh and Vinay, V},
doi = {10.1023/B:MACH.0000033113.59016.96},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Drineas et al. - 2004 - Clustering Large Graphs via the Singular Value Decomposition.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Mach. Learn.},
keywords = {Singular Value Decomposition,approximation,k-means clustering,randomized algorithms},
number = {1-3},
pages = {9--33},
title = {{Clustering Large Graphs via the Singular Value Decomposition}},
url = {https://www.cc.gatech.edu/{~}vempala/papers/dfkvv.pdf http://apps.webofknowledge.com/full{\_}record.do?product=WOS{\&}search{\_}mode=RelatedRecords{\&}qid=2{\&}SID=V1dGdjIgFanpbLh1Kbe{\&}page=1{\&}doc=8{\%}5Cnhttp://www.springerlink.com/openurl.asp?id=doi:10.1023/B:MACH.0000033113},
volume = {56},
year = {2004}
}
@article{Lan2013,
abstract = {We develop a new model and algorithms for machine learning-based learning analytics, which estimate a learner's knowledge of the concepts underlying a domain, and content analytics, which estimate the relationships among a collection of questions and those concepts. Our model represents the probability that a learner provides the correct response to a question in terms of three factors: their understanding of a set of underlying concepts, the concepts involved in each question, and each question's intrinsic difficulty. We estimate these factors given the graded responses to a collection of questions. The underlying estimation problem is ill-posed in general, especially when only a subset of the questions are answered. The key observation that enables a well-posed solution is the fact that typical educational domains of interest involve only a small number of key concepts. Leveraging this observation, we develop both a bi-convex maximum-likelihood and a Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem. We also incorporate user-defined tags on questions to facilitate the interpretability of the estimated factors. Experiments with synthetic and real-world data demonstrate the efficacy of our approach. Finally, we make a connection between SPARFA and noisy, binary-valued (1-bit) dictionary learning that is of independent interest.},
archivePrefix = {arXiv},
arxivId = {1303.5685},
author = {Lan, Andrew S and Waters, Andrew E and Studer, Christoph and Baraniuk, Richard G},
doi = {10.1145/2623330.2623631},
eprint = {1303.5685},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lan et al. - 2014 - Sparse Factor Analysis for Learning and Content Analytics.pdf:pdf},
isbn = {9781479903566},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {Bayesian latent factor analysis,factor analysis,personalized learning,sparse logistic regression,sparse probit regression},
pages = {1959--2008},
title = {{Sparse Factor Analysis for Learning and Content Analytics}},
url = {http://jmlr.org/papers/volume15/lan14a/lan14a.pdf http://arxiv.org/abs/1303.5685},
volume = {15},
year = {2013}
}
@article{Newman2006,
abstract = {Many networks of interest in the sciences, including social net-works, computer networks, and metabolic and regulatory net-works, are found to divide naturally into communities or modules. The problem of detecting and characterizing this community struc-ture is one of the outstanding issues in the study of networked systems. One highly effective approach is the optimization of the quality function known as ''modularity'' over the possible divisions of a network. Here I show that the modularity can be expressed in terms of the eigenvectors of a characteristic matrix for the net-work, which I call the modularity matrix, and that this expression leads to a spectral algorithm for community detection that returns results of demonstrably higher quality than competing methods in shorter running times. I illustrate the method with applications to several published network data sets. M any systems of scientific interest can be represented as networks, sets of nodes or vertices joined in pairs by lines or edges. Examples include the internet and the worldwide web, metabolic networks, food webs, neural networks, communica-tion and distribution networks, and social networks. The study of networked systems has a history stretching back several centu-ries, but it has experienced a particular surge of interest in the last decade, especially in the mathematical sciences, partly as a result of the increasing availability of accurate large-scale data describing the topology of networks in the real world. Statistical analyses of these data have revealed some unexpected structural features, such as high network transitivity (1), power-law degree distributions (2), and the existence of repeated local motifs (3); see refs. 4–6 for reviews. One issue that has received a considerable amount of attention is the detection and characterization of community structure in networks (7, 8), meaning the appearance of densely connected groups of vertices, with only sparser connections between groups (Fig. 1). The ability to detect such groups could be of significant practical importance. For instance, groups within the worldwide web might correspond to sets of web pages on related topics (9); groups within social networks might correspond to social units or communities (10). Merely the finding that a network contains tightly knit groups at all can convey useful information: if a metabolic network were divided into such groups, for instance, it could provide evidence for a modular view of the network's dynamics, with different groups of nodes performing different functions with some degree of independence (11, 12). Past work on methods for discovering groups in networks divides into two principal lines of research, both with long histories. The first, which goes by the name of graph partitioning, has been pursued particularly in computer science and related fields, with applications in parallel computing and integrated circuit design, among other areas (13, 14). The second, identified by names such as block modeling, hierarchical clustering, or community structure detection, has been pursued by sociologists and more recently by physicists, biologists, and applied mathe-maticians, with applications especially to social and biological networks (7, 15, 16). It is tempting to suggest that these two lines of research are really addressing the same question, albeit by somewhat different means. There are, however, important differences between the goals of the two camps that make quite different technical approaches desirable. A typical problem in graph partitioning is the division of a set of tasks between the processors of a parallel computer so as to minimize the necessary amount of interpro-cessor communication. In such an application the number of processors is usually known in advance and at least an approx-imate figure for the number of tasks that each processor can handle. Thus we know the number and size of the groups into which the network is to be split. Also, the goal is usually to find the best division of the network regardless of whether a good division even exists; there is little point in an algorithm or method that fails to divide the network in some cases. Community structure detection, by contrast, is perhaps best thought of as a data analysis technique used to shed light on the structure of large-scale network data sets, such as social net-works, internet and web data, or biochemical networks. Com-munity structure methods normally assume that the network of interest divides naturally into subgroups and the experimenter's job is to find those groups. The number and size of the groups are thus determined by the network itself and not by the experimenter. Moreover, community structure methods may explicitly admit the possibility that no good division of the network exists, an outcome that is itself considered to be of interest for the light it sheds on the topology of the network. This article focuses on community structure detection in network data sets representing real-world systems of interest. However, both the similarities and differences between commu-nity structure methods and graph partitioning will motivate many of the developments that follow.},
author = {Newman, M E J},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Newman - Unknown - Modularity and community structure in networks.pdf:pdf},
journal = {Proc. Natl. Acad. Sci.},
number = {23},
pages = {8577--8582},
title = {{Modularity and community structure in networks}},
volume = {103},
year = {2006}
}
@article{Peters2015a,
abstract = {Causal inference relies on the structure of a graph, often a directed acyclic graph (DAG). Different graphs may result in different causal inference statements and different intervention distributions. To quantify such differences, we propose a (pre-) distance between DAGs, the structural intervention distance (SID). The SID is based on a graphical criterion only and quantifies the closeness between two DAGs in terms of their corresponding causal inference statements. It is therefore well-suited for evaluating graphs that are used for computing interventions. Instead of DAGs it is also possible to compare CPDAGs, completed partially directed acyclic graphs that represent Markov equivalence classes. Since it differs significantly from the popular Structural Hamming Distance (SHD), the SID constitutes a valuable additional measure. We discuss properties of this distance and provide an efficient implementation with software code available on the first author's homepage (an R package is under construction).},
archivePrefix = {arXiv},
arxivId = {1306.1043},
author = {Peters, Jonas and B{\"{u}}hlmann, Peter},
doi = {10.1162/NECO_a_00708},
eprint = {1306.1043},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Peters, Uhlmann - Unknown - Structural Intervention Distance for Evaluating Causal Graphs(2).pdf:pdf},
isbn = {0899-7667},
issn = {1530888X},
journal = {Neural Comput.},
number = {3},
pages = {771--799},
pmid = {25602775},
title = {{Structural intervention distance for evaluating causal graphs}},
url = {https://www.mitpressjournals.org/doi/pdf/10.1162/NECO{\_}a{\_}00708},
volume = {27},
year = {2015}
}
@article{Scott2014,
abstract = {The modern service economy is substantively different from the agricultural and manufac-turing economies that preceded it. In particular, the cost of experimenting is dominated by opportunity cost rather than the cost of obtaining experimental units. The different economics require a new class of experiments, in which stochastic models play an important role. This article briefly summarizes mulit-armed bandit experiments, where the experimental design is modified as the experiment progresses to make the experiment as inexpensive as possible.},
author = {Scott, Steven L},
doi = {10.1002/asmb.2104},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Scott - 2014 - Multi-armed bandit experiments in the online service economy.pdf:pdf},
issn = {15241904},
journal = {Appl. Stoch. Model. Bus. Ind.},
keywords = {Bayesian,Thompson sampling,reinforcement learning,sequential experiment},
number = {June 2014},
title = {{Multi-armed bandit experiments in the online service economy}},
url = {https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42550.pdf http://doi.wiley.com/10.1002/asmb.2104},
year = {2015}
}
@article{Papernota,
abstract = {—Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97{\%} adversarial success rate while only modifying on average 4.02{\%} of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial per-turbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Swami, Ananthram},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Papernot et al. - Unknown - The Limitations of Deep Learning in Adversarial Settings.pdf:pdf},
title = {{The Limitations of Machine Learning in Adversarial Settings}},
url = {https://arxiv.org/pdf/1511.07528.pdf}
}
@article{Wang2013,
abstract = {Nonnegative Matrix Factorization (NMF), a relatively novel paradigm for dimensionality reduction, has been in the ascendant since its inception. It incorporates the nonnegativity constraint and thus obtains the parts-based representation as well as enhancing the interpretability of the issue correspondingly. This survey paper mainly focuses on the theoretical research into NMF over the last 5 years, where the principles, basic models, properties, and algorithms of NMF along with its various modifications, extensions, and generalizations are summarized systematically. The existing NMF algorithms are divided into four categories: Basic NMF (BNMF), Constrained NMF (CNMF), Structured NMF (SNMF), and Generalized NMF (GNMF), upon which the design principles, characteristics, problems, relationships, and evolution of these algorithms are presented and analyzed comprehensively. Some related work not on NMF that NMF should learn from or has connections with is involved too. Moreover, some open issues remained to be solved are discussed. Several relevant application areas of NMF are also briefly described. This survey aims to construct an integrated, state-of-the-art framework for NMF concept, from which the follow-up research may benefit.},
author = {Wang, Yu-Xiong and Zhang, Yu-Jin},
doi = {10.1109/TKDE.2012.51},
file = {:Users/ricardo/Downloads/06165290.pdf:pdf},
isbn = {9783662483305},
issn = {1041-4347},
journal = {IEEE Trans. Knowl. Data Eng.},
number = {6},
pages = {1336--1353},
pmid = {21263163},
title = {{Nonnegative Matrix Factorization: A Comprehensive Review}},
url = {http://ieeexplore.ieee.org/document/6165290/},
volume = {25},
year = {2013}
}
@article{Zhou2013,
abstract = {Classical regression methods treat covariates as a vector and estimate a corresponding vector of regression coefficients. Modern applications in medical imaging generate covariates of more complex form such as multidimensional arrays (tensors). Traditional statistical and computational methods are proving insufficient for analysis of these high-throughput data due to their ultrahigh dimensionality as well as complex structure. In this article, we propose a new family of tensor regression models that efficiently exploit the special structure of tensor covariates. Under this framework, ultrahigh dimensionality is reduced to a manageable level, resulting in efficient estimation and prediction. A fast and highly scalable estimation algorithm is proposed for maximum likelihood estimation and its associated asymptotic properties are studied. Effectiveness of the new methods is demonstrated on both synthetic and real MRI imaging data.},
archivePrefix = {arXiv},
arxivId = {1203.3209},
author = {Zhou, Hua and Li, Lexin and Zhu, Hongtu},
doi = {10.1080/01621459.2013.776499},
eprint = {1203.3209},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, Li, Zhu - Unknown - Tensor Regression with Applications in Neuroimaging Data Analysis.pdf:pdf},
isbn = {978-1-909493-43-8},
issn = {01621459},
journal = {J. Am. Stat. Assoc.},
keywords = {Brain imaging,Dimension reduction,Generalized linear model,Magnetic resonance imaging,Multidimensional array,Tensor regression},
number = {502},
pages = {540--552},
pmid = {24791032},
title = {{Tensor regression with applications in neuroimaging data analysis}},
url = {http://www.tandfonline.com/doi/pdf/10.1080/01621459.2013.776499?needAccess=true},
volume = {108},
year = {2013}
}
@article{Plumbley2004,
abstract = {Epilepsy is one of the most common brain disorders and may result in brain dysfunction and cognitive disturbances. Epileptic seizures usually begin in childhood without being accommodated by brain damage and are tolerated by drugs that produce no brain dysfunction. In this study, cognitive function is evaluated in children with mild epileptic seizures controlled with common antiepileptic drugs. Under this prism, we propose a concise technical framework of combining and validating both linear and nonlinear methods to efficiently evaluate (in terms of synchronization) neurophysiological activity during a visual cognitive task consisting of fractal pattern observation. We investigate six measures of quantifying synchronous oscillatory activity based on different underlying assumptions. These measures include the coherence computed with the traditional formula and an alternative evaluation of it that relies on autoregressive models, an information theoretic measure known as minimum description length, a robust phase coupling measure known as phase-locking value, a reliable way of assessing generalized synchronization in state-space and an unbiased alternative called synchronization likelihood. Assessment is performed in three stages; initially, the nonlinear methods are validated on coupled nonlinear oscillators under increasing noise interference; second, surrogate data testing is performed to assess the possible nonlinear channel interdependencies of the acquired EEGs by comparing the synchronization indexes under the null hypothesis of stationary, linear dynamics; and finally, synchronization on the actual data is measured. The results on the actual data suggest that there is a significant difference between normal controls and epileptics, mostly apparent in occipital-parietal lobes during fractal observation tests.},
author = {Plumbley, Mark D. and Oja, Erkki},
doi = {10.1109/TNN.2003.820672},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Plumbley, Oja - 2004 - A nonnegative PCA algorithm for independent component analysis.pdf:pdf},
isbn = {1558-0032 (Electronic)$\backslash$r1089-7771 (Linking)},
issn = {10459227},
journal = {IEEE Trans. Neural Networks},
keywords = {Independent component analysis (ICA),Nonlinear principal component analysis (nonlinear,Nonnegative matrix factorization,Subspace learning rule},
number = {1},
pages = {66--76},
pmid = {19273019},
title = {{A "nonnegative PCA" algorithm for independent component analysis}},
volume = {15},
year = {2004}
}
@article{Smith2011,
abstract = {There is great interest in estimating brain "networks" from FMRI data. This is often attempted by identifying a set of functional "nodes" (e.g., spatial ROIs or ICA maps) and then conducting a connectivity analysis between the nodes, based on the FMRI timeseries associated with the nodes. Analysis methods range from very simple measures that consider just two nodes at a time (e.g., correlation between two nodes' timeseries) to sophisticated approaches that consider all nodes simultaneously and estimate one global network model (e.g., Bayes net models). Many different methods are being used in the literature, but almost none has been carefully validated or compared for use on FMRI timeseries data. In this work we generate rich, realistic simulated FMRI data for a wide range of underlying networks, experimental protocols and problematic confounds in the data, in order to compare different connectivity estimation approaches. Our results show that in general correlation-based approaches can be quite successful, methods based on higher-order statistics are less sensitive, and lag-based approaches perform very poorly. More specifically: there are several methods that can give high sensitivity to network connection detection on good quality FMRI data, in particular, partial correlation, regularised inverse covariance estimation and several Bayes net methods; however, accurate estimation of connection directionality is more difficult to achieve, though Patel's $\tau$can be reasonably successful. With respect to the various confounds added to the data, the most striking result was that the use of functionally inaccurate ROIs (when defining the network nodes and extracting their associated timeseries) is extremely damaging to network estimation; hence, results derived from inappropriate ROI definition (such as via structural atlases) should be regarded with great caution. {\textcopyright} 2010 Elsevier Inc.},
author = {Smith, Stephen M and Miller, Karla L and Salimi-Khorshidi, Gholamreza and Webster, Matthew and Beckmann, Christian F and Nichols, Thomas E and Ramsey, Joseph D and Woolrich, Mark W},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Smith et al. - 2011 - Network modelling methods for FMRI.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$r1053-8119 (Linking)},
issn = {10538119},
journal = {Neuroimage},
keywords = {Causality,FMRI,Network modelling},
number = {2},
pages = {875--891},
title = {{Network modelling methods for FMRI}},
volume = {54},
year = {2011}
}
@article{Lakshminarayanan,
abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1612.01474},
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
eprint = {1612.01474},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Lakshminarayanan, Pritzel, Blundell - Unknown - Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles(2).pdf:pdf},
title = {{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}},
url = {https://arxiv.org/pdf/1612.01474.pdf http://arxiv.org/abs/1612.01474},
year = {2016}
}
@article{Yu2016,
abstract = {Probabilistic graphical models have been widely used to model complex systems and aid scientific discoveries. As a result, there is a large body of literature focused on consistent model selection. However, scientists are often interested in understanding uncertainty associated with the estimated parameters, which current literature has not addressed thoroughly. In this paper, we propose a novel estimator for edge parameters for pairwise graphical models based on Hyv$\backslash$"arinen scoring rule. Hyv$\backslash$"arinen scoring rule is especially useful in cases where the normalizing constant cannot be obtained efficiently in a closed form. We prove that the estimator is {\$}\backslashsqrt{\{}n{\}}{\$}-consistent and asymptotically Normal. This result allows us to construct confidence intervals for edge parameters, as well as, hypothesis tests. We establish our results under conditions that are typically assumed in the literature for consistent estimation. However, we do not require that the estimator consistently recovers the graph structure. In particular, we prove that the asymptotic distribution of the estimator is robust to model selection mistakes and uniformly valid for a large number of data-generating processes. We illustrate validity of our estimator through extensive simulation studies.},
author = {Yu, Ming and Kolar, Mladen and Gupta, Varun},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Yu, Kolar, Gupta - 2016 - Statistical Inference for Pairwise Graphical Models Using Score Matching.pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {2829--2837},
title = {{Statistical Inference for Pairwise Graphical Models Using Score Matching}},
url = {https://papers.nips.cc/paper/6530-statistical-inference-for-pairwise-graphical-models-using-score-matching.pdf},
year = {2016}
}
@article{Yang2017a,
abstract = {We propose an alternative framework to existing setups for controlling false alarms when multiple A/B tests are run over time. This setup arises in many practical applications, e.g. when pharmaceutical companies test new treatment options against control pills for different diseases, or when internet companies test their default webpages versus various alternatives over time. Our framework proposes to replace a sequence of A/B tests by a sequence of best-arm MAB instances, which can be continuously monitored by the data scientist. When interleaving the MAB tests with an an online false discovery rate (FDR) algorithm, we can obtain the best of both worlds: low sample complexity and any time online FDR control. Our main contributions are: (i) to propose reasonable definitions of a null hypothesis for MAB instances; (ii) to demonstrate how one can derive an always-valid sequential p-value that allows continuous monitoring of each MAB test; and (iii) to show that using rejection thresholds of online-FDR algorithms as the confidence levels for the MAB algorithms results in both sample-optimality, high power and low FDR at any point in time. We run extensive simulations to verify our claims, and also report results on real data collected from the New Yorker Cartoon Caption contest.},
archivePrefix = {arXiv},
arxivId = {1706.05378},
author = {Yang, Fanny and Ramdas, Aaditya and Jamieson, Kevin and Wainwright, Martin J},
eprint = {1706.05378},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2017 - A framework for Multi-A(rmed)B(andit) testing with online FDR control(2).pdf:pdf},
journal = {NIPS},
title = {{A framework for Multi-A(rmed)/B(andit) testing with online FDR control}},
url = {https://arxiv.org/pdf/1706.05378.pdf http://arxiv.org/abs/1706.05378},
year = {2017}
}
@article{Steeg2014,
abstract = {We introduce a method to learn a hierarchy of successively more abstract representations of complex data based on optimizing an information-theoretic objective. Intuitively, the optimization searches for a set of latent factors that best explain the correlations in the data as measured by multivariate mutual information. The method is unsupervised, requires no model assumptions, and scales linearly with the number of variables which makes it an attractive approach for very high dimensional systems. We demonstrate that Correlation Explanation (CorEx) automatically discovers meaningful structure for data from diverse sources including personality tests, DNA, and human language.},
archivePrefix = {arXiv},
arxivId = {1406.1222},
author = {Steeg, Greg Ver and Galstyan, Aram},
eprint = {1406.1222},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Steeg, Galstyan - Unknown - Discovering Structure in High-Dimensional Data Through Correlation Explanation.pdf:pdf},
issn = {10495258},
journal = {NIPS},
title = {{Discovering Structure in High-Dimensional Data Through Correlation Explanation}},
url = {https://arxiv.org/pdf/1406.1222.pdf http://arxiv.org/abs/1406.1222},
year = {2014}
}
@article{Kolda2009,
abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition:CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
archivePrefix = {arXiv},
arxivId = {1404.3905},
author = {Kolda, Tamara G and Bader, Brett W},
doi = {10.1137/07070111X},
eprint = {1404.3905},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kolda, Bader - Unknown - Tensor Decompositions and Applications.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
journal = {SIAM Rev.},
keywords = {65F99,AMS subject classifications 15A69},
number = {3},
pages = {455--500},
title = {{Tensor Decompositions and Applications}},
url = {http://www.maths.manchester.ac.uk/{~}mlotz/teaching/nur/tensordecompositions.pdf http://epubs.siam.org/doi/10.1137/07070111X},
volume = {51},
year = {2009}
}
@article{Orabona2017,
abstract = {Deep learning methods achieve state-of-the-art performance in many application scenarios. Yet, these methods require a significant amount of hyperparameters tuning in order to achieve the best results. In particular, tuning the learning rates in the stochastic optimization process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function. Instead, we reduce the optimization process to a game of betting on a coin and propose a learning-rate-free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms.},
archivePrefix = {arXiv},
arxivId = {1705.07795},
author = {Orabona, Francesco and Tommasi, Tatiana},
eprint = {1705.07795},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Orabona, Tommasi - 2017 - Training Deep Networks without Learning Rates Through Coin Betting.pdf:pdf},
journal = {NIPS},
title = {{Training Deep Networks without Learning Rates Through Coin Betting}},
url = {https://arxiv.org/pdf/1705.07795.pdf http://arxiv.org/abs/1705.07795},
year = {2017}
}
@article{Duchi,
abstract = {Working under a model of privacy in which data remains private even from the statistician, we study the tradeoff between privacy guarantees and the risk of the resulting statistical estima-tors. We develop private versions of classical information-theoretic bounds, in particular those due to Le Cam, Fano, and Assouad. These inequalities allow for a precise characterization of statistical rates under local privacy constraints and the development of provably (minimax) op-timal estimation procedures. We provide a treatment of several canonical families of problems: mean estimation and median estimation, generalized linear models, and nonparametric density estimation. For all of these families, we provide lower and upper bounds that match up to con-stant factors, and exhibit new (optimal) privacy-preserving mechanisms and computationally efficient estimators that achieve the bounds. Additionally, we present a variety of experimental results for estimation problems involving sensitive data, including salaries, censored blog posts and articles, and drug abuse; these experiments demonstrate the importance of deriving optimal procedures.},
author = {Duchi, John and Jordan, Michael and Wainwright, Martin},
doi = {10.1080/01621459.2017.1389735},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Duchi, Jordan, Wainwright - Unknown - Minimax Optimal Procedures for Locally Private Estimation.pdf:pdf},
title = {{Minimax Optimal Procedures for Locally Private Estimation}},
url = {http://www.tandfonline.com/doi/pdf/10.1080/01621459.2017.1389735?needAccess=true}
}
@article{Bzdok2018,
abstract = {Two major goals in the study of biological systems are inference and prediction. Inference creates a mathematical model of the data-generation process to formalize understanding or test a hypothesis about how the system behaves. Prediction aims at forecasting unob-served outcomes or future behavior, such as whether a mouse with a given gene expression pattern has a disease. Prediction makes it possible to identify best courses of action (e.g., treatment choice) without requiring understanding of the underlying mechanisms. In a typical research project, both inference and prediction can be of value—we want to know how biological processes work and what will happen next. For example, we might want to infer which bio-logical processes are associated with the dysregulation of a gene in a disease, as well as detect whether a subject has the disease and predict the best therapy. Many methods from statistics and machine learning (ML) may, in principle, be used for both prediction and inference. However, statistical methods have a long-standing focus on inference, which is achieved through the creation and fitting of a project-specific prob-ability model. The model allows us to compute a quantitative mea-sure of confidence that a discovered relationship describes a 'true' effect that is unlikely to result from noise. Furthermore, if enough data are available, we can explicitly verify assumptions (e.g., equal variance) and refine the specified model, if needed. By contrast, ML concentrates on prediction by using general-pur-pose learning algorithms to find patterns in often rich and unwieldy data 1,2 . ML methods are particularly helpful when one is dealing with 'wide data' , where the number of input variables exceeds the number of subjects, in contrast to 'long data' , where the number of subjects is greater than that of input variables. ML makes minimal assumptions about the data-generating systems; they can be effec-tive even when the data are gathered without a carefully controlled experimental design and in the presence of complicated nonlinear interactions. However, despite convincing prediction results, the lack of an explicit model can make ML solutions difficult to directly relate to existing biological knowledge. Classical statistics and ML vary in computational tractability as the number of variables per subject increases. Classical statistical modeling was designed for data with a few dozen input variables and sample sizes that would be considered small to moderate today. In this scenario, the model fills in the unobserved aspects of the system. However, as the numbers of input variables and possible associa-tions among them increase, the model that captures these relation-ships becomes more complex. Consequently, statistical inferences become less precise and the boundary between statistical and ML approaches becomes hazier. To compare traditional statistics to ML approaches, we'll use a simulation of the expression of 40 genes in two phenotypes (−/+). Mean gene expression will differ between phenotypes, but we'll set up the simulation so that the mean difference for the first 30 genes is not related to phenotype. The last ten genes will be dysregulated, with systematic differences in mean expression between phenotypes. To achieve this, we assign each gene an average log expression that is the same for both phenotypes. The dysregulated genes (31−40, labeled A−J) have their mean expression perturbed in the + pheno-type (Fig. 1a). Using these average expression values, we simulate an RNA-seq experiment in which the observed counts for each gene are sampled from a Poisson distribution with mean exp(x + $\epsilon$), where x is the mean log expression, unique to the gene and phenotype, and $\epsilon$ {\~{}} N(0, 0.15) acts as biological variability that varies from subject to subject (Fig. 1b). For genes 1−30, which do not have differential expression, the z-scores are approximately N(0, 1). For the dysregu-lated genes, which do have differential expression, the z-scores in one phenotype tend to be positive, and the z-scores in the other tend to be negative. Our goal in the simulation is to identify which genes are asso-ciated with the abnormal phenotype. We'll formally test the null hypothesis that the mean expression differs by phenotype with a Figure 2 | Analysis of gene ranking by classical inference and ML. (a) Unadjusted log-scaled P values from statistical differential expression analysis as a function of effect size, measured by fold change in expression. (b) Log-scaled P values from a as a function of gene importance from random forest classification. In a and b, red circles identify the ten differentially expressed genes from Figure 1; the remaining genes are indicated by open circles. (c) Distribution of the number of dysregulated genes correctly identified in 1,000 simulations by inference (gray fill) and random forest (black line).},
author = {Bzdok, Danilo and Altman, Naomi and Krzywinski, Martin},
doi = {10.1038/nmeth.4642},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bzdok, Altman, Krzywinski - 2018 - Statistics versus machine learning Statistics draws population inferences from a sample, and machi(2).pdf:pdf},
issn = {1548-7091},
journal = {Nature},
title = {{Statistics versus machine learning Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns}},
url = {https://www.nature.com/articles/nmeth.4642.pdf},
volume = {15},
year = {2018}
}
@article{Fox2010,
abstract = {During resting conditions the brain remains functionally and metabolically active. One manifestation of this activity that has become an important research tool is spontaneous fluctuations in the blood oxygen level-dependent (BOLD) signal of functional magnetic resonance imaging (fMRI). The identification of correlation patterns in these spontaneous fluctuations has been termed resting state functional connectivity (fcMRI) and has the potential to greatly increase the translation of fMRI into clinical care. In this article we review the advantages of the resting state signal for clinical applications including detailed discussion of signal to noise considerations. We include guidelines for performing resting state research on clinical populations, outline the different areas for clinical application, and identify important barriers to be addressed to facilitate the translation of resting state fcMRI into the clinical realm.},
author = {Fox, Michael D. and Greicius, Michael},
file = {:Users/ricardo/Downloads/fnsys{\_}2010{\_}00019.pdf:pdf},
journal = {Front. Syst. Neurosci.},
keywords = {brain,fcmri,fmri,intrinsic activity,neurological disease,psychiatric disease,spontaneous activity},
title = {{Clinical applications of resting state functional connectivity}},
volume = {4},
year = {2010}
}
@article{Cox2018,
abstract = {A broad review is given of the impact of big data on various aspects of investigation. There is some but not total emphasis on issues in epidemiological research.},
author = {Cox, D. R. and Kartsonaki, Christiana and Keogh, Ruth H.},
doi = {10.1016/j.spl.2018.02.015},
file = {:Users/ricardo/Downloads/1-s2.0-S0167715218300609-main.pdf:pdf},
issn = {01677152},
journal = {Stat. Probab. Lett.},
keywords = {Big data,Electronic health records,Epidemiology,Metrology,Precision},
pages = {111--115},
publisher = {Elsevier B.V.},
title = {{Big data: Some statistical issues}},
url = {https://doi.org/10.1016/j.spl.2018.02.015},
volume = {136},
year = {2018}
}
@article{Miconi2018,
abstract = {How can we build agents that keep learning from experience, quickly and efficiently, after their initial training? Here we take inspiration from the main mechanism of learning in biological brains: synaptic plasticity, carefully tuned by evolution to produce efficient lifelong learning. We show that plasticity, just like connection weights, can be optimized by gradient descent in large (millions of parameters) recurrent networks with Hebbian plastic connections. First, recurrent plastic networks with more than two million parameters can be trained to memorize and reconstruct sets of novel, high-dimensional 1000+ pixels natural images not seen during training. Crucially, traditional non-plastic recurrent networks fail to solve this task. Furthermore, trained plastic networks can also solve generic meta-learning tasks such as the Omniglot task, with competitive results and little parameter overhead. Finally, in reinforcement learning settings, plastic networks outperform a non-plastic equivalent in a maze exploration task. We conclude that differentiable plasticity may provide a powerful novel approach to the learning-to-learn problem.},
archivePrefix = {arXiv},
arxivId = {1804.02464},
author = {Miconi, Thomas and Clune, Jeff and Stanley, Kenneth O},
eprint = {1804.02464},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Miconi, Clune, Stanley - 2018 - Differentiable plasticity training plastic neural networks with backpropagation.pdf:pdf},
title = {{Differentiable plasticity: training plastic neural networks with backpropagation}},
url = {https://arxiv.org/pdf/1804.02464.pdf http://arxiv.org/abs/1804.02464},
year = {2018}
}
@article{Pearl2018,
abstract = {Systems that operate in purely statistical mode of inference entails theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human-level intelligence, learning machines need the guidance of a model of external reality, similar to the ones used in causal inference tasks. To demonstrate the essential role of such models, this paper presents a summary of seven tasks which are beyond reach of associational learning systems and which have been accomplished using the tools of causal modeling.},
author = {Pearl, Judea},
doi = {10.1145/nnnnnnn},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - 2018 - The Seven Tools of Causal Inference with Reflections on Machine Learning(2).pdf:pdf},
journal = {ACM Ref. Format Jud. Pearl},
number = {1},
title = {{The Seven Tools of Causal Inference with Reflections on Machine Learning}},
url = {https://doi.org/10.1145/nnnnnnn.},
volume = {1},
year = {2018}
}
@inproceedings{Joshi2017,
abstract = {Segmentation of anatomy on abdominal CT enables patient-specific image guidance in clinical endoscopic procedures and in endoscopy training. Because robust interpatient registration of abdom-inal images is necessary for existing multi-atlas-and statistical-shape-model-based segmentations, but remains challenging, there is a need for automated multi-organ segmentation that does not rely on regis-tration. We present a deep-learning-based algorithm for segmenting the liver, pancreas, stomach, and esophagus using dilated convolution units with dense skip connections and a new spatial prior. The algorithm was evaluated with an 8-fold cross-validation and compared to a joint-label-fusion-based segmentation based on Dice scores and boundary distances. The proposed algorithm yielded more accurate segmentations than the joint-label-fusion-based algorithm for the pancreas (median Dice scores 66 vs 37), stomach (83 vs 72) and esophagus (73 vs 54) and marginally less accurate segmentation for the liver (92 vs 93). We conclude that dilated convolutional networks with dense skip connections can segment the liver, pancreas, stomach and esophagus from abdominal CT with-out image registration and have the potential to support image-guided navigation in gastrointestinal endoscopy procedures.},
author = {Joshi, Anand A and Chong, Minqi and Leahy, Richard M},
booktitle = {MICCAI},
doi = {10.1007/978-3-319-66182-7},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Joshi, Chong, Leahy - Unknown - BrainSync An Orthogonal Transformation for Synchronization of fMRI Data Across Subjects.pdf:pdf},
isbn = {978-3-319-66181-0},
issn = {978-3-319-66181-0},
title = {{Medical Image Computing and Computer Assisted Intervention − MICCAI 2017}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F978-3-319-66182-7{\_}56.pdf http://link.springer.com/10.1007/978-3-319-66182-7},
volume = {10433},
year = {2017}
}
@article{Gretton2005b,
abstract = {We propose an independence criterion based on the eigen-spectrum of covariance operators in reproducing kernel Hilbert spaces (RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or HSIC). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly defined population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on HSIC do not suffer from slow learning rates. Finally, we show in the context of independent component analysis (ICA) that the performance of HSIC is competitive with that of previously published kernel-based criteria, and of other recently published ICA methods.},
author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gretton et al. - Unknown - LNAI 3734 - Measuring Statistical Dependence with Hilbert-Schmidt Norms.pdf:pdf},
journal = {Int. Conf. Algorithmic Learn. Theory},
pages = {63--77},
title = {{Measuring Statistical Dependence with Hilbert-Schmidt Norms}},
year = {2005}
}
@book{Hastie2016,
author = {Hastie, Trevor and Tibshirani, Robert and Hastie, Martin Wainwright},
publisher = {Monographs on Statistics and Applied Probability},
title = {{Statistical Learning with Sparsity}},
year = {2016}
}
@article{Sgouritsa2015,
abstract = {We address the problem of causal discov-ery in the two-variable case given a sample from their joint distribution. The proposed method is based on a known assumption that, if X → Y (X causes Y), the marginal distri-bution of the cause, P (X), contains no in-formation about the conditional distribution P (Y |X). Consequently, estimating P (Y |X) from P (X) should not be possible. However, estimating P (X|Y) based on P (Y) may be possible. This paper employs this asymmetry to pro-pose CURE, a causal discovery method which decides upon the causal direction by com-paring the accuracy of the estimations of P (Y |X) and P (X|Y). To this end, we pro-pose a method for estimating a conditional from samples of the corresponding marginal, which we call unsupervised inverse GP re-gression. We evaluate CURE on synthetic and real data. On the latter, our method outperforms existing causal inference meth-ods.},
author = {Sgouritsa, Eleni and Janzing, Dominik and Hennig, Philipp and Schoelkopf, Bernhard},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Sgouritsa et al. - Unknown - Inference of Cause and Effect with Unsupervised Inverse Regression.pdf:pdf},
issn = {15337928},
journal = {Proc. 18th Int. Conf. Artif. Intell. Stat.},
pages = {847--855},
title = {{Inference of Cause and Effect with Unsupervised Inverse Regression}},
url = {http://proceedings.mlr.press/v38/sgouritsa15.pdf},
volume = {38},
year = {2015}
}
@article{Hershman2008,
abstract = {Although well studied in vitro, the in vivo functions of G-quadruplexes (G4-DNA and G4-RNA) are only beginning to be defined. Recent studies have demonstrated enrichment for sequences with intramolecular G-quadruplex forming potential (QFP) in transcriptional promoters of humans, chickens and bacteria. Here we survey the yeast genome for QFP sequences and similarly find strong enrichment for these sequences in upstream promoter regions, as well as weaker but significant enrichment in open reading frames (ORFs). Further, four findings are consistent with roles for QFP sequences in transcriptional regulation. First, QFP is correlated with upstream promoter regions with low histone occupancy. Second, treatment of cells with N-methyl mesoporphyrin IX (NMM), which binds G-quadruplexes selectively in vitro, causes significant upregulation of loci with QFP-possessing promoters or ORFs. NMM also causes downregulation of loci connected with the function of the ribosomal DNA (rDNA), which itself has high QFP. Third, ORFs with QFP are selectively downregulated in sgs1 mutants that lack the G4-DNA-unwinding helicase Sgs1p. Fourth, a screen for yeast mutants that enhance or suppress growth inhibition by NMM revealed enrichment for chromatin and transcriptional regulators, as well as telomere maintenance factors. These findings raise the possibility that QFP sequences form bona fide G-quadruplexes in vivo and thus regulate transcription.},
author = {Hershman, Steve G. and Chen, Qijun and Lee, Julia Y. and Kozak, Marina L. and Yue, Peng and Wang, Li San and Johnson, F. Brad},
doi = {10.1093/nar/gkm986},
file = {:Users/ricardo/Downloads/gkm986.pdf:pdf},
isbn = {1362-4962 (Electronic)$\backslash$r0305-1048 (Linking)},
issn = {03051048},
journal = {Nucleic Acids Res.},
number = {1},
pages = {144--156},
pmid = {17999996},
title = {{Genomic distribution and functional analyses of potential G-quadruplex-forming sequences in Saccharomyces cerevisiae}},
volume = {36},
year = {2008}
}
@article{Roweis,
abstract = {I present an expectation-maximization (EM) algorithm for principal component analysis (PCA). The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data. It is computationally very efficient in space and time. It also natu-rally accommodates missing infonnation. I also introduce a new variant of PC A called sensible principal component analysis (SPCA) which de-fines a proper density model in the data space. Learning for SPCA is also done with an EM algorithm. I report results on synthetic and real data showing that these EM algorithms correctly and efficiently find the lead-ing eigenvectors of the covariance of datasets in a few iterations using up to hundreds of thousands of datapoints in thousands of dimensions.},
author = {Roweis, Sam},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Roweis{\textperiodcentered} - Unknown - EM Algorithms for PCA and SPCA(2).pdf:pdf},
journal = {Adv. Neural Inf. Process. Syst.},
title = {{EM Algorithms for PCA and SPCA}},
url = {http://papers.nips.cc/paper/1398-em-algorithms-for-pca-and-spca.pdf},
year = {1998}
}
@article{West2003,
abstract = {I discuss Bayesian factor regression models with many explanatory variables. These models are of particular interest and applicability in problems of prediction, but also for elucidating underlying structure in predictor variables. One key motivating application here is in studies of gene expression in functional genomics. I first discuss empirical factor (principal compo- nents) regression, and the use of general classes of shrinkage priors, with an example. These models raise foundational questions for Bayesians, and related practical issues, due to the use of design-dependent priors and the need to recover inferences on the effects of the original, high-dimensional predictors. I then discuss latent factor models for high-dimensional variables, and regression approaches in which low-dimensional latent factors are the predictor variables. These models generalise empirical factor regression, provide for more incisive evaluation of fac- tor structure underlying high-dimensional predictors, and resolve the modelling and practical issues in empirical factor models by casting the latter as limiting special cases. Finally, I turn to questions of prior specification in these models, and introduce sparse latent factor models to induce sparsity in factor loadings matrices. Embedding such sparse latent factor models in factor regressions provides a novel approach to variable selection with very many predictors. The paper concludes with an example of sparse factor analysis of gene expression data and comments about further research.},
author = {West, Mike},
doi = {10.1.1.18.3036},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bernardo et al. - Unknown - Bayesian Factor Regression Models in the {\&}quot Large p, Small n {\&}quot Paradigm.pdf:pdf},
isbn = {978-0-19-852615-5},
issn = {08966273},
journal = {Bayesian Stat. 7 - Proc. Seventh Val. Int. Meet.},
keywords = {1,covariates,dimension reduction,empirical factor regression models,gene expression analysis,high-dimensional,latent factor models,shrinkage priors},
pages = {723--732},
pmid = {12495626},
title = {{Bayesian factor regression models in the “large p, small n” paradigm}},
url = {http://www2.stat.duke.edu/{~}sayan/SAMSI/lec/02-12.pdf http://www.isds.duke.edu/courses/Spring06/sta376/Support/RegressionETC/v7.paper.pdf},
volume = {7},
year = {2003}
}
@article{Bouchard2013,
abstract = {In many applications, multiple interlinked sources of data are available and they cannot be represented by a single adjacency matrix, to which large scale factorization method could be applied. Collective matrix factorization is a simple yet powerful approach to jointly factorize multiple matrices, each of which represents a relation between two entity types. Existing algorithms to estimate parameters of collective matrix factorization models are based on non-convex formulations of the problem; in this paper, a convex formulation of this approach is proposed. This enables the derivation of large scale algorithms to estimate the parameters, including an iterative eigenvalue thresholding algorithm. Numerical experiments illustrate the benefits of this new approach.},
author = {Bouchard, Guillaume and Yin, Dawei and Guo, Shengbo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Bouchard, Guo, Yin - Unknown - Convex Collective Matrix Factorization.pdf:pdf},
issn = {15337928},
journal = {Proc. Sixt. Int. Conf. Artif. Intell. Stat.},
pages = {144--152},
title = {{Convex Collective Matrix Factorization}},
url = {http://proceedings.mlr.press/v31/bouchard13a.pdf http://jmlr.org/proceedings/papers/v31/bouchard13a.html},
volume = {31},
year = {2013}
}
@article{Kusner2017,
abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
archivePrefix = {arXiv},
arxivId = {1703.06856},
author = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
eprint = {1703.06856},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kusner et al. - Unknown - Counterfactual Fairness(3).pdf:pdf},
journal = {Neural Inf. Process. Syst.},
title = {{Counterfactual Fairness}},
year = {2017}
}
@incollection{Tillman2009,
abstract = {The recently proposed emph{\{}additive noise model{\}} has advantages over previous structure learning algorithms, when attempting to recover some true data generating mechanism, since it (i) does not assume linearity or Gaussianity and (ii) can recover a unique DAG rather than an equivalence class. However, its original extension to the multivariate case required enumerating all possible DAGs, and for some special distributions, e.g. linear Gaussian, the model is invertible and thus cannot be used for structure learning. We present a new approach which combines a PC style search using recent advances in kernel measures of conditional dependence with local searches for additive noise models in substructures of the equivalence class. This results in a more computationally efficient approach that is useful for arbitrary distributions even when additive noise models are invertible. Experiments with synthetic and real data show that this method is more accurate than previous methods when data are nonlinear and/or non-Gaussian.},
author = {Tillman, Robert and Gretton, Arthur and Spirtes, Peter},
booktitle = {Adv. Neural Process. Inf. Syst.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Tillman, Gretton, Spirtes - Unknown - Nonlinear directed acyclic structure learning with weakly additive noise models(2).pdf:pdf},
keywords = {causal learning},
pages = {1847--1855},
title = {{Nonlinear directed acyclic structure learning with weakly additive noise models}},
year = {2009}
}
@article{Journee2010,
abstract = {In this paper we develop a new approach to sparse principal component analysis (sparse PCA). We propose two single-unit and two block optimization formulations of the sparse PCA problem, aimed at extracting a single sparse dominant principal component of a data matrix, or more components at once, respectively. While the initial formulations involve nonconvex functions, and are therefore computationally intractable, we rewrite them into the form of an optimization program involving maximization of a convex function on a compact set. The dimension of the search space is decreased enormously if the data matrix has many more columns (variables) than rows. We then propose and analyze a simple gradient method suited for the task. It appears that our algorithm has best convergence properties in the case when either the objective function or the feasible set are strongly convex, which is the case with our single-unit formulations and can be enforced in the block case. Finally, we demonstrate numerically on a set of random and gene expression test problems that our approach outperforms existing algorithms both in quality of the obtained solution and in computational speed.},
archivePrefix = {arXiv},
arxivId = {0811.4724},
author = {Journ{\'{e}}e, M and Nesterov, Yurii and Richtarik, Peter and Sepulchre, Rodolphe},
doi = {10.1162/neco.2009.02-08-706},
eprint = {0811.4724},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Journ{\'{e}}e MJOURNEE et al. - 2010 - Generalized Power Method for Sparse Principal Component Analysis.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {block algorithms,gradient ascent,power method,sparse PCA,strongly convex sets},
pages = {517--553},
pmid = {19686071},
title = {{Generalized power method for sparse principal component analysis}},
url = {http://www.jmlr.org/papers/volume11/journee10a/journee10a.pdf http://dl.acm.org/citation.cfm?id=1756021},
volume = {11},
year = {2010}
}
@article{Danaher2014,
abstract = {We consider the problem of estimating multiple related but distinct graphical models on the basis of a high-dimensional data set with observations that belong to distinct classes. A motivating example occurs in the analysis of gene expression data for tissue samples with and without cancer. In this case, we might wish to estimate a gene expression network for the normal tissue and a gene expression network for the tumor tissue. We expect the two gene expression networks to be similar but not identical to each other, and so more accurate estimation of these two networks may be possible using a joint approach. We propose the joint graphical lasso for this purpose. Rather than estimating a graphical model for each class separately, or a single graphical model across all classes, we borrow strength across the classes in order to estimate multiple graphical models that share certain characteristics, such as the locations or weights of nonzero edges. Our approach is based upon maximizing a penalized log likelihood. We employ fused lasso or group lasso penalties, and implement a very fast computational approach that solves the joint graphical lasso problem. In a simulation study we demonstrate that our proposed approach leads to more accurate estimation of networks and covariance structure than competing approaches. We further illustrate our proposal on a publicly-available lung cancer gene expression data set.},
author = {Danaher, Patrick and Wang, Pei and Witten, Daniela M.},
file = {:Users/ricardo/Downloads/Danaher{\_}et{\_}al-2014-Journal{\_}of{\_}the{\_}Royal{\_}Statistical{\_}Society{\_}{\_}Series{\_}B{\_}(Statistical{\_}Methodology).pdf:pdf},
journal = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
keywords = {Alternating directions method of multipliers,Gaussian graphical model,Generalized fused lasso,Graphical lasso,Group lasso,High dimensional data,Network estimation},
number = {2},
pages = {373--397},
title = {{The joint graphical lasso for inverse covariance estimation across multiple classes}},
volume = {76},
year = {2014}
}
@article{Papernot2016,
abstract = {Deep Learning is increasingly used in several machine learning tasks as Deep Neural Networks (DNNs) frequently outperform other techniques. Yet, previous work showed that, once deployed, DNNs are vulnerable to integrity attacks. Indeed, adversaries can control DNN outputs and for instance force them to misclassify inputs by adding a carefully crafted and undistinguishable perturbation. However, these attacks assumed knowledge of the targeted DNN's architecture and parameters. In this paper however, we release these assumptions and introduce an attack conducted under the more realistic, yet more complex, threat model of an oracle: adversaries are only capable of accessing DNN label predictions for chosen inputs. We evaluate our attack in real-world settings by successfully forcing an oracle served by MetaMind, an online API for DNN classifiers, to misclassify inputs at a 84.24{\%} rate. We also perform an evaluation to fine-tune our attack strategy and maximize the oracle's misclassification rate for adversarial samples.},
archivePrefix = {arXiv},
arxivId = {1602.02697},
author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
doi = {10.1145/3052973.3053009},
eprint = {1602.02697},
isbn = {9781450349444},
journal = {arXiv},
title = {{Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples}},
url = {http://arxiv.org/abs/1602.02697},
year = {2016}
}
@article{Muller2018,
author = {Muller, Lyle and Chavane, Fr{\'{e}}d{\'{e}}ric and Reynolds, John and Sejnowski, Terrence J},
doi = {10.1038/nrn.2018.20},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Muller et al. - 2018 - Synchrony in the activity of neuronal populations has long been of interest in neuroscience.pdf:pdf},
journal = {Nat. Publ. Gr.},
title = {{Synchrony in the activity of neuronal populations has long been of interest in neuroscience}},
url = {https://www.nature.com/articles/nrn.2018.20.pdf?WT.ec{\_}id=NRN-201805},
volume = {19},
year = {2018}
}
@article{Mitra,
abstract = {The discovery that spontaneous fluctuations in blood oxygen level-dependent (BOLD) signals contain information about the functional organization of the brain has caused a paradigm shift in neuroimaging. It is now well established that intrinsic brain activity is organized into spatially segregated resting-state networks (RSNs). Less is known regarding how spatially segregated networks are integrated by the propagation of intrinsic activity over time. To explore this question, we examined the latency structure of spontaneous fluctuations in the fMRI BOLD signal. Our data reveal that intrinsic activity propagates through and across networks on a timescale of ∼1 s. Variations in the latency structure of this activity resulting from sensory state manipulation (eyes open vs. closed), antecedent motor task (button press) performance, and time of day (morning vs. evening) suggest that BOLD signal lags reflect neuronal processes rather than hemodynamic delay. Our results emphasize the importance of the temporal structure of the brain's spontaneous activity.},
author = {Mitra, A and Snyder, A Z and Hacker, C D and Raichle, M E},
doi = {10.1152/jn.00804.2013},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Mitra et al. - Unknown - Lag structure in resting-state fMRI.pdf:pdf},
isbn = {1522-1598 (Electronic)$\backslash$r0022-3077 (Linking)},
issn = {0022-3077},
journal = {J. Neurophysiol.},
number = {11},
pages = {2374--2391},
pmid = {24598530},
title = {{Lag structure in resting-state fMRI}},
url = {http://www.physiology.org/doi/pdf/10.1152/jn.00804.2013 http://jn.physiology.org/cgi/doi/10.1152/jn.00804.2013},
volume = {111},
year = {2014}
}
@article{Tootoonian2014,
abstract = {We study the early locust olfactory system in an attempt to explain its well-characterized structure and dynamics. We first propose its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements. Detailed experimental knowledge about this system rules out standard algorithmic solutions to this problem. Instead, we show that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data. Further biological constraints lead us to a reduced form of this dual formulation in which the system uses in-dependent component analysis to continuously adapt to its olfactory environment to allow accurate sparse recovery. Our work demonstrates the challenges and re-wards of attempting detailed understanding of experimentally well-characterized systems.},
author = {Tootoonian, Sina and Lengyel, Mate},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Tootoonian, Lengyel - Unknown - A Dual Algorithm for Olfactory Computation in the Locust Brain(2).pdf:pdf},
journal = {NIPS},
number = {Section 5},
pages = {1--9},
title = {{A Dual Algorithm for Olfactory Computation in the Locust Brain}},
url = {http://papers.nips.cc/paper/5400-a-dual-algorithm-for-olfactory-computation-in-the-locust-brain.pdf},
year = {2014}
}
@article{Madry2017,
abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
archivePrefix = {arXiv},
arxivId = {1706.06083},
author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
doi = {10.1227/01.NEU.0000255452.20602.C9},
eprint = {1706.06083},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/M{\c{a}}dry et al. - Unknown - Towards Deep Learning Models Resistant to Adversarial Attacks.pdf:pdf},
isbn = {9781405161251},
issn = {1607-551X},
pmid = {17460516},
title = {{Towards Deep Learning Models Resistant to Adversarial Attacks}},
url = {https://github.com/MadryLab/mnist{\_}challenge http://arxiv.org/abs/1706.06083},
year = {2017}
}
@article{Lohmann2012,
abstract = {Dynamic causal modelling (DCM) (Friston et al., 2003) is a technique designed to investigate the influence between brain areas using time series data obtained by EEG/MEG or functional magnetic resonance imaging (fMRI). The basic idea is to fit various models to time series data, and select one of those models using Bayesian model comparison. Here, we present a critical evaluation of DCM in which we show that DCM can be challenged on several grounds. We will discuss three main points relating to combinatorial explosion, the validity of the model selection procedure, and problems with respect to model validation. {\textcopyright} 2011 Elsevier Inc.},
author = {Lohmann, Gabriele and Erfurth, Kerstin and M{\"{u}}ller, Karsten and Turner, Robert},
doi = {10.1016/j.neuroimage.2011.09.025},
file = {:Users/ricardo/Documents/Projects/Score matching GGM/Writing/General/UAI/NIPS 2018/1-s2.0-S1053811911010718-main.pdf:pdf},
isbn = {1095-9572 (Electronic){\$}\backslash{\$}n1053-8119 (Linking)},
issn = {10538119},
journal = {Neuroimage},
keywords = {Combinatorial explosion,Dynamic causal modelling,Model validation},
number = {3},
pages = {2322--2329},
pmid = {22001162},
publisher = {Elsevier Inc.},
title = {{Critical comments on dynamic causal modelling}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2011.09.025},
volume = {59},
year = {2012}
}
@article{Ablin2017,
abstract = {Independent Component Analysis (ICA) is a technique for unsupervised exploration of multi-channel data that is widely used in observational sciences. In its classic form, ICA relies on modeling the data as linear mixtures of non-Gaussian independent sources. The maximization of the corresponding likelihood is a challenging problem if it has to be completed quickly and accurately on large sets of real data. We introduce the Preconditioned ICA for Real Data (Picard) algorithm, which is a relative L-BFGS algorithm preconditioned with sparse Hessian approximations. Extensive numerical comparisons to several algorithms of the same class demonstrate the superior performance of the proposed technique, especially on real data, for which the ICA model does not necessarily hold.},
archivePrefix = {arXiv},
arxivId = {1706.08171},
author = {Ablin, Pierre and Cardoso, Jean-Fran{\c{c}}ois and Gramfort, Alexandre},
doi = {10.1109/TSP.2018.2844203},
eprint = {1706.08171},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ablin, Cardoso, Gramfort - 2017 - Faster independent component analysis by preconditioning with Hessian approximations.pdf:pdf},
issn = {1053587X},
journal = {IEEE Trans. Signal Process.},
keywords = {Blind source separation,Independent Component Analysis,maximum likelihood estimation,preconditioning,quasi-Newton methods,second order methods},
title = {{Faster independent component analysis by preconditioning with Hessian approximations}},
url = {https://arxiv.org/pdf/1706.08171.pdf http://arxiv.org/abs/1706.08171},
year = {2017}
}
@article{Smith2018,
author = {Smith, Stephen M and Nichols, Thomas E},
doi = {10.1016/j.neuron.2017.12.018},
file = {:Users/ricardo/Downloads/1-s2.0-S0896627317311418-main.pdf:pdf},
issn = {08966273},
journal = {Neuron},
number = {2},
pages = {263--268},
publisher = {Elsevier Inc.},
title = {{Statistical Challenges in “Big Data” Human Neuroimaging}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627317311418},
volume = {97},
year = {2018}
}
@article{Schiefer2017,
abstract = {Knowing brain connectivity is of great importance both in basic research and for clinical applications. We are proposing a method to infer directed connectivity from zero-lag covariances of neuronal activity recorded at multiple sites. This allows us to identify causal relations that are reflected in neuronal population activity. To derive our strategy, we assume a generic linear model of interacting continuous variables, the components of which represent the activity of local neuronal populations. The suggested method for inferring connectivity from recorded signals exploits the fact that the covariance matrix derived from the observed activity contains information about the existence, the direction and the sign of connections. Assuming a sparsely coupled network, we disambiguate the underlying causal structure via {\$}L{\^{}}1{\$}-minimization. In general, this method is suited to infer effective connectivity from resting state data of various types. We show that our method is applicable over a broad range of structural parameters regarding network size and connection probability of the network. We also explored parameters affecting its activity dynamics, like the eigenvalue spectrum. Also, based on the simulation of suitable Ornstein-Uhlenbeck processes to model BOLD dynamics, we show that with our method it is possible to estimate directed connectivity from zero-lag covariances derived from such signals. In this study, we consider measurement noise and unobserved nodes as additional confounding factors. Furthermore, we investigate the amount of data required for a reliable estimate. Additionally, we apply the proposed method on a fMRI dataset. The resulting network exhibits a tendency for close-by areas being connected as well as inter-hemispheric connections between corresponding areas. Also, we found that a large fraction of identified connections were inhibitory.},
archivePrefix = {arXiv},
arxivId = {1708.02423},
author = {Schiefer, Jonathan and Niederb{\"{u}}hl, Alexander and Pernice, Volker and Lennartz, Carolin and LeVan, Pierre and Henning, J{\"{u}}rgen and Rotter, Stefan},
eprint = {1708.02423},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Schiefer et al. - 2017 - From Correlation to Causation Estimation of Effective Connectivity from Continuous Brain Signals based on Zero-.pdf:pdf;:Users/ricardo/Downloads/journal.pcbi.1006056.pdf:pdf},
isbn = {1111111111},
pages = {1--18},
title = {{From Correlation to Causation: Estimation of Effective Connectivity from Continuous Brain Signals based on Zero-Lag Covariance}},
url = {http://arxiv.org/abs/1708.02423},
year = {2017}
}
@article{Witten2009,
abstract = {SUMMARY We present a penalized matrix decomposition (PMD), a new framework for computing a rank-K approximation for a matrix. We approximate the matrix X a X = K k=1 d k u k v T k , where d k , u k , and v k minimize the squared Frobenius norm of X X, subject to penalties on u k and v k . This results in a regularized version of the singular value decomposition. Of particular interest is the use of L 1 -penalties on u k and v k , which yields a decomposition of X using sparse vectors. We show that when the PMD is applied using an L 1 -penalty on v k but not on u k , a method for sparse principal components results. In fact, this yields an efficient algorithm for the " SCoTLASS " proposal (Jolliffe and others 2003) for obtain-ing sparse principal components. This method is demonstrated on a publicly available gene expression data set. We also establish connections between the SCoTLASS method for sparse principal component analysis and the method of Zou and others (2006). In addition, we show that when the PMD is applied to a cross-products matrix, it results in a method for penalized canonical correlation analysis (CCA). We apply this penalized CCA method to simulated data and to a genomic data set consisting of gene expression and DNA copy number measurements on the same set of samples.},
author = {Witten, Daniela M. and Tibshirani, Robert and Hastie, Trevor},
doi = {10.1093/biostatistics/kxp008},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Witten, Tibshirani, Hastie - 2009 - A penalized matrix decomposition, with applications to sparse principal components and canonical cor.pdf:pdf},
isbn = {1468-4357 (Electronic)$\backslash$r1465-4644 (Linking)},
issn = {14654644},
journal = {Biostatistics},
keywords = {Canonical correlation analysis,DNA copy number,Integrative genomic analysis,L1,Matrix decomposition,Principal component analysis,SVD,Sparse principal component analysis},
month = {jul},
number = {3},
pages = {515--534},
pmid = {19377034},
publisher = {Oxford University Press},
title = {{A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis}},
url = {https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxp008},
volume = {10},
year = {2009}
}
@article{Gelman,
abstract = {The missionary zeal of many Bayesians of old has been matched, in the other direction, by an attitude among some theoreticians that Bayesian methods were absurd—not merely misguided but obviously wrong in principle. We consider sev-eral examples, beginning with Feller's classic text on probability theory and continuing with more recent cases such as the per-ceived Bayesian nature of the so-called doomsday argument. We analyze in this note the intellectual background behind various misconceptions about Bayesian statistics, without aiming at a complete historical coverage of the reasons for this dismissal. 1. A VIEW FROM 1950 Younger readers of this journal may not be fully aware of the passionate battles over Bayesian inference among statisti-cians in the last half of the twentieth century. During this pe-riod, the missionary zeal of many Bayesians was matched, in the other direction, by a view among some theoreticians that Bayesian methods are absurd—not merely misguided but obvi-ously wrong in principle. Such anti-Bayesianism could hardly be maintained in the present era, given the many recent practical successes of Bayesian methods. But by examining the historical background of these beliefs, we may gain some insight into the statistical debates of today. Feller's first volume in college, after taking probability but before taking any statistics courses. The second author's (CPR) research is partly supported by the Agence Nationale de la Recherche (ANR, 212, rue de Bercy 75012 Paris) through the 2007–2010 grant ANR-07-BLAN-0237 " SPBayes. " He remembers buying Feller's first volume in a bookstore in Ann Arbor during a Bayesian econometrics conference where he was kindly supported by Jim Berger.},
author = {Gelman, Andrew and Robert, Christian P and Christian, ) and Robert, P},
doi = {10.1080/00031305.2013.760987},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gelman et al. - Unknown - General {\&}quot Not Only Defended But Also Applied {\&}quot The Perceived Absurdity of Bayesian Inference.pdf:pdf},
keywords = {Bogosity,Doomsdsay argument,Foundations,Frequentist,Laplace law of succession},
title = {{"Not Only Defended But Also Applied" The Perceived Absurdity of Bayesian Inference}},
url = {http://www.stat.columbia.edu/{~}gelman/research/published/feller8.pdf}
}
@article{Loh2014,
abstract = {We establish a new framework for statistical estimation of directed acyclic graphs (DAGs) when data are generated from a linear, possibly non-Gaussian structural equation model. Our framework consists of two parts: (1) inferring the moralized graph from the support of the inverse covariance matrix; and (2) selecting the best-scoring graph amongst DAGs that are consistent with the moralized graph. We show that when the error variances are known or estimated to close enough precision, the true DAG is the unique minimizer of the score computed using the reweighted squared l{\_}2-loss. Our population-level results have implications for the identifiability of linear SEMs when the error covariances are specified up to a constant multiple. On the statistical side, we establish rigorous conditions for high-dimensional consistency of our two-part algorithm, defined in terms of a "gap" between the true DAG and the next best candidate. Finally, we demonstrate that dynamic programming may be used to select the optimal DAG in linear time when the treewidth of the moralized graph is bounded.},
archivePrefix = {arXiv},
arxivId = {1311.3492},
author = {Loh, Po-Ling and B{\"{u}}hlmann, Peter},
eprint = {1311.3492},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Loh, B{\"{u}}hlmann, Zou - 2014 - High-Dimensional Learning of Linear Causal Networks via Inverse Covariance Estimation.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {J. Mach. Learn. Res.},
keywords = {causal inference,dynamic programming,identifiability,inverse covariance matrix estimation,linear structural equation models},
pages = {3065--3105},
title = {{High-dimensional learning of linear causal networks via inverse covariance estimation}},
url = {http://www.jmlr.org/papers/volume15/loh14a/loh14a.pdf http://arxiv.org/abs/1311.3492},
volume = {15},
year = {2013}
}
@inproceedings{Anderson,
abstract = {The scale of functional magnetic resonance image data is rapidly increasing as large multi-subject datasets are becoming widely available and high-resolution scanners are adopted. The inherent low-dimensionality of the information in this data has led neuroscientists to consider factor analysis methods to extract and analyze the underlying brain activity. In this work, we consider two recent multi-subject factor analysis methods: the Shared Response Model and Hierarchical Topographic Factor Analysis. We perform analytical, algorithmic, and code optimization to enable multi-node parallel implementations to scale. Single-node improvements result in 99x and 1812x speedups on these two methods, and enables the processing of larger datasets. Our distributed implementations show strong scaling of 3.3x and 5.5x respectively with 20 nodes on real datasets. We also demonstrate weak scaling on a synthetic dataset with 1024 subjects, on up to 1024 nodes and 32,768 cores.},
archivePrefix = {arXiv},
arxivId = {1608.04647},
author = {Anderson, Michael J and Capota, Mihai and Turek, Javier S and Zhu, Xia and Willke, Theodore L and Wang, Yida and Chen, Po Hsuan and Manning, Jeremy R and Ramadge, Peter J and Norman, Kenneth A},
booktitle = {Proc. - 2016 IEEE Int. Conf. Big Data, Big Data 2016},
doi = {10.1109/BigData.2016.7840719},
eprint = {1608.04647},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Anderson et al. - Unknown - Enabling Factor Analysis on Thousand-Subject Neuroimaging Datasets.pdf:pdf},
isbn = {9781467390040},
keywords = {Factor Analysis,Multi-subject Analysis,Scaling,functional Magnetic Resonance Imaging},
pages = {1151--1160},
title = {{Enabling factor analysis on thousand-subject neuroimaging datasets}},
url = {https://arxiv.org/pdf/1608.04647.pdf},
year = {2016}
}
@article{Gutmann2010,
abstract = {We present a new principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated , using the model log-density function in the regression nonlinearity. We},
author = {Gutmann, Michael and Hyv{\"{a}}rinen, Aapo},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gutmann, Hyv{\"{a}}rinen - Unknown - Noise-contrastive estimation A new estimation principle for unnormalized statistical models.pdf:pdf},
issn = {15324435},
journal = {Int. Conf. Artif. Intell. Stat.},
pages = {1--8},
pmid = {2685552},
title = {{Noise-contrastive estimation: A new estimation principle for unnormalized statistical models}},
url = {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf},
year = {2010}
}
@article{Price,
abstract = {Characterization of disease using stationary resting-state functional connectivity (FC) has provided important hallmarks of abnormal brain activation in many domains. Recent studies of resting-state functional magnetic resonance imaging (fMRI), however, suggest there is a considerable amount of additional knowledge to be gained by investigating the variability in FC over the course of a scan. While a few studies have begun to explore the properties of dynamic FC for characterizing disease, the analysis of dynamic FC over multiple networks at multiple time scales has yet to be fully examined. In this study, we combine dynamic connectivity features in a multi-network, multi-scale approach to eval-uate the method's potential in better classifying childhood autism. Specifically, from a set of group-level intrinsic connectivity networks (ICNs), we use slid-ing window correlations to compute intra-network connectivity on the subject level. We derive dynamic FC features for all ICNs over a large range of window sizes and then use a multiple kernel support vector machine (MK-SVM) model to combine a subset of these features for classification. We compare the perfor-mance our multi-network, dynamic approach to the best results obtained from single-network dynamic FC features and those obtained from both single-and multi-network static FC features. Our experiments show that integrating multiple networks on different dynamic scales has a clear superiority over these existing methods.},
author = {Price, True and Wee, Chong-Yaw and Gao, Wei and Shen, Dinggang},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Price et al. - Unknown - Multiple-Network Classification of Childhood Autism using Functional Connectivity Dynamics.pdf:pdf},
journal = {MICCAI},
title = {{Multiple-Network Classification of Childhood Autism using Functional Connectivity Dynamics}},
url = {https://pdfs.semanticscholar.org/718f/31251beb87ae16cccf11326d72beafd36439.pdf http://cs.unc.edu/{~}jtprice/papers/miccai{\_}2014{\_}price.pdf},
year = {2014}
}
@article{Brakel2017,
abstract = {Reliable measures of statistical dependence could be useful tools for learning independent features and performing tasks like source separation using Independent Component Analysis (ICA). Unfortunately, many of such measures, like the mutual information, are hard to estimate and optimize directly. We propose to learn independent features with adversarial objectives which optimize such measures implicitly. These objectives compare samples from the joint distribution and the product of the marginals without the need to compute any probability densities. We also propose two methods for obtaining samples from the product of the marginals using either a simple resampling trick or a separate parametric distribution. Our experiments show that this strategy can easily be applied to different types of model architectures and solve both linear and non-linear ICA problems.},
archivePrefix = {arXiv},
arxivId = {1710.05050},
author = {Brakel, Philemon and Bengio, Yoshua},
eprint = {1710.05050},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Brakel, Bengio - Unknown - Learning independent features with adversarial nets for non-linear ICA.pdf:pdf},
title = {{Learning Independent Features with Adversarial Nets for Non-linear ICA}},
url = {https://arxiv.org/pdf/1710.05050.pdf http://arxiv.org/abs/1710.05050},
year = {2017}
}
@article{Park2014,
abstract = {We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.},
archivePrefix = {arXiv},
arxivId = {1410.6791},
author = {Park, Mijung and Jitkrittum, Wittawat and Qamar, Ahmad and Szabo, Zoltan and Buesing, Lars and Sahani, Maneesh},
eprint = {1410.6791},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Park et al. - Unknown - Bayesian Manifold Learning The Locally Linear Latent Variable Model.pdf:pdf},
issn = {10495258},
journal = {NIPS},
title = {{Bayesian Manifold Learning: The Locally Linear Latent Variable Model (LL-LVM)}},
url = {http://papers.nips.cc/paper/5973-bayesian-manifold-learning-the-locally-linear-latent-variable-model-ll-lvm.pdf http://arxiv.org/abs/1410.6791},
year = {2014}
}
@techreport{Neath1997,
author = {Neath, Andrew A and Samaniego, Francisco J},
booktitle = {Am. Stat.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Neath, Samaniego - 1997 - On the Efficacy of Bayesian Inference for Nonidentifiable Models(2).pdf:pdf},
number = {3},
pages = {225--232},
title = {{On the Efficacy of Bayesian Inference for Nonidentifiable Models}},
url = {https://www.jstor.org/stable/pdf/2684892.pdf?refreqid=excelsior{\%}3A5c4fce81c18bc4821068d213a816a9d9},
volume = {51},
year = {1997}
}
@article{Gretton2005,
author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alexander and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Gretton et al. - 2005 - Measuring Statistical Dependence with Hilbert-Schmidt Norms(2).pdf:pdf},
title = {{Measuring Statistical Dependence with Hilbert-Schmidt Norms}},
url = {http://www.kyb.tuebingen.mpg.de/techreports.html},
year = {2005}
}
@book{Spirtes2000,
author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard and Heckerman, David and Meek, Christopher and Richardson, Thomas},
publisher = {MIT Press},
title = {{Causation, Prediction and Search}},
year = {2000}
}
@article{Louizos2017a,
abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
archivePrefix = {arXiv},
arxivId = {1705.08821},
author = {Louizos, Christos and Shalit, Uri and Mooij, Joris and Sontag, David and Zemel, Richard and Welling, Max},
eprint = {1705.08821},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Louizos et al. - Unknown - Causal Effect Inference with Deep Latent-Variable Models.pdf:pdf},
journal = {NIPS},
title = {{Causal Effect Inference with Deep Latent-Variable Models}},
url = {http://www.cs.toronto.edu/{~}zemel/documents/1705.08821.pdf http://arxiv.org/abs/1705.08821},
year = {2017}
}
@article{Ioffe,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parame-ters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phe-nomenon as internal covariate shift, and ad-dress the problem by normalizing layer inputs. Our method draws its strength from making nor-malization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less care-ful about initialization, and in some cases elim-inates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Nor-malization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensem-ble of batch-normalized networks, we improve upon the best published result on ImageNet clas-sification: reaching 4.82{\%} top-5 test error, ex-ceeding the accuracy of human raters.},
author = {Ioffe, Sergey and Szegedy, Christian},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ioffe, Szegedy - Unknown - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://proceedings.mlr.press/v37/ioffe15.pdf}
}
@article{Stachenfeld2014,
abstract = {Hippocampal place fields have been shown to reflect behaviorally relevant aspects of space. For instance, place fields tend to be skewed along commonly traveled directions, they cluster around rewarded locations, and they are constrained by the geometric structure of the environment. We hypothesize a set of design principles for the hippocampal cognitive map that explain how place fields represent space in a way that facilitates navigation and reinforcement learning. In particular, we suggest that place fields encode not just information about the current location, but also predictions about future locations under the current transition distribu- tion. Under this model, a variety of place field phenomena arise naturally from the structure of rewards, barriers, and directional biases as reflected in the tran- sition policy. Furthermore, we demonstrate that this representation of space can support efficient reinforcement learning. We also propose that grid cells compute the eigendecomposition of place fields in part because is useful for segmenting an enclosure along natural boundaries. When applied recursively, this segmentation can be used to discover a hierarchical decomposition of space. Thus, grid cells might be involved in computing subgoals for hierarchical reinforcement learning.},
author = {Stachenfeld, Kl L and Botvinick, M M and Gershman, S J},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Stachenfeld, Botvinick, Gershman - Unknown - Design Principles of the Hippocampal Cognitive Map.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst. 27},
pages = {1--9},
pmid = {1000105561},
title = {{Design Principles of the Hippocampal Cognitive Map}},
url = {http://web.mit.edu/sjgershm/www/Stachenfeld14.pdf{\%}5Cnhttp://papers.nips.cc/paper/5340-design-principles-of-the-hippocampal-cognitive-map{\%}5Cnhttp://web.mit.edu/sjgershm/www/Stachenfeld14.pdf{\%}5Cnhttp://papers.nips.cc/paper/5340-design-principles-of-the-hipp},
year = {2014}
}
@article{Kummerfeld2016,
abstract = {Many scientific research programs aim to learn the causal structure of real world phenomena. This learning problem is made more difficult when the target of study cannot be directly observed. One strategy commonly used by social scientists is to create measurable "indicator" variables that covary with the latent variables of interest. Before leveraging the indicator variables to learn about the latent variables, however, one needs a measurement model of the causal relations between the indicators and their corresponding latents. These measurement models are a special class of Bayesian networks. This paper addresses the problem of reliably inferring measurement models from measured indicators, without prior knowledge of the causal relations or the number of latent variables. We present a provably correct novel algorithm, FindOneFactorClusters (FOFC), for solving this inference problem. Compared to other state of the art algorithms, FOFC is faster, scales to larger sets of indicators, and is more reliable at small sample sizes. We also present the first correctness proofs for this problem that do not assume linearity or acyclicity among the latent variables. {\textcopyright} 2016 ACM.},
author = {Kummerfeld, Erich and Ramsey, Joseph},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kummerfeld, Ramsey - Unknown - Causal Clustering for 1-Factor Measurement Models.pdf:pdf},
journal = {KDD},
pages = {1655--1664},
title = {{Causal Clustering for 1-Factor Measurement Models}},
year = {2016}
}
@article{Barber2017,
abstract = {In many practical applications of multiple hypothesis testing using the False Discovery Rate (FDR), the given hypotheses can be naturally partitioned into groups, and one may not only want to control the number of false discoveries (wrongly rejected null hypotheses), but also the number of falsely discovered groups of hypotheses (we say a group is falsely discovered if at least one hypothesis within that group is rejected, when in reality the group contains only nulls). In this paper, we introduce the p-filter, a procedure which unifies and generalizes the standard FDR procedure by Benjamini and Hochberg and global null testing procedure by Simes. We first prove that our proposed method can simultaneously control the overall FDR at the finest level (individual hypotheses treated separately) and the group FDR at coarser levels (when such groups are user-specified). We then generalize the p-filter procedure even further to handle multiple partitions of hypotheses, since that might be natural in many applications. For example, in neuroscience experiments, we may have a hypothesis for every (discretized) location in the brain, and at every (discretized) timepoint: does the stimulus correlate with activity in location x at time t after the stimulus was presented? In this setting, one might want to group hypotheses by location and by time. Importantly, our procedure can handle multiple partitions which are nonhierarchical (i.e. one partition may arrange p-values by voxel, and another partition arranges them by time point; neither one is nested inside the other). We prove that our procedure controls FDR simultaneously across these multiple lay- ers, under assumptions that are standard in the literature: we do not need the hypotheses to be independent, but require a nonnegative dependence condition known as PRDS.},
archivePrefix = {arXiv},
arxivId = {1512.03397},
author = {Barber, Rina Foygel and Ramdas, Aaditya},
doi = {10.1111/rssb.12218},
eprint = {1512.03397},
file = {:Users/ricardo/Downloads/Barber{\_}et{\_}al-2016-Journal{\_}of{\_}the{\_}Royal{\_}Statistical{\_}Society{\_}{\_}Series{\_}B{\_}(Statistical{\_}Methodology).pdf:pdf},
issn = {14679868},
journal = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
keywords = {False discovery rate,Grouped hypotheses,Multilayer,Multilevel,Multiple testing,Multiresolution,p-filter},
number = {4},
pages = {1247--1268},
title = {{The p-filter: multilayer false discovery rate control for grouped hypotheses}},
volume = {79},
year = {2017}
}
@article{Cook2013,
abstract = {Efficient estimation of the regression coefficients is a fundamental problem in multivariate linear regression. The envelope model proposed by Cook et al. (2010) was shown to have the potential to achieve substantial efficiency gains by accounting for linear combinations of the response vector that are essentially immaterial to coefficient estimation. This requires in part that the distribution of those linear combinations be invariant to changes in the nonstochastic predictor vector. However, inference based on an envelope is not invariant or equivariant under rescaling of the responses, tending to limit application to responses that are measured in the same or similar units. The efficiency gains promised by envelopes often cannot be realized when the responses are measured in different scales. To overcome this limitation and broaden the scope of envelope methods, we propose a scaled version of the envelope model, which preserves the potential of the original envelope methods to increase efficiency and is invariant to scale changes. Likelihood-based estimators are derived and theoretical properties of the estimators are studied in various circumstances. It is shown that estimating appropriate scales for the responses can produce substantial efficiency gains when the original envelope model offers none. Simulations and an example are given to support the theoretical claims.},
author = {Cook, R Dennis and Su, Zhihua},
doi = {10.1093/biomet/ast026},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Cook, Su - 2013 - Scaled envelopes scale-invariant and efficient estimation in multivariate linear regression.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Dimension reduction,Envelope model,Reducing subspace,Similarity transformation},
number = {4},
pages = {939--954},
title = {{Scaled envelopes: Scale-invariant and efficient estimation in multivariate linear regression}},
url = {http://www.stat.ufl.edu/{~}zhihuasu/resources/biomet.ast026.full.pdf},
volume = {100},
year = {2013}
}
@article{Greenewald2017,
abstract = {In this work, we present an additive model for space-time data that splits the data into a temporally correlated component and a spatially correlated component. We model the spatially correlated portion using a time-varying Gaussian graphical model. Under assumptions on the smoothness of changes in covariance matrices, we derive strong single sample convergence results, confirming our ability to estimate meaningful graphical structures as they evolve over time. We apply our methodology to the discovery of time-varying spatial structures in human brain fMRI signals.},
archivePrefix = {arXiv},
arxivId = {1711.03701},
author = {Greenewald, Kristjan and Park, Seyoung and Zhou, Shuheng and Giessing, Alexander},
eprint = {1711.03701},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Greenewald et al. - Unknown - Time-dependent spatially varying graphical models, with application to brain fMRI data analysis.pdf:pdf},
title = {{Time-dependent spatially varying graphical models, with application to brain fMRI data analysis}},
url = {https://arxiv.org/pdf/1711.03701.pdf http://arxiv.org/abs/1711.03701},
year = {2017}
}
@article{Anandkumar,
abstract = {Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by multiple latent factors (topics), as op-posed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden. This work provides a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (i.e., third order moments, which may be estimated with documents containing just three words). The method, called Excess Corre-lation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on k × k matrices, where k is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space.},
author = {Anandkumar, Animashree and Foster, Dean P and Hsu, Daniel and Kakade, Sham M and Liu, Yi-Kai},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Anandkumar et al. - Unknown - A Spectral Algorithm for Latent Dirichlet Allocation.pdf:pdf},
journal = {NIPS},
title = {{A Spectral Algorithm for Latent Dirichlet Allocation}},
url = {https://papers.nips.cc/paper/4637-a-spectral-algorithm-for-latent-dirichlet-allocation.pdf},
year = {2012}
}
@article{Baluja,
abstract = {Steganography is the practice of concealing a secret message within another, ordinary, message. Commonly, steganography is used to unobtrusively hide a small message within the noisy regions of a larger image. In this study, we attempt to place a full size color image within another image of the same size. Deep neural networks are simultaneously trained to create the hiding and revealing processes and are designed to specifically work as a pair. The system is trained on images drawn randomly from the ImageNet database, and works well on natural images from a wide variety of sources. Beyond demonstrating the successful application of deep learning to hiding images, we carefully examine how the result is achieved and explore extensions. Unlike many popular steganographic methods that encode the secret message within the least significant bits of the carrier image, our approach compresses and distributes the secret image's representation across all of the available bits.},
author = {Baluja, Shumeet},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Baluja - Unknown - Hiding Images in Plain Sight Deep Steganography.pdf:pdf},
number = {Nips},
title = {{Hiding Images in Plain Sight: Deep Steganography}},
url = {http://www.esprockets.com/papers/nips2017.pdf},
year = {2017}
}
@article{Russo2017,
abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
archivePrefix = {arXiv},
arxivId = {1707.02038},
author = {Russo, Daniel and {Van Roy}, Benjamin and Kazerouni, Abbas and Osband, Ian},
eprint = {1707.02038},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Russo et al. - 2017 - A Tutorial on Thompson Sampling.pdf:pdf},
title = {{A Tutorial on Thompson Sampling}},
url = {https://arxiv.org/pdf/1707.02038.pdf http://arxiv.org/abs/1707.02038},
year = {2017}
}
@article{Brock2016,
abstract = {The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network, a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.},
archivePrefix = {arXiv},
arxivId = {1609.07093},
author = {Brock, Andrew and Lim, Theodore and Ritchie, J M and Weston, Nick},
doi = {10.1177/1470320311410924},
eprint = {1609.07093},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Brock et al. - Unknown - NEURAL PHOTO EDITING WITH INTROSPECTIVE AD- VERSARIAL NETWORKS.pdf:pdf},
isbn = {3200705426},
issn = {03008495},
pmid = {19142273},
title = {{Neural Photo Editing with Introspective Adversarial Networks}},
url = {https://arxiv.org/pdf/1609.07093.pdf http://arxiv.org/abs/1609.07093},
year = {2016}
}
@article{Silva2006,
abstract = {We describe anytime search procedures that (1) find disjoint subsets of recorded variables for which the members of each subset are d-separated by a single common unrecorded cause, if such exists; (2) return information about the causal relations among the latent factors so identified. We prove the procedure is point-wise consistent assuming (a) the causal relations can be represented by a directed acyclic graph (DAG) satisfying the Markov Assumption and the Faithfulness Assumption; (b) unrecorded variables are not caused by recorded variables; and (c) dependencies are linear. We compare the procedure with standard approaches over a variety of simulated structures and sample sizes, and illustrate its practical value with brief studies of social science data sets. Finally, we consider generalizations for non-linear systems.},
author = {Silva, Ricardo and Scheines, Richard and Glymour, Clark and Spirtes, Peter},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Silva et al. - 2006 - Learning the Structure of Linear Latent Variable Models.pdf:pdf},
journal = {J. Mach. Learn. Res.},
keywords = {causality,graphical models,latent variable models},
pages = {191--246},
title = {{Learning the structure of linear latent variable models}},
volume = {7},
year = {2006}
}
@inproceedings{Meek1995,
abstract = {This paper presents correct algorithms for answering the following two questions; (i) Does there exist a causal explanation con­ sistent with a set of background knowledge which explains all of the observed indepen­ dence facts in a sample? (ii) Given that there is such a causal explanation what are the causal relationships common to every such causal explanation?},
author = {Meek, Christopher},
booktitle = {Proc. 11th Conf. Uncertain. Artif. Intell.},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Meek - Unknown - Causal inference and causal explanation with background knowledge.pdf:pdf},
pages = {403--418},
title = {{Causal inference and causal explanation with background knowledge}},
year = {1995}
}
@article{Goodfellow2015,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow, Shlens, Szegedy - Unknown - EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES(2).pdf:pdf},
isbn = {1412.6572},
issn = {0012-7183},
journal = {ICLR},
pmid = {729514},
title = {{Explaining and Harnessing Adversarial Examples}},
url = {https://arxiv.org/pdf/1412.6572.pdf http://arxiv.org/abs/1412.6572},
year = {2015}
}
@article{Berk2011,
abstract = {MOTIVATION: Metabolomics is the study of the complement of small molecule metabolites in cells, biofluids and tissues. Many metabolomic experiments are designed to compare changes observed over time under two experimental conditions or groups (e.g. a control and drug-treated group) with the goal of identifying discriminatory metabolites or biomarkers that characterize each condition. A common study design consists of repeated measurements taken on each experimental unit thus producing time courses of all metabolites. We describe a statistical framework for estimating time-varying metabolic profiles and their within-group variability and for detecting between-group differences. Specifically, we propose (i) a smoothing splines mixed effects (SME) model that treats each longitudinal measurement as a smooth function of time and (ii) an associated functional test statistic. Statistical significance is assessed by a non-parametric bootstrap procedure. RESULTS: The methodology has been extensively evaluated using simulated data and has been applied to real nuclear magnetic resonance spectroscopy data collected in a preclinical toxicology study as part of a larger project lead by the COMET (Consortium for Metabonomic Toxicology). Our findings are compatible with the previously published studies. AVAILABILITY: An R script is freely available for download at http://www2.imperial.ac.uk/{\~{}}gmontana/sme.htm.},
author = {Berk, Maurice and Ebbels, Timothy and Montana, Giovanni},
doi = {10.1093/bioinformatics/btr289},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Berk, Ebbels, Montana - 2011 - A statistical framework for biomarker discovery in metabolomic time course data.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
month = {jul},
number = {14},
pages = {1979--1985},
pmid = {21729866},
publisher = {Oxford University Press},
title = {{A statistical framework for biomarker discovery in metabolomic time course data}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btr289},
volume = {27},
year = {2011}
}
@article{Yu2017,
author = {Yu, Linin and H{\"{a}}rdle, Wolfgang and Borke, Lukas and Benschop, Thijs},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2017 - FRM A financial risk meter based on penalizing tail events occurrence.pdf:pdf},
journal = {Collab. Res. Cent.},
pages = {2017--3},
title = {{FRM : a Financial Risk Meter based on penalizing tail events occurrence}},
volume = {649},
year = {2017}
}
@article{Varoquaux2010,
abstract = {Spontaneous brain activity, as observed in functional neuroimaging, has been shown to display reproducible structure that expresses brain architecture and carries markers of brain pathologies. An important view of modern neuroscience is that such large-scale structure of coherent activity reflects modularity properties of brain connectivity graphs. However, to date, there has been no demonstration that the limited and noisy data available in spontaneous activity observations could be used to learn full-brain probabilistic models that generalize to new data. Learning such models entails two main challenges: i) modeling full brain connectivity is a difficult estimation problem that faces the curse of dimensionality and ii) variability between subjects, coupled with the variability of functional signals between experimental runs, makes the use of multiple datasets challenging. We describe subject-level brain functional connectivity structure as a multivariate Gaussian process and introduce a new strategy to estimate it from group data, by imposing a common structure on the graphical model in the population. We show that individual models learned from functional Magnetic Resonance Imaging (fMRI) data using this population prior generalize better to unseen data than models based on alternative regularization schemes. To our knowledge, this is the first report of a cross-validated model of spontaneous brain activity. Finally, we use the estimated graphical model to explore the large-scale characteristics of functional architecture and show for the first time that known cognitive networks appear as the integrated communities of functional connectivity graph.},
author = {Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Poline, Jean Baptiste and Thirion, Bertrand},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Varoquaux, Poline, Thirion - Unknown - Brain covariance selection better individual functional connectivity models using population prio.pdf:pdf},
journal = {NIPS},
title = {{Brain covariance selection: better individual functional connectivity models using population prior}},
year = {2010}
}
@article{Ding2006,
abstract = {Currently, most research on nonnegative matrix factorization (NMF)focus on 2-factor {\$}X=FG{\^{}}T{\$} factorization. We provide a systematicanalysis of 3-factor {\$}X=FSG{\^{}}T{\$} NMF. While it unconstrained 3-factor NMF is equivalent to it unconstrained 2-factor NMF, itconstrained 3-factor NMF brings new features to it constrained 2-factor NMF. We study the orthogonality constraint because it leadsto rigorous clustering interpretation. We provide new rules for updating {\$}F,S, G{\$} and prove the convergenceof these algorithms. Experiments on 5 datasets and a real world casestudy are performed to show the capability of bi-orthogonal 3-factorNMF on simultaneously clustering rows and columns of the input datamatrix. We provide a new approach of evaluating the quality ofclustering on words using class aggregate distribution andmulti-peak distribution. We also provide an overview of various NMF extensions andexamine their relationships.},
author = {Ding, Chris and Li, Tao and Peng, Wei and Park, Haesun},
doi = {10.1145/1150402.1150420},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Ding et al. - Unknown - Orthogonal Nonnegative Matrix Tri-factorizations for Clustering(2).pdf:pdf},
isbn = {1595933395},
issn = {07352689},
journal = {Proc. 12th ACM SIGKDD},
keywords = {clustering,multi-peak distribution,nmf,nonnegative matrix factorization,orthogonal factorization,tri-factorization},
pages = {126--135},
title = {{Orthogonal nonnegative matrix tri-factorizations for clustering}},
url = {https://users.cs.fiu.edu/{~}taoli/pub/p126-DLPH-KDD05.pdf},
year = {2006}
}
@article{Pham2001,
abstract = {Most source separation algorithms are based on a model of$\backslash$nstationary sources. However, it is a simple matter to take advantage of$\backslash$npossible nonstationarities of the sources to achieve separation. This$\backslash$npaper develops novel approaches in this direction based on the$\backslash$nprinciples of maximum likelihood and minimum mutual information. These$\backslash$nprinciples are exploited by efficient algorithms in both the off-line$\backslash$ncase (via a new joint diagonalization procedure) and in the on-line case$\backslash$n(via a Newton-like procedure). Some experiments showing the good$\backslash$nperformance of our algorithms and evidencing an interesting feature of$\backslash$nour methods are presented: their ability to achieve a kind of$\backslash$nsuper-efficiency. The paper concludes with a discussion contrasting$\backslash$nseparating methods for non-Gaussian and nonstationary models and$\backslash$nemphasizing that, as a matter of fact, {\&}ldquo;what makes the algorithms$\backslash$nwork{\&}rdquo; is-strictly speaking-not the nonstationarity itself but$\backslash$nrather the property that each realization of the source signals has a$\backslash$ntime-varying envelope},
author = {Pham, Dinh Tuan and Cardoso, Jean Fran{\c{c}}ois},
doi = {10.1109/78.942614},
isbn = {3376631263},
issn = {1053587X},
journal = {IEEE Trans. Signal Process.},
keywords = {Blind source separation,Independent component analysis,Joint diagonalization,Maximum likelihood,Mutual information,Nonstationarity},
number = {9},
pages = {1837--1848},
title = {{Blind separation of instantaneous mixtures of nonstationary sources}},
url = {http://ieeexplore.ieee.org/document/942614/},
volume = {49},
year = {2001}
}
@article{Jenatton,
abstract = {We present an extension of sparse PCA, or sparse dictionary learning, where the sparsity patterns of all dictionary elements are structured and con-strained to belong to a prespecified set of shapes. This structured sparse PCA is based on a struc-tured regularization recently introduced by Jenat-ton et al. (2009). While classical sparse priors only deal with cardinality, the regularization we use encodes higher-order information about the data. We propose an efficient and simple opti-mization procedure to solve this problem. Ex-periments with two practical tasks, the denoising of sparse structured signals and face recognition, demonstrate the benefits of the proposed struc-tured approach over unstructured approaches.},
author = {Jenatton, Rodolphe and Obozinski, Guillaume and Bach, Francis},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Jenatton, Obozinski, Bach - Unknown - Structured Sparse Principal Component Analysis.pdf:pdf},
title = {{Structured Sparse Principal Component Analysis}},
url = {http://proceedings.mlr.press/v9/jenatton10a/jenatton10a.pdf}
}
@book{Kreyszig1990,
author = {Kreyszig, Erwin},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Kreyszig - Unknown - Introductory Functional Analysis with Applications.pdf:pdf},
isbn = {9781461268246},
title = {{Introductory Functional Analysis with Applications}},
url = {http://www-personal.acfr.usyd.edu.au/spns/cdm/resources/Kreyszig - Introductory Functional Analysis with Applications.pdf},
year = {1990}
}
@inproceedings{Henderson2017,
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to maintaining this rapid progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results difficult to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines, and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field, by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
archivePrefix = {arXiv},
arxivId = {1709.06560},
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
booktitle = {AAAI},
eprint = {1709.06560},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Henderson et al. - 2017 - Deep Reinforcement Learning that Matters(2).pdf:pdf},
title = {{Deep Reinforcement Learning that Matters}},
url = {https://arxiv.org/pdf/1709.06560.pdf http://arxiv.org/abs/1709.06560 https://arxiv.org/pdf/1709.06560.pdf{\%}0Ahttps://arxiv.org/pdf/1709.06560.pdf{\%}0Ahttp://arxiv.org/abs/1709.06560},
year = {2017}
}
@article{Silva2010,
abstract = {In a variety of disciplines such as social sciences, psychology, medicine and economics, the recorded data are considered to be noisy measurements of latent variables connected by some causal structure. This corresponds to a family of graphical models known as the structural equation model with latent variables. While linear non-Gaussian variants have been well-studied, inference in nonparametric structural equation models is still underdeveloped. We introduce a sparse Gaussian process parameterization that defines a non-linear structure connecting latent variables, unlike common formulations of Gaussian process latent variable models. The sparse parameterization is given a full Bayesian treatment without compromising Markov chain Monte Carlo efficiency. We compare the stability of the sampling procedure and the predictive ability of the model against the current practice.},
archivePrefix = {arXiv},
arxivId = {arXiv:1002.4802v2},
author = {Silva, Ricardo and Gramacy, Robert B},
eprint = {arXiv:1002.4802v2},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Silva, Gramacy - 2010 - Gaussian Process Structural Equation Models with Latent Variables.pdf:pdf},
keywords = {()},
pages = {12},
title = {{Gaussian process structural equation models with latent variables}},
url = {https://arxiv.org/pdf/1002.4802.pdf http://eprints.pascal-network.org/archive/00007578/},
year = {2010}
}

@article{khemakhem2019variational,
	title={Variational autoencoders and nonlinear ica: A unifying framework},
	author={Khemakhem, Ilyes and Kingma, Diederik P and Hyv{\"a}rinen, Aapo},
	journal={arXiv preprint arXiv:1907.04809},
	year={2019}
}

@article{Louizos2017,
abstract = {We propose a practical method for {\$}L{\_}0{\$} norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of {\$}L{\_}0{\$} regularization. However, since the {\$}L{\_}0{\$} norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected {\$}L{\_}0{\$} norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the $\backslash$emph{\{}hard concrete{\}} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
archivePrefix = {arXiv},
arxivId = {1712.01312},
author = {Louizos, Christos and Welling, Max and Kingma, Diederik P},
eprint = {1712.01312},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Louizos, Welling, Kingma - Unknown - LEARNING SPARSE NEURAL NETWORKS THROUGH L 0 REGULARIZATION.pdf:pdf},
journal = {ICLR},
title = {{Learning Sparse Neural Networks through {\$}L{\_}0{\$} Regularization}},
url = {https://arxiv.org/pdf/1712.01312.pdf http://arxiv.org/abs/1712.01312},
year = {2017}
}
@article{Varoquaux2013,
abstract = {Functional connectomes capture brain interactions via synchronized fluctuations in the functional magnetic resonance imaging signal. If measured during rest, they map the intrinsic functional architecture of the brain. With task-driven experiments they represent integration mechanisms between specialized brain areas. Analyzing their variability across subjects and conditions can reveal markers of brain pathologies and mechanisms underlying cognition. Methods of estimating functional connectomes from the imaging signal have undergone rapid developments and the literature is full of diverse strategies for comparing them. This review aims to clarify links across functional-connectivity methods as well as to expose different steps to perform a group study of functional connectomes. {\textcopyright} 2013 Elsevier Inc.},
author = {Varoquaux, Ga{\"{e}}l and Craddock, R. Cameron},
file = {:Users/ricardo/Downloads/varoquauxCraddock.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$n1053-8119 (Linking)},
issn = {10538119},
journal = {Neuroimage},
keywords = {Connectome,Effective connectivity,FMRI,Functional connectivity,Group study,Resting-state},
pages = {405--415},
title = {{Learning and comparing functional connectomes across subjects}},
volume = {80},
year = {2013}
}
@article{Poldrack2012,
abstract = {Neuroimaging research has largely focused on the identification of associations between brain activation and specific mental functions. Here we show that data mining techniques applied to a large database of neuroimaging results can be used to identify the conceptual structure of mental functions and their mapping to brain systems. This analysis confirms many current ideas regarding the neural organization of cognition, but also provides some new insights into the roles of particular brain systems in mental function. We further show that the same methods can be used to identify the relations between mental disorders. Finally, we show that these two approaches can be combined to empirically identify novel relations between mental disorders and mental functions via their common involvement of particular brain networks. This approach has the potential to discover novel endophenotypes for neuropsychiatric disorders and to better characterize the structure of these disorders and the relations between them.},
author = {Poldrack, Russell A. and Mumford, Jeanette A. and Schonberg, Tom and Kalar, Donald and Barman, Bishal and Yarkoni, Tal},
editor = {Sporns, Olaf},
file = {:Users/ricardo/Library/Application Support/Mendeley Desktop/Downloaded/Poldrack et al. - 2012 - Discovering Relations Between Mind, Brain, and Mental Disorders Using Topic Mapping.pdf:pdf},
journal = {PLoS Comput. Biol.},
number = {10},
pages = {e1002707},
publisher = {Public Library of Science},
title = {{Discovering Relations Between Mind, Brain, and Mental Disorders Using Topic Mapping}},
volume = {8},
year = {2012}
}

@inproceedings{Kano2003,
	abstract = {Path analysis, often applied to observational data to study causal struc- tures, describes causal relationship between observed variables. The path analysis is of confirmatory nature and can make statistical tests for assumed causal relations based on comparison of the implied ...},
	author = {Kano, Yutaka and Shimizu, Shohei},
	booktitle = {Int. Symp. Sci. Model. 30th Anniv. Inf. Criterion},
	keywords = {Causal inference,higher-order moments and cumulants,independent component analysis,nonnormality,path analysis},
	title = {{Causal inference using nonnormality}},
	year = {2003}
}


@article{Mahowald,
abstract = {What makes a word memorable? Prior research has identified numerous factors: word frequency, concreteness, imageability, and valence have all been shown to affect recognition performance. One important dimension that has not received much attention is the nature of the relationship between words and meanings. Under the hypothesis that words are encoded primarily by their meanings, and not by their surface forms, this relationship should be central to determining word memorability. In particular, rational analysis suggests that people will more easily remember words that convey a large amount of information about their intended meaning and that have few alternatives – that is, memorable words will be those with few possible meanings and synonyms. To test this hypothesis, we ran two large-scale recognition memory experiments (each with 2,222 words, 600+ participants). Memory performance was overall high, on par with memory for pictures in a similar paradigm. Critically, however, not all words were remembered equally well. Consistent with our proposal, the best recognized words had few meanings and few synonyms. Indeed, the most memorable words had a one-to-one relationship with their meanings. Estimates of memorability derived from this rational account explain a large amount of the variance in word memorability.},
author = {Mahowald, Kyle and Isola, Phillip and Fedorenko, Evelina and Gibson, Edward and Oliva, Aude},
file = {:Users/ricardo/Downloads/Mahowald{\_}etal.pdf:pdf},
number = {1997},
pages = {1--10},
title = {{Memorable words are monogamous: The role of synonymy and homonymy in word recognition memory}}
}

@article{huang2018neural,
	title={Neural autoregressive flows},
	author={Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
	journal={arXiv preprint arXiv:1804.00779},
	year={2018}
}

@article{papamakarios2019normalizing,
	title={Normalizing Flows for Probabilistic Modeling and Inference},
	author={Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	journal={arXiv preprint arXiv:1912.02762},
	year={2019}
}

@article{dinh2016density,
	title={Density estimation using real nvp},
	author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	journal={arXiv preprint arXiv:1605.08803},
	year={2016}
}

@article{mooij2016distinguishing,
	title={Distinguishing cause from effect using observational data: methods and benchmarks},
	author={Mooij, Joris M and Peters, Jonas and Janzing, Dominik and Zscheischler, Jakob and Sch{\"o}lkopf, Bernhard},
	journal={The Journal of Machine Learning Research},
	volume={17},
	number={1},
	pages={1103--1204},
	year={2016},
	publisher={JMLR. org}
}

@article{monti2019causal,
	title={Causal discovery with general non-linear relationships using non-linear {I}{C}{A}},
	author={Monti, Ricardo Pio and Zhang, Kun and Hyvarinen, Aapo},
	journal={arXiv preprint arXiv:1904.09096},
	year={2019}
}

@inproceedings{zheng2018dags,
	title={ {D}{A}{G}s with {N}{O} {T}{E}{A}{R}{S}: Continuous optimization for structure learning},
	author={Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep K and Xing, Eric P},
	booktitle={Advances in Neural Information Processing Systems},
	pages={9472--9483},
	year={2018}
}

@article{kobyzev2019normalizing,
	title={Normalizing flows: Introduction and ideas},
	author={Kobyzev, Ivan and Prince, Simon and Brubaker, Marcus A},
	journal={arXiv preprint arXiv:1908.09257},
	year={2019}
}

@article{neyman1933ix,
	title={IX. On the problem of the most efficient tests of statistical hypotheses},
	author={Neyman, Jerzy and Pearson, Egon Sharpe},
	journal={Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	volume={231},
	number={694-706},
	pages={289--337},
	year={1933},
	publisher={The Royal Society London}
}

@inproceedings{spirtes2016causal,
	title={Causal discovery and inference: concepts and recent methodological advances},
	author={Spirtes, Peter and Zhang, Kun},
	booktitle={Applied informatics},
	volume={3},
	number={1},
	pages={3},
	year={2016},
	organization={Springer}
}

@article{pearl2009causal,
	title={Causal inference in statistics: An overview},
	author={Pearl, Judea and others},
	journal={Statistics surveys},
	volume={3},
	pages={96--146},
	year={2009},
	publisher={The author, under a Creative Commons Attribution License}
}