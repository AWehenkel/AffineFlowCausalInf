%%%%%%%% ICML 2020 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}
\usepackage{amsfonts}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2020} with \usepackage[nohyperref]{icml2020} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2020}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2020}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Autoregressive flow-based causal inference}

\begin{document}

\twocolumn[
\icmltitle{Autoregressive flow-based causal inference}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2020
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
%\icmlauthor{A research memo}{gatsby}
\icmlauthor{Ricardo Pio Monti}{gatsby}
\icmlauthor{Ilyes Khemakhem}{gatsby}
\icmlauthor{Aapo Hyv\"{a}rinen}{helsinki}
\end{icmlauthorlist}

\icmlaffiliation{gatsby}{Department of Computation, University of Torontoland, Torontoland, Canada}
%\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
%\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We posit that autoregressive flow models are well-suited to 
performing a range of causal inference tasks --- ranging from causal discovery to making
interventional and 
counterfactual predictions. 
%We demonstrate how the key strengths of flow models, namely the fact
%the estimate normalized log-densities and their invertible nature
%In particular, 
%Our approach is based on the (somewhat obvious) fact that 
%We highlight the fact that 
In particular, we exploit the fact that 
autoregressive architectures %implicitly 
define an 
ordering over 
variables,
analogous to a causal ordering, in order to 
propose a single flow architecture to perform all three aforementioned tasks.  
%which can be seen as analogous to the causal ordering defined 
%in
%structural equation  models. % (SEM). 
%
We first leverage the fact that flow models estimate normalized 
log-densities of data %conditional on an ordering of variables 
%to infer 
to derive a bivariate measure of causal direction based on likelihood ratios. 
Whilst traditional measures of causal direction often require restrictive assumptions on the nature of 
causal relationships (e.g., linearity),
the flexibility of flow models allows for arbitrary causal dependencies.
%
%for such a measure to accurately identify
%causal variables in a variety  
%the associated causal ordering. 
%The proposed method is benchmarked against alternative methods on both
%synthetic and 
%
Our approach compares favorably against %standard 
alternative methods on synthetic data 
as well as on the Cause-Effect Pairs benchmark dataset.
%and 
%real data.
%
%
Subsequently, we demonstrate that the invertible nature of flows naturally allows for 
direct evaluation of both 
%everage the invertible nature of flow models in order to make accurate 
interverntional and counterfactual predictions, which require 
marginalization and conditioning over latent variables respectively. 
We present examples  over synthetic data 
where autoregressive flows, when trained 
under the correct causal ordering, are able to make accurate interventional and counterfactual predictions. 
\end{abstract}


\section{Introduction}
\label{sec:intro}

Causal models play a fundamental role in modern scientific endeavor \citep{Spirtes2000, Pearl2009} 
%The questions which motivate a significant amount of research in 
%health, social and beh
with
many of the questions which drive research in science being not associational but 
rather causal in nature. % \citep{}.
%Whilst randomized controlled trials may be 
%the gold standard, in many cases they are  \citep{spirtes2016causal}. 
%
To this end, the framework of structural equation models (SEMs) 
was developed 
to both encapsulate causal knowledge as well as 
answer 
interventional and counterfactual queries \citep{pearl2009causal}

At a fundamental level, SEMs define a generative
%(latent variable)
model for data based on 
causal relationships. % and assumptions regarding latent variables. 
As such, SEMs 
implicitly 
define probabilistic models
in a similar way to many methods in machine learning and statistics. 
In this work we leverage  advances in 
probabilistic modeling based on deep networks in order to 
develop novel causal inference methods
with a particular emphasis on 
affine autoregressive flow models \citep{papamakarios2019normalizing}.  %are a special %case of
%
%In this work, we  leverage the
%fact that both SEMs and many methods in machine learning share a common goal, that of 
%obtaining well specified probabilistic models, in order
%to develop novel causal inference methods.  
%In particular, 
%throughout this work we focus on 
%Within machine learning, 
%affine autoregressive flow models \citep{papamakarios2019normalizing}.  %are a special %case of normalizing flow models
%class of models which
%which
%have 
%proven to be  well suited to building 
%well-specified 
%probabilistic models  \citep{papamakarios2019normalizing}. 
%In particular,
More formally,  
we consider the ordering of variables in an affine
autoregressive flow model from a causal perspective, 
and 
show that such models are well suited to performing a variety of 
causal inference tasks. 
%
Throughout a series of experiments, we demonstrate that autoregressive 
flow models are able to uncover causal structure from purely observational data, termed 
\textit{causal discovery}. Furthermore, 
we show that 
when autoregressive flow models are conditioned upon the correct 
causal ordering, they may be directly employed to 
accurately answer both interventional and counterfactual queries. 



%\vskip 2.5cm 
%
%
%
%Autoregressive models such as MADE and  NADE have demonstrated significant success.
%
%
%The framework of structural 
%
%
%%Such questions 
%and
%are therefore best 
%approached 
%using the frameworks and algorithmic tools of causal inference. 
%
%
%
%\noindent Mention the three levels on the ladder of causation and that we present a single 
%autoregressive flow architecture which is capable of performing all three. \\
%
%\noindent Write some more generic things about causal inference and causal discovery.\\
%
%\noindent Also highlight the main strengths of flow models:
%\begin{itemize}
%	\item They estimate normalized log-densities of data via maximum likelihood. This property is what we will leverage for our non-linear measure of causal direction. 
%	\item They learn invertible transformations of observations, $\mathbf{x}$, to latent variables, $\mathbf{z}$. This property will be fundamental when obtaining interventional and counterfactual predictions. 
%	\item Furthermore, autoregressive flow models 
%\end{itemize}

%\newpage 

The remainder of this manuscript is structured as follows: in Section 
\ref{sec::background} we briefly cover SEMs and highlight the correspondence between such a model and affine autoregressive flow models. In Section \ref{sec::flowCD}
we present the an autoregressive  flow based measure of causal direction. The performance of the
proposed method is quantified over a series of synthetic and real datasets and is competitive
against established causal discovery methods. 
Finally, in Section \ref{sec::flowCI} we demonstrate that 
the same autoregressive flow architecture, when trained under the true causal ordering 
over variables,, may be used to obtain accurate interventional and counterfactual predictions. 



\section{Background}
\label{sec::background}

In this section we 
introduce the class of causal models to be studied and 
highlight their correspondence with autoregressive flow models. 


\subsection{Structural equation models}
\label{sec::SEM}

Suppose we observe $d$-dimensional random variables $\textbf{x}= (x_1, \ldots, x_d)$ 
with joint distribution $\mathbb{P}(\textbf{x})$.  %$\mathcal{L}(\textbf{X})$. 
%The objective of causal discovery is to use the observed data, 
%which give the empirical version of $\mathbb{P}(\textbf{X})$, to infer the associated 
%causal graph
%which describes the data generating procedure \citep{Spirtes2000, Pearl2009}.
%
A structural equation model (SEM) is here defined 
%(generalizing the traditional definition) 
as a 
% pair 
% $(\mathcal{S}, \mathbb{P}(\textbf{N}))$, 
% where $\mathcal{S} = (\mathcal{S}_1, \ldots, \mathcal{S}_d)$ is a 
collection of $d$ 
structural equations:
\begin{equation}
\label{SEM_eq}
% \mathcal{S}_j : ~~
x_j = f_j ( \textbf{pa}_j, n_j), ~~ ~ j=1, \ldots ,d
\end{equation}
together with a joint distribution, $\mathbb{P}(\textbf{n})$, %is the joint distribution 
over latent disturbance (noise)
variables, $n_j$,  which are assumed to be mutually  independent. 
We write $\textbf{pa}_j$ to denote the parents of the variable $x_j$. 
The causal graph, $\mathcal{G}$, associated with 
%  a set of
a SEM in equation (\ref{SEM_eq})
is a graph
consisting of one node corresponding to each variable $x_j$;
%with arrows drawn from each parent to its direct descendants. 
% is obtained by drawing arrows from each parent to its direct effect. 
throughout this work we assume $\mathcal{G}$ 
is a directed acyclic graph (DAG). The DAG implies a \textit{causal}
ordering,
$\pi$, over $\{1, \ldots, d\}$ such that 
if $\pi(i) < \pi(j)$ then variable $x_i$ is before $x_j$ in the DAG, and therefore a potential parent of 
$x_j$. 
%
%ordering over variables $x_1, \ldots, x_d$. 
%
Thus, given the causal ordering of the associated DAG
%we can define the 
%\textit{ancestors} of a 
%
we may re-write 
equation (\ref{SEM_eq}) as 
\begin{equation}
\label{SEM_eq_causalOrder}
%x_{\pi(j)} = f_{\pi(j)} \left (  \textbf{x}_{ < \pi(j)}  ,  n_{\pi(j)} \right ), % ~~ ~ j=1, \ldots ,d
x_j = f_j \left (  \textbf{x}_{ < \pi(j)}  ,  n_j \right ), ~~ ~ j=1, \ldots ,d
\end{equation}
where $\mathbf{x}_{< \pi(j)} = \{x_i: \pi(i) < \pi(j)\}$ denotes all variables before $x_j$ in the causal ordering.  
We allow 
%Moreover,
%in the above definition of SEMs we allow 
$f_j$ %in equation (\ref{SEM_eq}) are allowed 
to be any
(possibly non-linear) function.
%the causal discovery community has focused on 
%specific special cases in order to obtain identifiability results as 
%well as provide practical algorithms. 
%Pertinent examples include:
%\textit{a)} 
%the linear non-Gaussian acyclic model  (LiNGAM; \citeauthor{Shimizu2006}, 2006),
%%LiNGAM, 
%which assumes each $f_j$ is a linear function and the $N_j$ are non-Gaussian,
%\textit{b)} the additive noise model (ANM; \citeauthor{Hoyer2009}, 2009), which assumes the noise is additive,
%%\begin{equation*}
%%%\mathcal{S}_j : ~~
%% X_j = f_j ( \textbf{PA}_j) + N_j, ~~ ~ j=1, \ldots ,d,
%%\end{equation*}
%and \textit{c)} the post-nonlinear causal model, which also captures possible non-linear distortion in the observed variables \citep{Zhangb}. 
%%The objective of causal discovery methods is to employ data, $\textbf{X}$,
%%to infer the edge structure of $\mathcal{G}$.

 
\subsection{Affine autoregressive flow models}

We briefly introduce affine autoregressive flow models, for further details we refer readers to  
\cite{papamakarios2019normalizing} and \cite{kobyzev2019normalizing}. 


Normalizing flows seek to express the log-density of observations 
$\textbf{x}\in\mathbb{R}^d$ as an invertible and differentiable 
transformation $T$ of latent variables, $\mathbf{z}\in\mathbb{R}^d$,
which follow a base distribution,  $p_{{z}}(\mathbf{z})$. Typically the base distribution is 
assumed to be factorial. 
The generative model implied under such a framework is:
\begin{align}
\mathbf{z} &\sim p_z (\mathbf{z}), ~~~
\mathbf{x} = T( \mathbf{z})
\end{align}
This allows for the density of $\mathbf{x}$ to be obtained via a change of variables as follows:
\begin{equation}
p_x(\mathbf{x}) = p_{{z}}(\mathbf{z}) | \det J_T(\mathbf{z} )|^{-1}   .
\label{flow_inverse}
\end{equation} 
Throughout this work,  $T$ or $T^{-1}$ will be implemented with neural networks. As such,  an important consideration 
is ensuring the determinant of $T$ can be efficiently calculated. 
\textit{Autoregressive} flow models are designed precisely to achieve this goal by restricting the 
Jacobian of the transformation to be lower triangular \citep{huang2018neural}. 
%In the general case, autoregressive flows can be implemented as:
While autoregressive flows can be implemented in a variety of ways, we consider 
affine transformations of the form:
\begin{equation}
z_j = s_j( \mathbf{x}_{1:j-1} ) + e^{t_j( \mathbf{x}_{1:j-1}  )} \cdot x_j
\end{equation}
where both $s_j(\cdot)$ and $t_j(\cdot)$ are parameterized by neural networks \citep{dinh2016density}. 
We write $s_j$, $t_j$ to denote that such neural networks are distinct for each
$j = 1, \ldots, d$. 
Such a transformation can also be trivially inverted as:
\begin{align}
\label{flow_inverse_1}
x_j &= \left (z_j - s( \mathbf{x}_{1:j-1} ) \right ) \cdot  e^{-t( \mathbf{x}_{1:j-1}  )} \\
&= - s( \mathbf{x}_{1:j-1} ) \cdot  e^{-t( \mathbf{x}_{1:j-1}  )} +  z_j \cdot  e^{-t( \mathbf{x}_{1:j-1}  )} .
\label{flow_inverse_2}
\end{align}
%In particular, throughout this work we consider 
It is important to note that whilst the transformation detailed in 
equation (\ref{flow_inverse}) is an affine transformation, and therefore of limited expressivity, 
%it is possible to \textit{compose} multiple 
flows are \textit{composable}, implying that we may take the composition of multiple affine flows
$T_1, T_2, \ldots T_k$  to obtain $T = T_1 \circ T_2 \circ \cdots \circ T_k $ which will continue to be 
invertible and have a tractable Jacobian \citep{papamakarios2019normalizing}.



The ideas presented in this manuscript 
highlight the similarities between 
equations (\ref{SEM_eq_causalOrder}) and (\ref{flow_inverse_2}). 
In particular, both models explicitly define an ordering over variables
and 
both models assume 
%similar assumptions are often made over 
latent variables (denoted by $\textbf{n}$ or 
%and latent variables in flow models,
 $\textbf{z}$ respectively)
 follow simple, isotropic distributions. 
%
%we note the correspondence between 
%autoregressive flow models and SEMs where each latent variable, $z_j$, can be 
%seen as a latent disturbance, $n_j$.  
Throughout the remainder of this manuscript we will look to 
build upon these similarities in order to employ
autoregressive flow models for causal inference. 
%We will  also use $z_j$ and $n_j$ interchangeably. 

%In the past, similar relationships between SEMs and linear and nonlinear ICA 
%models have been exploited to develop causal inference 
%methods \citep{Shimizu2006, monti2019causal, khemakhem2019variational}. One important distinction between 
%the proposed method and previous ICA based methods, is that the latent representations of 
%autoregressive flow models are non-identifiable. 

\section{Flow-based measures of causal direction}
\label{sec::flowCD}

In this section we exploit the correspondence between nonlinear SEMs 
and 
autoregressive flow models highlighted in 
Section \ref{sec::background} in order to derive new measures of causal direction. 
For simplicity, we restrict ourselves to the case of $d=2$ dimensional data in this section; higher 
dimensional examples are considered in Section \ref{sec::flowCI}. 

The objective of the 
proposed method is  to uncover the causal direction between two univariate variables 
${x}_1$ and ${x}_2$. 
Suppose that ${x}_1 \rightarrow {x}_2$ implying that 
the associated SEM is of the form:
\begin{equation}
%\label{bivariate_eq1}
{x}_1 = f_1( n_1 ) ~~~ \mbox{ and } ~~~ {x}_2 = f_2({x}_1, n_2),
\label{bivariate_eq2}
\end{equation}
where $n_1, n_2$ are %non-Gaussian 
latent disturbances with factorial joint distributions. 

We follow \cite{Hyvarinen2013} 
and pose causal discovery as a model selection problem. To this end, we seek to compare
two candidate models: $x_1 \rightarrow x_2$ against $x_1 \leftarrow x_2$. 
Likelihood ratios are an attractive way to deciding between alternative models 
and have been proven to be uniformly most powerful when comparing simple hypothesis \citep{neyman1933ix}.
%An attractive way to decide between alternative models is to via the likelihood ratio \citep{}. 
%and derive likelihood ratio based measures of causal direction. 
To this end, \cite{Hyvarinen2013} focus on the case of linear SEMs with non-Gaussian latent disturbances and present a variety of methods for estimating the log-likelihood, $\log L_{\pi}$, under each candidate model, where $\pi = (1,2)$ or $\pi=(2,1)$. As such, they compute the log-likelihood ratio as
\begin{equation}
 R = \log L_{1\rightarrow 2 } - \log L_{2 \rightarrow 1 }.
\end{equation} 
The value of $R$ may thus be employed as a measure of causal direction. 
They conclude that $x_1 \rightarrow x_2$ if $R$ is positive and $x_1 \leftarrow x_2$ otherwise. 

In this work, we leverage the expressivity of autoregressive flow architectures in order to 
derive a analogous measures of causal direction for nonlinear SEMs. 
%
To this end, we note that the 
log-likelihood of the bivariate SEM, $x_1 \rightarrow x_2$, can be computed as:
\begin{align*}
\log L_{1\rightarrow 2 } ( \mathbf{x} ) =& \log p_{x_1}(x_1) + \log p_{x_2|x_1}( x_2 | x_1) \\
=& \log p_{z_1} ( f_1^{-1} (x_1)) + \log p_{z_2} (f_2^{-1}( x_1, x_2)) \\
&+ \log | \det \mathbf{J} \mathbf{f}^{-1}|, 
%\label{eq:flowApprox}
\end{align*}
where the latter equation 
% (\ref{eq:flowApprox}) 
details how such a
 log-likelihood is computed under an autoregressive flow model. 
 We note that in the context of linear SEMs, as studied by \cite{Hyvarinen2013}, 
 the log determinant term will be equal under both candidate models ($x_1 \rightarrow x_2$ and $x_1 \leftarrow x_2$) and therefore cancel. 

As such, we propose to fit two 
autoregressive flow models, each conditioned on a 
distinct causal order
over variables: $\pi = (1,2)$ or $\pi=(2,1)$. 
% distinct ordering over variables. 
For each candidate model we train parameters for each flow via maximum likelihood. 
However, in order to avoid overfitting we look to evaluate log-likelihood for each model over a 
held out testing dataset. As such, the proposed measure of causal direction is 
defined as:
\begin{align}
\label{eq:flowLR}
\begin{split}
 R = &\log {L_{1\rightarrow 2 }(\mathbf{x}_{test}; \mathbf{x}_{train}, \theta) } \\- &\log { L_{2\rightarrow 1 }(\mathbf{x}_{test}; \mathbf{x}_{train}, \theta') }
\end{split}
\end{align}
where $\log L_{1\rightarrow 2 }(\mathbf{x}_{test}; \mathbf{x}_{train}) $ is the estimated log-likelihood 
%under the constraint that $X_1$ is the 
%causal variable and  $X_2$ the effect, 
evaluated on an unseen test data $\mathbf{x}_{test}$. % \rightarrow X_2$.
We denote by $\theta$ and $\theta'$ the parameters 
for each flow model respectively. 
%We evaluate this likelihood using an autoregressive flow network trained via maximum likelihood on training 
%samples $ \mathbf{x}_{train}= \{ \mathbf{x}_1, \ldots, \mathbf{x}_n\}$. 


%Whilst \cite{Hyvarinen2013} focused exclusively on the 
%case of linear SEMs with non-Gaussian latent disturbances, our proposed
%measure of causal direction leverages the expressivity of 
%autoregressive flow architectures to accommodate 
%arbitrary nonlinear SEMs.  
%In order to eva



%pose the issue of inferring causal direction as a model selection problem.

%Our proposed measure of causal direction is based on the generalized likelihood ratio statistic:
%\begin{equation}
%R = \frac{L_{1\rightarrow 2 }(\mathbf{x}_{test}; \mathbf{x}_{train}, \theta) }{ L_{2\rightarrow 1 }(\mathbf{x}_{test}; \mathbf{x}_{train}, \theta') }
%\label{eq:flowLR}
%\end{equation}
%where $L_{1\rightarrow 2 }(\mathbf{x}_{test}; \mathbf{x}_{train}) $ is the estimated likelihood under the constraint that $X_1$ is the 
%causal variable and  $X_2$ the effect, 
%evaluated on an unseen test dataset $\mathbf{x}_{test}$. % \rightarrow X_2$.
%We evaluate this likelihood using an autoregressive flow network trained via maximum likelihood on training 
%samples $ \mathbf{x}_{train}= \{ \mathbf{x}_1, \ldots, \mathbf{x}_n\}$. 
%%Due to the flexibility of flow models, when evaluating 
%Equation (\ref{eq:flowLR}) can be seen as an extension of the linear measure of causal direction proposed in \cite{Hyvarinen2013} to accommodate nonlinear SEMs. 
%A further important difference is that 
%in order to account for the additional flexibility of flow models, we evaluate the likelihood over 
%unseen test data. 

\begin{figure*}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=2\columnwidth]{Figures/CausalDiscSims.pdf}}% CausalDiscFigure.png}}
		\caption{Performance of various algorithms on synthetic data generated under three
			distinct SEMs over varying complexity. We note that for all three SEMs the proposed likelihood ratio measure of causal discovery
			(Affline flow LR) performs competitively and is able to robustly identify the underlying causal direction. }
		\label{Fig:causalDiscSimulations}
	\end{center}
	\vskip -0.2in
\end{figure*}

\subsection{Experimental results}

In order to demonstrate the capabilities of the proposed method we consider its performance over a variety of synthetic 
datasets as well as on the Cause-Effect Pairs benchmark dataset \citep{mooij2016distinguishing}. 
We compare the performance of the proposed method against several state-of-the-art methods. 
As a comparison against a linear methods we include the 
linear likelihood ratio method of \cite{Hyvarinen2013} as well as the recently
proposed NO-TEARs method of \cite{zheng2018dags}.
We also compare against nonlinear causal discovery methods by considering 
%
the additive noise model (ANM) method 
proposed by \cite{Hoyer2009, Peters2013}. 
%Here we employ Gaussian process 
%regression together with HSIC as a measure of statistical dependence. 
Finally, we also
compare against the Regression Error Causal Inference (RECI) method of 
\cite{Blobaum2018}. 
%Gaussian process regression was employed for both ANM and RECI models.
%In the case of 
For the proposed flow-based method for causal discovery, we 
employ a two layer autoregressive architecture throughout all synthetic experiments
with a base distribution of isotropic Laplace random variables. 



%We benchmark the proposed method against multiple state of the art methods for linear and 
%nonlinear SEMs. These are:
%\begin{enumerate}
%	\item The likelihood ratio for linear non-Gaussian causal models presented by 
%	\cite{Hyvarinen2013}. 
%	\item The additive noise model (ANM) proposed by \cite{Hoyer2009}. 
%	\item The regression error cause inference (RECI) method of \cite{Blobaum2018}. This method is related to the measure of causal direction for nonlinear models 
%	presented in Section 5.2 of \cite{Hyvarinen2013}. 
%	\item The NO-TEARS method proposed by \cite{zheng2018dags}. 
%\end{enumerate}
%%Of the aforementioned methods, both ANM and RECI have been explicitly developed for 
%%nonlinear SEMs whilst the
%In the case of ANM and RECI we employ a Gaussian Process regression.



\subsubsection*{Results on synthetic data}%{Simulation of artificial data}
In order to evaluate the performance of the proposed measure of causal direction, we consider
a series of synthetic experiments where the underlying 
causal model is known. 
%In particular, 
Data was generated according to the following 
SEM:
\begin{equation}
x_1 = n_1 ~~~ \mbox{ and } ~~~ x_2 = f( x_1, n_2),\label{eq:SEMgen}
\end{equation}
where $n_1, n_2$ follow a standard Laplace distribution. 
%where w
We consider three distinct forms for $f$:
\begin{equation*}
x_2 = f(x_1, n_2) = \begin{cases}
\alpha x_1 + n_2       & \quad \text{linear, } \\
x_1 + \alpha x_1^3 + n_2  & \quad \text{nonlinear, }\\
\sigma \left (  \sigma \left ( \alpha x_1 \right ) + n_2 \right ) &\quad \text{neural net.}
\end{cases}
\end{equation*}
We write $\sigma$ to denote the sigmoid non-linearity. 
For each distinct class of SEMs, we consider the performance of each algorithm under 
various distinct sample sizes ranging from $N=25$ to $N=500$ samples. 
Furthermore, each experiment is repeated 250 times. For each repetition, 
the causal ordering selected at random and 
synthetic data is genererated by 
%we generate synthetic bivariate data by
 first sampling $n_1$ and $n_2$
from a standard Laplace distribution and then passing through equation (\ref{eq:SEMgen}). 
%We consider datasets of varying sample sizes, ranging from 25 to 500 observations. 
%We randomly alternate between $x_1$ 

%linear and nonlinear SEMs:
%\begin{itemize}
%	\item LiNGAM: $X_2 = \alpha X_1 + N_2 $
%	\item Additive noise model: $X_2 = X_1 +  \alpha X_1^3 + N_2 $
%	\item Two layer neural network: $X_2 = \sigma \left (  \sigma \left ( \alpha X_1 \right ) + N_2 \right )$
%\end{itemize}






Results are presented in Figure \ref{Fig:causalDiscSimulations}. 
The left panel consider the case of linear SEMs with non-Gaussian disturbances. In such a setting, all
algorithms perform competitively as the sample size increases. 
The middle panel shows results under a nonlinear additive noise model.
%\footnote{We note that a 
%	similar model is studied in \cite{Hoyer2009}}. 
We note that the linear method of \cite{Hyvarinen2013} performs particularly 
poorly in this setting. 
Finally, in the right panel we consider a nonlinear model with non-additive noise structure. 
In this setting, only the proposed method is able to consistently uncover the true causal direction. 
We note that the same 
architecture and training parameters were employed throughout all experiments,
highlighting the fact that the 
%flow architecture  results serve to highlight the fact that the 
proposed method is agnostic to the nature of the true 
underlying causal relationship. 


\subsubsection*{Results on cause effect pairs data}

We also consider performance of the proposed method on cause-effect pairs benchmark dataset
\citep{mooij2016distinguishing}. 
This benchmark consists of  108 distinct  
%This is a collection of 
real-world  bivariate datasets where the objective is to distinguish between cause and effect.
Each dataset consists of observational data from various domains (ranging from biology and medicine to engineering) with known causal associations. 
For each dataset,  the log-likelihood ratio is evaluated as described in equation 
(\ref{eq:flowLR}). 
%
Results for the proposed method, as well as for alternative algorithms, are provided in Table 
\ref{sample-table}, where we note that the proposed method performs marginally better than alternative 
algorithms. 

%The results are provided in Table \ref{sample-table}, which highlight that the proposed 
%method outperforms alternative algorithms. 
%\vksip{-.1cm} 
\begin{table}[h!]
	\caption{Percentage of correct 
		causal variables identified over 108 pairs from the Cause Effect Pairs benchmark.}
	\label{sample-table}
	\vskip 0.15in
	\begin{center}
		\begin{small}
			\begin{sc}
				\begin{tabular}{ccccr}
					\toprule
					Proposed & Linear LR & ANM  & RECI    \\
					\midrule
					73 $\%$ & 66$\%$ & 69 $\%$   &  69$\%$ \\
					\bottomrule
				\end{tabular}
			\end{sc}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}


\section{Affine flow-based causal inference}
\label{sec::flowCI}

The previous section exploited the fact that autoregressive flow architectures are able to estimate
normalized log-densities of data subject to an ordering over variables.  
In this section, we further exploit the \textit{invertible} nature of flow architectures in 
order to perform both interventional and counterfactual inference. 
%
Throughout this section we assume:
\begin{enumerate}%[a)]
	\item the associated causal ordering over variables in known (e.g.,  as the
	result of expert judgment or a randomized control trial or as by using some of the methods 
	described in Section \ref{sec::flowCD}), and 
	\item we train an autoregressive flow model conditional on such an ordering via maximum likelihood.
\end{enumerate}

\subsection*{Interventions}

We now demonstrate how the $do$ operator of \cite{Pearl2009}
can be  incorporated into autoregressive flow models. 
For simplicity, we focus on performing interventions over root nodes in the associated 
DAG, which are assumed known\footnote{This simplifies issues as such nodes have no parents in the DAG and thus there is a one-to-one mapping between the observed variable and 
	the corresponding latent variable. Performing interventions over non-root variables
	would require marginalizing over the parents in the DAG, which could be achieved empirically.}. 
As described in \cite{Pearl2009}, %and \cite{pearl2009causal}, 
intervention on a given variable $x_i$ defines a new 
\textit{mutilated} generative model where
the structural equation associated with variable $x_i$ is replaced by the interventional value.
More formally, the intervention $do( x_i = \alpha)$ changes the structural equation for variable 
$x_i$ from $x_i = f_i( \mathbf{pa}_i, n_i)$ to $x_i = \alpha$. 
This is further simplified if $x_i$ is a root node as this implies that $\mathbf{pa_i} = \emptyset$. 
This allows us to directly infer the value of the
latent variable, $n_i$, associated with the intervention as
%associated latent variable as 
$n_i = f_i^{-1}(  \alpha)$, where $f_i$ is parameterized within the 
autoregressive flow model (see equation (\ref{flow_inverse_1})). Thereafter, we can directly draw samples from 
the base distribution of our flow model for all remaining latent variables and obtain an 
empirical estimate for the interventional distribution by passing
these samples through the flow. This is described in Algorithm \ref{alg:internvention} of the supplementary material. 



\subsubsection*{Toy example}
As a simple example we generate synthetic data from the following SEM:
\begin{align}
\begin{split}
\label{intervention_SEM}
x_1&= n_1, ~~ x_2 = n_2\\
x_3 &= x_1 + \frac{1}{2} x_2^3 + n_3\\
x_4 &= \frac{1}{2} x_1^2 - x_2 + n_4
\end{split}
\end{align}
where each $n_i$ is drawn independently from a standard Laplace distribution. 
We consider the expected values of $x_3$ and $x_4$ under various distinct 
interventions to variable $x_1$. 
From the SEM above we can derive the  expectations for $x_3$ and $x_4$ under
an intervention on $x_1$ as:
\begin{align*}
\mathbb{E}&[x_3 | do(x_1=\alpha)] = \alpha +\frac{1}{2} n_2^3 + n_3 = \alpha \\
\mathbb{E}&[x_4 | do(x_1=\alpha)] =  \frac{1}{2} \alpha^2 -n_2 + n_4 =  \frac{1}{2} \alpha^2 .
\end{align*}

%$$ and  $.
Figure \ref{Fig:interventionExample1} visualizes the predicted expectations for $x_3$ and $x_4$ under
the intervention $do( X_1=\alpha)$ for the proposed method. We note that 
the proposed autoregressive flow architecture is able to correctly infer the nature of 
the true interventional distributions. 
%various values of $\alpha$


%The inferred predictive means, shown in Figure \ref{Fig:interventionExample1},
%are consistent with the true interventional distribution.  

\begin{figure}[t!]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=.85\columnwidth]{Figures/Intervention_Fig1.png}}
		\caption{Predicted expectations for variables $X_3$ and $X_4$ under intervention $do(X_1=x)$ for $x\in [-3, 3]$. Note that the
			flow is able to accurately learn the form the interventional distributions; linear for $X_3$ and
			quadratic for $X_4$ as stipulated in equation (\ref{intervention_SEM}).   }
		\label{Fig:interventionExample1}
	\end{center}
	\vskip -0.2in
\end{figure}

%\subsubsection*{Things that require improvement}
%\begin{itemize}
%	\item how to perform interventions on non-root variables. Due to the autoregressive nature of the 
%	flow, this will require us to evaluate $n_i = \int f_i^{-1}( \mathbf{pa}_i, x_i) ~ d\mathbf{pa}_i$. This can 
%	probably be approximated via bootstrapping observations across the parent variables but maybe 
%	it is possible to do something smarter. 
%	\item what alternative baseline algorithms/datasets are available to evaluate the proposed method?
%\end{itemize}



\subsection*{Counterfactuals} 

A counterfactual query seeks 
quantify statements of the form: what would the value for variable $x_i$ have been if variable $x_j$ had taken 
value $\alpha$, %$\tilde x_j$,
 \textbf{given that we have observed} $\mathbf{x}=\mathbf{x}^{obs}$? 
While 
structural equation models introduce strong assumptions, they also 
facilitate the 
estimation of counterfactual queries. %as described above. 
By construction, the value of observed variables 
$\mathbf{x}$ is fully determined by noise/latent variables $\mathbf{z}$ and the associated structural equations, as described in equation (\ref{SEM_eq}). Abusing notation, this may be written as 
$\mathbf{x} = T( \mathbf{z})$ where $T$ encodes the structural equations. 
%
As such, given a set of structural equations and an observation $\mathbf{x}^{obs}$, 
%we are able to calculate the 
we follow the notation of \cite{Pearl2009} and write 
${x_i}_{x_j \leftarrow \alpha}(\mathbf{z})$
to denote the 
value of $x_i$ under the counterfactual that $x_j\leftarrow  \alpha$ given observation $\mathbf{x}^{obs}= T(\mathbf{z}^{obs})$. 


%Following \cite{Pearl2009}, we write 
%$X_{X_i \leftarrow \tilde x_i}(Z)$ to denote the counterfactual of $X$ 

The process of obtaining counterfactual predictions is described in 
\cite{pearl2009causal} as consisting of three steps:
\begin{enumerate}
	\item \textbf{Abduction}: given an observation $\mathbf{x}^{obs}$, infer the conditional distribution/values over 
	latent variables $\mathbf{z}^{obs}$. 
	In the context of an autoregressive flow model this is obtained as $\mathbf{z}^{obs}= T^{-1} ( \mathbf{x}^{obs} )$. 
	\item \textbf{Action}: substitute
	the values of $\mathbf{z}$ 
%	the equations for $Z$ 
	with the interventional values $\mathbf{z}$, 
	based on the counterfactual query, $\mathbf{x}_{x_j \leftarrow \alpha}$.
	More concretely, for a counterfactual, $\mathbf{x}_{x_j \leftarrow \alpha}$, we 
	replace the 
	structural equations for $x_j$ with 
	$x_j = \alpha$ and 
	adjust the inferred value of latent $z^{obs}_j$ accordingly. 
	As was the case with interventions, 
	if $x_j$ is a root node, then $z^{obs}_j = f_j^{-1}( \alpha )$
	
	\item \textbf{Prediction}:  compute the implied distribution over $\mathbf{x}$ by propagating latent variables, $\mathbf{z}^{obs}$, 
	through the structural equation models. 
\end{enumerate}



\subsubsection*{Toy example}
We continue with the simple 4 dimensional structural equation model
described in equation (\ref{intervention_SEM}). 
We assume we observe $\mathbf{x}^{obs} = (2.00  ,  1.50 ,  0.81, -0.28)$
and consider the counterfactual 
values under two distinct scenarios:
\begin{itemize}
	\item  the expected counterfactual value $x_4$ if 
	instead $x_1=\alpha$ for $\alpha \in [-3,3,]$ instead of
	$x_1=2$ as was observed. This is denoted as $\mathbb{E}[{x_4}_{x_1\leftarrow \alpha}(\mathbf{z}) | \mathbf{x}^{obs}]$.
	\item the expected counterfactual value $x_3$ if $x_2=\alpha$ for $\alpha \in [-3,3,]$ instead of
	$X_2=1.5$ as was observed. This is denoted as $\mathbb{E}[{x_3}_{x_2\leftarrow \alpha}(\mathbf{z}) | \mathbf{x}^{obs} ]$.
\end{itemize}
%
As the true structural equations are provided in equation (\ref{intervention_SEM}), we are able to
compute the true counterfactual expectations and compare these to 
results obtained from an autoregressive flow model. Results, provided in Figure \ref{Fig:counterfactualExample1}, 
highlight the capabilities the proposed method.  



\begin{figure}[ht]
	\vskip 0.2in
	\begin{center}
		\centerline{\includegraphics[width=\columnwidth]{Figures/Counterfactual_Fig1.png}}
		\caption{Predicted and true counterfactual expectations  for variables $x_3$ and $x_4$. Note that 
			flow is able to accurately learn the counterfactual expectations; 
			quadratic in the case of  $x_4$ under a counterfactual on variable $x_1$ and 
			cubic for $x_3$ when considering counterfactuals on $x_2$.   }
		\label{Fig:counterfactualExample1}
	\end{center}
	\vskip -0.2in
\end{figure}


\section{Conclusion}



\newpage 
\bibliographystyle{plainnat}
%\bibliographystyle{icml2020}
\bibliography{library}

%\bibliography{example_paper}

\section*{Supplementary}

\begin{algorithm}[hb]
	\caption{Generate samples from an interventional distribution}
	\label{alg:internvention}
	\begin{algorithmic}
		\STATE {\bfseries Input:}  interventional (root) variable $x_i$, intervention value $\alpha$, number of samples $S$
		\STATE Infer $n_i$ by inverting flow: $n_i = f_i^{-1}( \alpha)$. 
		\FOR{$s=1$ {\bfseries to} $S$}
		\STATE sample $n_j(s)$ from flow base distribution for $j\neq i$
		\STATE set $n_i(s) = n_i$
		\STATE generate interventional sample as $x(s) = T ( n(s))$, i.e., 
		by passing $n(s)$ through flow. 
		%   \FOR{ $j\neq i$}
		%   \STATE  sample $n_j(i)$ from flow base distribution
		%   \ENDFOR
		\ENDFOR
		\STATE {\bfseries Return:} samples $\mathbf{x} = \{x(t): t=1, \ldots, T\}$
	\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2020. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
